{
  "articles": [
    {
      "path": "agroclimatic.html",
      "title": " PLS regression with agroclimatic metrics",
      "author": [],
      "contents": "\r\nAdjusting PLS for use with non-monotonic relationships\r\nThe challenge in using PLS regression to detect the chilling period in colder regions lies in the non-monotonic relationship between temperature and chill effectiveness. Warmer temperatures can either increase or decrease chill accumulation, depending on their relative warmth. To address this, temperature needs to be transformed into a monotonically related measure of chill accumulation.\r\nChill models quantify chill accumulation but raise concerns about their accuracy. Despite this, the analysis proceeds with these models, assuming they reasonably approximate biological processes. Any model uncertainties should be acknowledged.\r\nPLS Analysis with Chilling and Forcing Data\r\nThe chillR package’s PLS_chill_force function performs PLS analysis using daily chill and heat accumulation rates. First, a daily chill object (daily_chill_obj) containing chill/heat rates and mean temperatures is created. The make_daily_chill_plot2 function visualizes daily chill accumulation before applying PLS_chill_force.\r\n\r\n\r\ntemps_hourly <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  stack_hourly_temps(latitude = 50.6)\r\n\r\ndaychill <- daily_chill(hourtemps = temps_hourly,\r\n                        running_mean = 1,\r\n                        models = list(\r\n                          Chilling_Hours = Chilling_Hours,\r\n                          Utah_Chill_Units = Utah_Model,\r\n                          Chill_Portions = Dynamic_Model,\r\n                          GDH = GDH)\r\n                        )\r\n\r\ndc <- make_daily_chill_plot2(daychill,\r\n                             metrics = c(\"Chill_Portions\"),\r\n                             cumulative = FALSE,\r\n                             startdate = 300,\r\n                             enddate = 30,\r\n                             focusyears = c(2008), \r\n                             metriclabels = \"Chill Portions\")\r\n\r\n\r\n\r\nThe plot highlights specific years with the focusyears parameter, and switching to a cumulative view compares current chill accumulation to historical patterns.\r\n\r\n\r\ndc <- make_daily_chill_plot2(daychill,\r\n                             metrics = c(\"Chill_Portions\"),\r\n                             cumulative = TRUE,\r\n                             startdate = 300,\r\n                             enddate = 30,\r\n                             focusyears = c(2008),\r\n                             metriclabels = \"Chill Portions\")\r\n\r\n\r\n\r\nThe plot includes double ticks on the x-axis to account for leap years, ensuring Julian dates align accurately with calendar dates. The next step is to use the daily chill object with the PLS_chill_force function and pear bloom data for further analysis.\r\n\r\n\r\nAlex_first <- read_tab(\"data/Alexander_Lucas_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, \r\n         JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\n\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame = Alex_first,\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\")\r\n\r\n\r\nThe PLS_chill_force function evaluates multiple chill and heat models and outputs results that can be accessed via plscf$Chill_Portions$GDH$PLS_summary. These results are visualized using the plot_PLS function.\r\n\r\n\r\nplot_PLS(plscf,\r\n         PLS_results_path = \"data/plscf_outputs\")\r\n\r\n\r\nAlthough the PLS_chill_force function generates a plot, a more modern approach is to use ggplot2 for better customization.\r\nPlot of results from the PLS_chill_force procedure, as plotted with chillR’s standard plotting functionThe lack of clarity in the results is due to the absence of a running mean, especially for the Dynamic Model. Applying an 11-day running mean helps smooth the data for clearer results.\r\n\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame=Alex_first,\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\",\r\n                         runn_means = 11)\r\nplot_PLS(plscf,\r\n         PLS_results_path = \"data/plscf_outputs_11days\")\r\n\r\n\r\n\r\nThe plots now show clearer results: the left plot links bloom dates with chill accumulation, and the right plot does the same for heat accumulation. Both are derived from the same PLS analysis.\r\nTo identify chilling and forcing periods, consistent negative model coefficients are examined. The forcing period is evident between early January and bloom (mid-March to early May), while the chilling period is seen from November/December to February.\r\nChilling and forcing periods are debated, but in this dataset, the chilling period spans from November 13th to March 3rd (Julian dates -48 to 62), and the forcing period runs from January 3rd to the median bloom date (Julian dates 3 to 105.5). These periods can be marked in the plot for clarity.\r\n\r\n\r\nplot_PLS(plscf,\r\n         PLS_results_path = \"data/plscf_outputs_11days_periods\",\r\n         add_chill = c(-48,62),\r\n         add_heat = c(3,105.5))\r\n\r\n\r\n\r\nggplotting the results\r\nThe process has been mostly covered during the creation of the original PLS plots, but now it will be applied to the PLS_chill_force outputs. The key difference is splitting the results by chill and heat analysis using facet_wrap. The data needs to be prepared for ggplotting.\r\n\r\n\r\nPLS_gg <- plscf$Chill_Portions$GDH$PLS_summary %>%\r\n  mutate(Month = trunc(Date/100),\r\n         Day = Date - Month * 100,\r\n         Date = ISOdate(2002,\r\n                        Month,\r\n                        Day))\r\n\r\nPLS_gg[PLS_gg$JDay <= 0,\"Date\"]<-\r\n  ISOdate(2001,\r\n          PLS_gg$Month[PLS_gg$JDay <= 0],\r\n          PLS_gg$Day[PLS_gg$JDay <= 0])\r\n\r\nPLS_gg <- PLS_gg %>%\r\n  mutate(VIP_importance = VIP >= 0.8,\r\n         VIP_Coeff = factor(sign(Coef) * VIP_importance))\r\n\r\nchill_start_JDay <- -48\r\nchill_end_JDay <- 62\r\nheat_start_JDay <- 3\r\nheat_end_JDay <- 105.5\r\n\r\nchill_start_date <- ISOdate(2001,\r\n                            12,\r\n                            31) + chill_start_JDay * 24 * 3600\r\nchill_end_date <- ISOdate(2001,\r\n                          12,\r\n                          31) + chill_end_JDay * 24 * 3600\r\nheat_start_date <- ISOdate(2001,\r\n                           12,\r\n                           31) + heat_start_JDay * 24 * 3600\r\nheat_end_date <- ISOdate(2001,\r\n                         12,\r\n                         31) + heat_end_JDay * 24 * 3600\r\n\r\n\r\nThe process begins with the bottom plot, as it is the most complicated. The complexity arises from needing different labels on the y-axes for the two facets, as well as different scales. The daily chill accumulation rate ranges from 0 to around 1 Chill Portion, while the daily heat accumulation rate can exceed 300 GDH. Solving these issues for the bottom plot may influence the approach for the other plots.\r\n\r\n\r\ntemp_plot <- ggplot(PLS_gg,\r\n                    x = Date) +\r\n  annotate(\"rect\",\r\n           xmin = chill_start_date,\r\n           xmax = chill_end_date,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"blue\") +\r\n  annotate(\"rect\",\r\n           xmin = heat_start_date,\r\n           xmax = heat_end_date,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"red\") +\r\n  annotate(\"rect\",\r\n           xmin = ISOdate(2001,\r\n                          12,\r\n                          31) +\r\n             min(plscf$pheno$pheno,\r\n                 na.rm = TRUE) * 24 * 3600,\r\n           xmax = ISOdate(2001,\r\n                          12,\r\n                          31) +\r\n             max(plscf$pheno$pheno,\r\n                 na.rm = TRUE) * 24 * 3600,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"black\") +\r\n  geom_vline(xintercept = ISOdate(2001,\r\n                                  12,\r\n                                  31) +\r\n               median(plscf$pheno$pheno,\r\n                      na.rm = TRUE) * 24 * 3600,\r\n             linetype = \"dashed\") +\r\n  geom_ribbon(aes(x = Date,\r\n                  ymin = MetricMean - MetricStdev ,\r\n                  ymax = MetricMean + MetricStdev),\r\n              fill=\"grey\") +\r\n  geom_ribbon(aes(x = Date,\r\n                  ymin = MetricMean - MetricStdev * (VIP_Coeff == -1),\r\n                  ymax = MetricMean + MetricStdev * (VIP_Coeff == -1)),\r\n              fill = \"red\") +\r\n  geom_ribbon(aes(x = Date,\r\n                  ymin = MetricMean - MetricStdev * (VIP_Coeff == 1),\r\n                  ymax = MetricMean + MetricStdev * (VIP_Coeff == 1)),\r\n              fill = \"dark green\") +\r\n  geom_line(aes(x = Date,\r\n                y = MetricMean ))\r\n\r\ntemp_plot\r\n\r\n\r\n\r\n\r\n\r\ntemp_plot <- temp_plot +\r\n  facet_wrap(vars(Type),\r\n             scales = \"free_y\",\r\n             strip.position = \"left\",\r\n             labeller =\r\n               labeller(Type = as_labeller(\r\n               c(Chill = \"Chill (CP)\",\r\n                 Heat = \"Heat (GDH)\")))) +\r\n  ggtitle(\"Daily chill and heat accumulation rates\") +\r\n  theme_bw(base_size = 15) + \r\n  theme(strip.background = element_blank(),\r\n        strip.placement = \"outside\",\r\n        strip.text.y = element_text(size =12),\r\n        plot.title = element_text(hjust = 0.5),\r\n        axis.title.y = element_blank()\r\n        )\r\n\r\ntemp_plot\r\n\r\n\r\n\r\nAfter exploring ways to customize the y-axis labels for each facet without finding a viable solution, the facet labels were used instead and moved to the left side to serve as y-axis labels. The labeller element in facet_wrap allows easy customization with chosen text. A title was also added to the plot.\r\nThe same approach can now be applied to the VIP and model coefficient plots to maintain a consistent structure when combining them later.\r\n\r\n\r\nVIP_plot<- ggplot(PLS_gg,\r\n                  aes(x = Date,\r\n                      y = VIP)) +\r\n  annotate(\"rect\",\r\n           xmin = chill_start_date,\r\n           xmax = chill_end_date,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"blue\") +\r\n  annotate(\"rect\",\r\n           xmin = heat_start_date,\r\n           xmax = heat_end_date,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"red\") +\r\n  annotate(\"rect\",\r\n           xmin = ISOdate(2001,\r\n                          12,\r\n                          31) +\r\n             min(plscf$pheno$pheno,\r\n                 na.rm = TRUE) * 24 * 3600,\r\n           xmax = ISOdate(2001,\r\n                          12,\r\n                          31) +\r\n             max(plscf$pheno$pheno,\r\n                 na.rm = TRUE) * 24 * 3600,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"black\") +\r\n  geom_vline(xintercept = ISOdate(2001,\r\n                                  12,\r\n                                  31) +\r\n               median(plscf$pheno$pheno,\r\n                      na.rm = TRUE) * 24 * 3600,\r\n             linetype = \"dashed\") +\r\n  geom_bar(stat = 'identity',\r\n           aes(fill = VIP > 0.8))\r\n\r\nVIP_plot\r\n\r\n\r\n\r\n\r\n\r\nVIP_plot <- VIP_plot + \r\n  facet_wrap(vars(Type),\r\n             scales = \"free\",\r\n             strip.position = \"left\",\r\n             labeller = \r\n               labeller(Type = as_labeller(\r\n                 c(Chill = \"VIP for chill\",\r\n                   Heat = \"VIP for heat\")))) +\r\n  scale_y_continuous(\r\n    limits = c(0,\r\n               max(plscf$Chill_Portions$GDH$PLS_summary$VIP))) +\r\n  ggtitle(\"Variable Importance in the Projection (VIP) scores\") +\r\n  theme_bw(base_size = 15) + \r\n  theme(strip.background = element_blank(),\r\n        strip.placement = \"outside\",\r\n        strip.text.y = element_text(size = 12),\r\n        plot.title = element_text(hjust = 0.5),\r\n        axis.title.y = element_blank()\r\n        )\r\n  \r\nVIP_plot\r\n\r\n\r\n\r\n\r\n\r\nVIP_plot <- VIP_plot +\r\n  scale_fill_manual(name = \"VIP\", \r\n                    labels = c(\"<0.8\", \">0.8\"), \r\n                    values = c(\"FALSE\" = \"grey\",\r\n                               \"TRUE\" = \"blue\")) +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank(),\r\n        axis.title.y = element_blank())\r\n\r\nVIP_plot\r\n\r\n\r\n\r\n\r\n\r\ncoeff_plot <- ggplot(PLS_gg,\r\n                     aes(x = Date,\r\n                         y = Coef)) +\r\n  annotate(\"rect\",\r\n           xmin = chill_start_date,\r\n           xmax = chill_end_date,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"blue\") +\r\n  annotate(\"rect\",\r\n           xmin = heat_start_date,\r\n           xmax = heat_end_date,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"red\") +\r\n  annotate(\"rect\",\r\n           xmin = ISOdate(2001,\r\n                          12,\r\n                          31) +\r\n             min(plscf$pheno$pheno,\r\n                 na.rm=TRUE) * 24 * 3600,\r\n           xmax = ISOdate(2001,\r\n                          12,\r\n                          31) +\r\n             max(plscf$pheno$pheno,\r\n                 na.rm = TRUE) * 24 * 3600,\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           alpha = .1,\r\n           fill = \"black\") +\r\n  geom_vline(xintercept = ISOdate(2001,\r\n                                  12,\r\n                                  31) +\r\n               median(plscf$pheno$pheno,\r\n                      na.rm = TRUE) * 24 * 3600,\r\n             linetype = \"dashed\") +\r\n  geom_bar(stat = 'identity',\r\n           aes(fill = VIP_Coeff))\r\n\r\ncoeff_plot\r\n\r\n\r\n\r\n\r\n\r\ncoeff_plot <- coeff_plot +\r\n  facet_wrap(vars(Type),\r\n             scales = \"free\",\r\n             strip.position = \"left\",\r\n             labeller =\r\n               labeller(\r\n                 Type = as_labeller(\r\n                   c(Chill = \"MC for chill\",\r\n                     Heat = \"MC for heat\")))) +\r\n  scale_y_continuous(\r\n    limits = c(min(plscf$Chill_Portions$GDH$PLS_summary$Coef),\r\n               max(plscf$Chill_Portions$GDH$PLS_summary$Coef))) +\r\n  ggtitle(\"Model coefficients (MC)\") +\r\n  theme_bw(base_size = 15) + \r\n  theme(strip.background = element_blank(),\r\n        strip.placement = \"outside\",\r\n        strip.text.y = element_text(size = 12),\r\n        plot.title = element_text(hjust = 0.5),\r\n        axis.title.y = element_blank()\r\n        )\r\n  \r\ncoeff_plot \r\n\r\n\r\n\r\n\r\n\r\ncoeff_plot <- coeff_plot +  \r\n  scale_fill_manual(name=\"Effect direction\", \r\n                    labels = c(\"Advancing\",\r\n                               \"Unimportant\",\r\n                               \"Delaying\"), \r\n                    values = c(\"-1\" = \"red\",\r\n                               \"0\" = \"grey\",\r\n                               \"1\" = \"dark green\")) +\r\n  ylab(\"PLS coefficient\") +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank(),\r\n        axis.title.y = element_blank())\r\n\r\ncoeff_plot\r\n\r\n\r\n\r\nThe plots can now be combined using the patchwork package.\r\n\r\n\r\nplot<- (VIP_plot +\r\n          coeff_plot +\r\n          temp_plot +\r\n          plot_layout(ncol = 1,\r\n            guides = \"collect\")\r\n        ) & theme(legend.position = \"right\",\r\n                  legend.text = element_text(size = 8),\r\n                  legend.title = element_text(size = 10),\r\n                  axis.title.x = element_blank())\r\n\r\nplot\r\n\r\n\r\n\r\nA function will be created from this process:\r\n\r\n\r\nplot_PLS_chill_force <- function(plscf,\r\n                                 chill_metric = \"Chill_Portions\",\r\n                                 heat_metric = \"GDH\",\r\n                                 chill_label = \"CP\",\r\n                                 heat_label = \"GDH\",\r\n                                 chill_phase = c(-48, 62),\r\n                                 heat_phase = c(3, 105.5))\r\n{\r\n  PLS_gg <- plscf[[chill_metric]][[heat_metric]]$PLS_summary %>%\r\n    mutate(Month = trunc(Date/100),\r\n           Day = Date - Month * 100,\r\n           Date = ISOdate(2002,\r\n                          Month,\r\n                          Day))\r\n  \r\n  PLS_gg[PLS_gg$JDay <= 0,\"Date\"]<-\r\n    ISOdate(2001,\r\n            PLS_gg$Month[PLS_gg$JDay <= 0],\r\n            PLS_gg$Day[PLS_gg$JDay <= 0])\r\n  \r\n  PLS_gg <- PLS_gg %>%\r\n    mutate(VIP_importance = VIP >= 0.8,\r\n           VIP_Coeff = factor(sign(Coef) * VIP_importance))\r\n  \r\n  chill_start_date <- ISOdate(2001,\r\n                              12,\r\n                              31) + chill_phase[1] * 24 * 3600\r\n  chill_end_date <- ISOdate(2001,\r\n                            12,\r\n                            31) + chill_phase[2] * 24 * 3600\r\n  heat_start_date <- ISOdate(2001,\r\n                             12,\r\n                             31) + heat_phase[1] * 24 * 3600\r\n  heat_end_date <- ISOdate(2001,\r\n                           12,\r\n                           31) + heat_phase[2] * 24 * 3600\r\n\r\n\r\n\r\n\r\n  temp_plot <- ggplot(PLS_gg) +\r\n    annotate(\"rect\",\r\n             xmin = chill_start_date,\r\n             xmax = chill_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"blue\") +\r\n    annotate(\"rect\",\r\n             xmin = heat_start_date,\r\n             xmax = heat_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"red\") +\r\n    annotate(\"rect\",\r\n             xmin = ISOdate(2001,\r\n                            12,\r\n                            31) +\r\n               min(plscf$pheno$pheno,\r\n                   na.rm = TRUE) * 24 * 3600,\r\n             xmax = ISOdate(2001,\r\n                            12,\r\n                            31) +\r\n               max(plscf$pheno$pheno,\r\n                   na.rm = TRUE) * 24 * 3600,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"black\") +\r\n    geom_vline(xintercept = ISOdate(2001,\r\n                                    12,\r\n                                    31) +\r\n                 median(plscf$pheno$pheno,\r\n                        na.rm=TRUE) * 24 * 3600,\r\n               linetype = \"dashed\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = MetricMean - MetricStdev ,\r\n                    ymax = MetricMean + MetricStdev ),\r\n                fill = \"grey\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = MetricMean - MetricStdev * (VIP_Coeff == -1),\r\n                    ymax = MetricMean + MetricStdev * (VIP_Coeff == -1)),\r\n                fill = \"red\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = MetricMean - MetricStdev * (VIP_Coeff == 1),\r\n                    ymax = MetricMean + MetricStdev * (VIP_Coeff == 1)),\r\n                fill = \"dark green\") +\r\n    geom_line(aes(x = Date,\r\n                  y = MetricMean)) +\r\n    facet_wrap(vars(Type),\r\n               scales = \"free_y\",\r\n               strip.position = \"left\",\r\n               labeller = \r\n                 labeller(\r\n                   Type =\r\n                     as_labeller(c(Chill = paste0(\"Chill (\",\r\n                                                  chill_label,\r\n                                                  \")\"),\r\n                                   Heat = paste0(\"Heat (\",\r\n                                                 heat_label,\r\n                                                 \")\"))))) +\r\n    ggtitle(\"Daily chill and heat accumulation rates\") +\r\n    theme_bw(base_size = 15) + \r\n    theme(strip.background = element_blank(),\r\n          strip.placement = \"outside\",\r\n          strip.text.y = element_text(size = 12),\r\n          plot.title = element_text(hjust = 0.5),\r\n          axis.title.y = element_blank()\r\n          )\r\n  \r\n  VIP_plot <- ggplot(PLS_gg,\r\n                     aes(x = Date,\r\n                         y = VIP)) +\r\n    annotate(\"rect\",\r\n             xmin = chill_start_date,\r\n             xmax = chill_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"blue\") +\r\n    annotate(\"rect\",\r\n             xmin = heat_start_date,\r\n             xmax = heat_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"red\") +\r\n    annotate(\"rect\",\r\n             xmin = ISOdate(2001,\r\n                            12,\r\n                            31) + min(plscf$pheno$pheno,\r\n                                      na.rm = TRUE) * 24 * 3600,\r\n             xmax = ISOdate(2001,\r\n                            12,\r\n                            31) + max(plscf$pheno$pheno,\r\n                                      na.rm = TRUE) * 24 * 3600,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"black\") +\r\n    geom_vline(xintercept = ISOdate(2001,\r\n                                    12,\r\n                                    31) + median(plscf$pheno$pheno,\r\n                                                 na.rm = TRUE) * 24 * 3600,\r\n               linetype = \"dashed\") +\r\n    geom_bar(stat = 'identity',\r\n             aes(fill = VIP>0.8)) +\r\n    facet_wrap(vars(Type), \r\n               scales = \"free\",\r\n               strip.position = \"left\",\r\n               labeller = \r\n                 labeller(\r\n                   Type = as_labeller(c(Chill=\"VIP for chill\",\r\n                                        Heat=\"VIP for heat\")))) +\r\n    scale_y_continuous(\r\n      limits = c(0,\r\n                 max(plscf[[chill_metric]][[heat_metric]]$PLS_summary$VIP))) +\r\n    ggtitle(\"Variable Importance in the Projection (VIP) scores\") +\r\n    theme_bw(base_size = 15) + \r\n    theme(strip.background = element_blank(),\r\n          strip.placement = \"outside\",\r\n          strip.text.y = element_text(size = 12),\r\n          plot.title = element_text(hjust = 0.5),\r\n          axis.title.y = element_blank()\r\n          ) +\r\n    scale_fill_manual(name = \"VIP\", \r\n                      labels = c(\"<0.8\", \">0.8\"), \r\n                      values = c(\"FALSE\" = \"grey\",\r\n                                 \"TRUE\" = \"blue\")) +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank(),\r\n          axis.title.y = element_blank())\r\n  \r\n  coeff_plot <- ggplot(PLS_gg,\r\n                       aes(x = Date,\r\n                           y = Coef)) +\r\n    annotate(\"rect\",\r\n             xmin = chill_start_date,\r\n             xmax = chill_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"blue\") +\r\n    annotate(\"rect\",\r\n             xmin = heat_start_date,\r\n             xmax = heat_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"red\") +\r\n    annotate(\"rect\",\r\n             xmin = ISOdate(2001,\r\n                            12,\r\n                            31) + min(plscf$pheno$pheno,\r\n                                      na.rm = TRUE) * 24 * 3600,\r\n             xmax = ISOdate(2001,\r\n                            12,\r\n                            31) + max(plscf$pheno$pheno,\r\n                                      na.rm = TRUE) * 24 * 3600,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,\r\n             fill = \"black\") +\r\n    geom_vline(xintercept = ISOdate(2001,\r\n                                    12,\r\n                                    31) + median(plscf$pheno$pheno,\r\n                                                 na.rm = TRUE) * 24 * 3600,\r\n               linetype = \"dashed\") +\r\n    geom_bar(stat = 'identity',\r\n             aes(fill = VIP_Coeff)) +\r\n    facet_wrap(vars(Type),\r\n               scales = \"free\",\r\n               strip.position = \"left\",\r\n               labeller =\r\n                 labeller(\r\n                   Type = as_labeller(c(Chill = \"MC for chill\",\r\n                                        Heat = \"MC for heat\")))) +\r\n    scale_y_continuous(\r\n      limits = c(min(plscf[[chill_metric]][[heat_metric]]$PLS_summary$Coef),\r\n                 max(plscf[[chill_metric]][[heat_metric]]$PLS_summary$Coef))) +\r\n    ggtitle(\"Model coefficients (MC)\") +\r\n    theme_bw(base_size = 15) + \r\n    theme(strip.background = element_blank(),\r\n          strip.placement = \"outside\",\r\n          strip.text.y = element_text(size = 12),\r\n          plot.title = element_text(hjust = 0.5),\r\n          axis.title.y = element_blank()\r\n          ) +\r\n    scale_fill_manual(name = \"Effect direction\", \r\n                      labels = c(\"Advancing\",\r\n                                 \"Unimportant\",\r\n                                 \"Delaying\"), \r\n                      values = c(\"-1\" = \"red\",\r\n                                 \"0\" = \"grey\",\r\n                                 \"1\" = \"dark green\")) +\r\n    ylab(\"PLS coefficient\") +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank(),\r\n          axis.title.y = element_blank())\r\n  \r\n  library(patchwork)\r\n  \r\n  plot <- (VIP_plot +\r\n             coeff_plot +\r\n             temp_plot +\r\n             plot_layout(ncol = 1,\r\n                         guides = \"collect\")\r\n           ) & theme(legend.position = \"right\",\r\n                     legend.text = element_text(size = 8),\r\n                     legend.title = element_text(size = 10),\r\n                     axis.title.x = element_blank())\r\n\r\nplot\r\n\r\n}\r\n\r\nplot_PLS_chill_force(plscf)\r\n\r\n\r\n\r\nWith the plot production automated, it’s now easy to explore how other chill models perform in distinguishing chilling and forcing periods.\r\n\r\n\r\ndaychill <- daily_chill(hourtemps = temps_hourly,\r\n                        running_mean = 11,\r\n                        models = list(Chilling_Hours = Chilling_Hours,\r\n                                      Utah_Chill_Units = Utah_Model,\r\n                                      Chill_Portions = Dynamic_Model,\r\n                                      GDH = GDH)\r\n                        )\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame = Alex_first,\r\n                         split_month = 6,\r\n                         chill_models = c(\"Chilling_Hours\",\r\n                                          \"Utah_Chill_Units\",\r\n                                          \"Chill_Portions\"),\r\n                       heat_models = c(\"GDH\"))\r\n\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Chilling_Hours\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CH\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(0,0),\r\n                     heat_phase = c(0,0))\r\n\r\n\r\n\r\n\r\n\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Utah_Chill_Units\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CU\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(0,0),\r\n                     heat_phase = c(0,0))\r\n\r\n\r\n\r\nThe other two common models also struggle to accurately identify the chilling period. The reasons for this will be reflected upon later.\r\nExercises on chill model comparison\r\nRepeat the PLS_chill_force procedure for the ‘Roter Boskoop’ dataset. Include plots of daily chill and heat accumulation.\r\n\r\n\r\n# Read bloom dataset of 'Roter Boskoop' \r\nRB_first <- read_tab(\"data/Roter_Boskoop_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, \r\n         JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\n# Read hourly temperatures \r\ntemps_hourly <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  stack_hourly_temps(latitude = 50.6)\r\n\r\n# Produce daily chill and heat accumulation rates\r\ndaychill <- daily_chill(hourtemps = temps_hourly,\r\n                        running_mean = 1,\r\n                        models = list(\r\n                          Chilling_Hours = Chilling_Hours,\r\n                          Utah_Chill_Units = Utah_Model,\r\n                          Chill_Portions = Dynamic_Model,\r\n                          GDH = GDH)\r\n                        )\r\n# Implement PLS analysis based on daily chill and heat accumulation rates \r\nplscf_RB <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame = RB_first,\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\")\r\n\r\n\r\n\r\n\r\n# Split the results according to chill vs. heat analysis\r\nPLS_gg <- plscf_RB$Chill_Portions$GDH$PLS_summary %>%\r\n  mutate(Month = trunc(Date/100),\r\n         Day = Date - Month * 100,\r\n         Date = ISOdate(2002,\r\n                        Month,\r\n                        Day))\r\n\r\nPLS_gg[PLS_gg$JDay <= 0,\"Date\"]<-\r\n  ISOdate(2001,\r\n          PLS_gg$Month[PLS_gg$JDay <= 0],\r\n          PLS_gg$Day[PLS_gg$JDay <= 0])\r\n\r\nPLS_gg <- PLS_gg %>%\r\n  mutate(VIP_importance = VIP >= 0.8,\r\n         VIP_Coeff = factor(sign(Coef) * VIP_importance))\r\n\r\nchill_start_JDay <- -48\r\nchill_end_JDay <- 62\r\nheat_start_JDay <- 3\r\nheat_end_JDay <- 117\r\n\r\nchill_start_date <- ISOdate(2001,\r\n                            12,\r\n                            31) + chill_start_JDay * 24 * 3600\r\nchill_end_date <- ISOdate(2001,\r\n                          12,\r\n                          31) + chill_end_JDay * 24 * 3600\r\nheat_start_date <- ISOdate(2001,\r\n                           12,\r\n                           31) + heat_start_JDay * 24 * 3600\r\nheat_end_date <- ISOdate(2001,\r\n                         12,\r\n                         31) + heat_end_JDay * 24 * 3600\r\n\r\n\r\n\r\n\r\n# Plot daily chill and heat accumulation rates \r\ntemp_plot <- temp_plot +\r\n  facet_wrap(vars(Type),\r\n             scales = \"free_y\",\r\n             strip.position = \"left\",\r\n             labeller =\r\n               labeller(Type = as_labeller(\r\n               c(Chill = \"Chill (CP)\",\r\n                 Heat = \"Heat (GDH)\")))) +\r\n  ggtitle(\"Daily chill and heat accumulation rates\") +\r\n  theme_bw(base_size = 15) + \r\n  theme(strip.background = element_blank(),\r\n        strip.placement = \"outside\",\r\n        strip.text.y = element_text(size =12),\r\n        plot.title = element_text(hjust = 0.5),\r\n        axis.title.y = element_blank()\r\n        )\r\n\r\ntemp_plot\r\n\r\n\r\n\r\nRun PLS_chill_force analyses for all three major chill models. Delineate your best estimates of chilling and forcing phases for all of them.\r\n\r\n\r\nplscf_RB <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame = RB_first,\r\n                         split_month = 6,\r\n                         chill_models = c(\"Chilling_Hours\",\r\n                                          \"Utah_Chill_Units\",\r\n                                          \"Chill_Portions\"),\r\n                       heat_models = c(\"GDH\"))\r\n\r\n\r\nPlot results for all three analyses, including shaded plot areas for the chilling and forcing periods you estimated.\r\n\r\n\r\n# Chilling Hours Model\r\nplot_PLS_chill_force(plscf_ch,\r\n                     chill_metric = \"Chilling_Hours\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CH\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(0,0),\r\n                     heat_phase = c(0,0))\r\n\r\n\r\n\r\n\r\n\r\n# Utah Model\r\nplot_PLS_chill_force(plscf_utah,\r\n                     chill_metric = \"Utah_Chill_Units\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CU\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(0,0),\r\n                     heat_phase = c(0,0))\r\n\r\n\r\n\r\n\r\n\r\n# Dynamic Model\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Chill_Portions\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CU\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(0,0),\r\n                     heat_phase = c(0,0))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:24:25+01:00"
    },
    {
      "path": "chill.html",
      "title": "Chill models",
      "author": [],
      "contents": "\r\nIn this chapter, various chill models will be explored using the chillR package in R, which simplifies the calculation of chilling hours and other dormancy-related metrics based on temperature data.\r\nChilling_Hours() Function\r\nThe Chilling_Hours() function calculates the time during which temperatures fall within a key range for chill accumulation. It takes hourly temperature data as input and, by default, provides the cumulative amount of chilling accumulated over time.\r\n\r\n\r\nChilling_Hours(Winters_hours_gaps$Temp)[1:100]\r\n\r\n  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  2  2  3  4  5\r\n [22]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\r\n [43]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  7  8\r\n [64]  9 10 11 12 13 14 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16\r\n [85] 16 17 18 19 20 21 22 23 24 25 25 25 25 25 25 25\r\n\r\nThe result will show the first 100 values, where the cumulative chilling hours increase as the temperature falls within the specified range.\r\nUtah Model\r\nThe Utah Model assigns different weights to various temperature ranges, reflecting their impact on chill accumulation. The Utah_Model() function in chillR calculates these weighted chilling contributions for each hour of temperature data. The output will show the Utah model values for the first 100 hours, where positive, zero, and negative weights are applied based on the temperature:\r\n\r\n\r\nUtah_Model(Winters_hours_gaps$Temp)[1:100]\r\n\r\n  [1]  0.0 -0.5 -1.5 -2.5 -3.5 -4.5 -5.5 -6.0 -6.0 -6.0 -5.5 -5.0 -4.0\r\n [14] -3.0 -2.0 -1.0  0.0  0.5  1.5  2.5  3.5  4.5  5.0  5.0  5.0  4.0\r\n [27]  3.0  2.0  1.0  0.0 -1.0 -2.0 -2.5 -2.5 -2.0 -1.5 -1.0 -0.5  0.5\r\n [40]  1.0  1.5  2.0  2.0  2.5  3.0  3.5  4.0  4.0  4.0  3.5  2.5  1.5\r\n [53]  0.5 -0.5 -1.5 -2.5 -3.0 -3.0 -2.5 -1.5 -0.5  0.5  1.5  2.5  3.5\r\n [66]  4.5  5.5  6.5  7.5  8.5  9.5 10.0 10.0  9.5  9.0  8.5  8.0  7.5\r\n [79]  7.0  6.5  6.5  7.0  7.5  8.5  9.5 10.5 11.5 12.5 13.5 14.5 15.5\r\n [92] 16.5 17.5 18.5 19.0 19.0 19.0 18.5 17.5 16.5\r\n\r\nCreating Custom Chill Models with step_model()\r\nThe step_model() function, part of the chillR package, enables the creation of custom chill models based on temperature thresholds and weights. This process involves defining a data frame that specifies temperature ranges and their corresponding weights. Here’s an example of a data frame that defines temperature ranges and their corresponding weights:\r\n\r\n\r\ndf <- data.frame(\r\n  lower = c(-1000, 1, 2, 3, 4, 5,    6),\r\n  upper = c(    1, 2, 3, 4, 5, 6, 1000),\r\n  weight = c(   0, 1, 2, 3, 2, 1,    0))\r\n\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nweight\r\n\r\n\r\n-1000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n2\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n1000\r\n\r\n\r\n0\r\n\r\n\r\nA function called custom() implements a chill model based on this data frame. This function is then applied to the Winters_hours_gaps dataset to calculate the chilling contributions:\r\n\r\n\r\ncustom <- function(x) step_model(x, df)\r\ncustom(Winters_hours_gaps$Temp)[1:100]\r\n\r\n  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  4\r\n [22]  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\r\n [43]  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\r\n [64]  7  8 10 13 16 19 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\r\n [85] 22 22 22 23 25 27 29 31 34 37 37 37 37 37 37 37\r\n\r\nDynamic model\r\nThe Dynamic Model provides a more complex and reliable approach to calculating chill, with the Dynamic_Model() function handling the intricate equations involved. This function can be easily applied to the Winters_hours_gaps dataset, producing output that displays dynamic chill values for the first 100 hours, reflecting the underlying physiological processes:\r\n\r\n\r\nDynamic_Model(Winters_hours_gaps$Temp)[1:100]\r\n\r\n  [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n  [7] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [13] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [19] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [25] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [31] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [37] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [43] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [49] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [55] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [61] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\r\n [67] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.9698435\r\n [73] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435\r\n [79] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435\r\n [85] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435\r\n [91] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435\r\n [97] 0.9698435 0.9698435 0.9698435 0.9698435\r\n\r\nChilling and tempResponse functions\r\nThe chillR package offers several functions for analyzing hourly temperature data, including wrapper functions that enable the computation of chill between specific start and end dates. The chilling() function automatically calculates various basic metrics, including Chilling Hours, Utah Model, Dynamic Model, and Growing Degree Hours. It is important to use the make_JDay() function to add Julian dates (which count the days of the year) to the dataset, ensuring proper functionality.\r\n\r\n\r\nchill_output <- chilling(make_JDay(Winters_hours_gaps), Start_JDay = 90, End_JDay = 100)\r\n\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\nChilling_Hours\r\n\r\n\r\nUtah_Model\r\n\r\n\r\nChill_portions\r\n\r\n\r\nGDH\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n11\r\n\r\n\r\n11\r\n\r\n\r\n100\r\n\r\n\r\n40\r\n\r\n\r\n15.5\r\n\r\n\r\n2.009147\r\n\r\n\r\n2406.52\r\n\r\n\r\n\r\nHowever, there may be instances where not all metrics are desired, or there is a need for different metrics altogether. In such cases, the tempResponse function can be employed. This function is similar to chilling() but offers the flexibility to take a list of specific temperature models to be computed as input.\r\n\r\n\r\nchill_output <- tempResponse(make_JDay(Winters_hours_gaps), \r\n                       Start_JDay = 90, \r\n                       End_JDay = 100, \r\n                       models = list(Chill_Portions = Dynamic_Model, GDH = GDH))\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\nChill_Portions\r\n\r\n\r\nGDH\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n11\r\n\r\n\r\n11\r\n\r\n\r\n100\r\n\r\n\r\n2.009147\r\n\r\n\r\n2406.52\r\n\r\n\r\nThis will return only the Dynamic Model and Growing Degree Hours (GDH) values for the specified period.\r\nExercises on chill models\r\nRun the chilling() function on the Winters_hours_gap dataset.\r\n\r\n\r\naugust <- chilling(make_JDay(Winters_hours_gaps), Start_JDay = 214, End_JDay = 244)\r\n\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\nChilling_Hours\r\n\r\n\r\nUtah_Model\r\n\r\n\r\nChill_portions\r\n\r\n\r\nGDH\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n31\r\n\r\n\r\n31\r\n\r\n\r\n100\r\n\r\n\r\n0\r\n\r\n\r\n-569.5\r\n\r\n\r\n0\r\n\r\n\r\n9933.327\r\n\r\n\r\n\r\nCreate your own temperature-weighting chill model using the step_model() function.\r\n\r\n\r\ndf <- data.frame(\r\n  lower = c(-1000, 0,  5, 10, 15, 20,   25),  \r\n  upper = c(    0, 5, 10, 15, 20, 25, 1000), \r\n  weight = c(   0, 1,  2,  3,  2,  1,    0))\r\n\r\ncustom <- function(x) step_model(x, df)\r\n\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nweight\r\n\r\n\r\n-1000\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n5\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n10\r\n\r\n\r\n2\r\n\r\n\r\n10\r\n\r\n\r\n15\r\n\r\n\r\n3\r\n\r\n\r\n15\r\n\r\n\r\n20\r\n\r\n\r\n2\r\n\r\n\r\n20\r\n\r\n\r\n25\r\n\r\n\r\n1\r\n\r\n\r\n25\r\n\r\n\r\n1000\r\n\r\n\r\n0\r\n\r\n\r\nRun this model on the Winters_hours_gaps dataset using the tempResponse() function.\r\n\r\n\r\nmodels <- list(\r\n  Chilling_Hours = Chilling_Hours,\r\n  Utah_Chill_Units = Utah_Model,\r\n  Chill_Portions = Dynamic_Model,\r\n  GDH = GDH,\r\n  custom = custom)\r\n\r\nresult <- tempResponse(make_JDay(Winters_hours_gaps), \r\n                       Start_JDay = 214, \r\n                       End_JDay = 244, \r\n                       models)\r\n\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\nChilling_Hours\r\n\r\n\r\nUtah_Chill_Units\r\n\r\n\r\nChill_Portions\r\n\r\n\r\nGDH\r\n\r\n\r\ncustom\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n31\r\n\r\n\r\n31\r\n\r\n\r\n100\r\n\r\n\r\n0\r\n\r\n\r\n-569.5\r\n\r\n\r\n0\r\n\r\n\r\n9933.327\r\n\r\n\r\n838\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:24:39+01:00"
    },
    {
      "path": "chill_heat.html",
      "title": "The relative importance of chill and heat",
      "author": [],
      "contents": "\r\nChilling vs. forcing temperatures\r\nIn the last chapter, the mean temperatures during the chilling and forcing phases were analyzed in relation to the bloom dates of ‘Alexander Lucas’ pears.\r\nBloom dates of pear ‘Alexander Lucas’ in Klein-Altendorf, as a function of mean temperatures during the chilling and forcing phasesA similar analysis was conducted for the leaf emergence dates of the walnut cultivar ‘Payne’ in Davis, California.\r\nBloom dates of ‘Payne’ walnuts in Davis, California, as a function of mean temperatures during the chilling and forcing phasesDetecting a clear pattern for pears in Klein-Altendorf was challenging, but the walnut analysis revealed that early leaf emergence occurred with cool endodormancy and warm ecodormancy. This aligns with the idea that dormancy is shortened by abundant chill in endodormancy and heat in ecodormancy. Given previous insights on temperature-chill relationships, the observed temperature response curve is expected. To explore different climates, various similar analyses have been conducted, starting with chestnuts in Beijing:\r\nBloom dates of chestnuts in Beijing, as a function of mean temperatures during the chilling and forcing phases Guo et al., 2013The following plot illustrates the results for cherries in Klein-Altendorf:\r\nBloom dates of cherries ‘Schneiders späte Knorpelkirsche’ in Klein-Altendorf, as a function of mean temperatures during the chilling and forcing phases Luedeling et al., 2013aThe following figure presents the results for apricots in the UK:\r\nBloom dates of apricots in the UK, as a function of mean temperatures during the chilling and forcing phases Martı́nez-Lüscher et al., 2017Patterns in temperature responses\r\nThe general color scheme remains consistent across the plots, with a tendency for the earliest dates to appear in the upper left corner and the latest in the bottom right. However, variations exist, particularly in the slope of the color gradient. While these differences may not be immediately obvious in the previous plots, they become clearer when examining data across a climatic gradient. To investigate tree phenology responses to temperature variations, apricot bloom data from five locations across China were compared.\r\nStudy locations in China used to study phenology responses to temperature across a temperature gradient Guo et al., 2015aThe following visualization shows the distinct winter temperature variations across these five locations:\r\nTemperature profiles of the study locations in China Guo et al., 2015aPhenology data for each location were obtained from the Chinese Phenology database, and similar analyses were conducted as before. However, since different apricot cultivars were used, this introduces a potential source of error. To simplify visualization, the full color surfaces for all five locations are not displayed in a single figure; instead, the following plot presents only the contour lines:\r\nBloom dates of apricots in five locations in China, as a function of mean temperatures during the chilling and forcing phases (Guo et al., 2015a)As temperature increases, the contour lines become steeper. In Jiamusi, the coldest location, the lines are nearly horizontal, while in Guiyang, the warmest site, they appear more diagonal. The other locations follow a gradient between these extremes, with progressively steeper slopes.\r\nThis pattern suggests a possible general trend in the temperature response of temperate trees. The following figure illustrates this temperature response hypothesis, originally presented in grayscale by Guo et al. (2015a):\r\nHypothetical response of temperate tree phenology to temperature during the chilling and forcing periodsAccording to this hypothesis, trees in cold-winter climates primarily respond to temperatures during ecodormancy, as chill accumulation is typically sufficient, making temperature variations during this period negligible. This could explain the difficulty in clearly defining the chilling period in colder regions.\r\nAs endodormancy temperatures rise, chill accumulation begins to have an effect. In the yellow region of the diagram, contour lines start bending upward, indicating that warmer chilling periods now delay bud development, though forcing temperatures still play a dominant role. This delaying effect strengthens as temperatures continue to increase.\r\nAt a certain threshold, chill accumulation conditions become the primary driver, potentially leading to stagnation or even delays in budbreak rather than earlier phenology.\r\nThis remains a hypothesis, requiring further testing. Additional support comes from an analysis of ‘Fuji’ apples at four locations in Shaanxi Province, China. While the temperature gradient here is less pronounced than in the apricot study, there is still variation, and importantly, the same apple cultivar was examined across all four locations.\r\nStudy locations in Shaanxi, China Guo et al., 2019Mean temperatures during the chilling season varied from 2.6°C to 6.0°C, with an overall range of approximately 1°C to 7°C. Contour analysis results suggest that these temperature differences significantly influenced the bloom date response to both chilling and forcing conditions.\r\nBloom date response of ‘Fuji’ apples across four locations in Shaanxi Province, China, to temperatures during the chilling and forcing periods Guo et al., 2019A winter visit to Yan’an, the coldest location, confirmed the extremely low temperatures there. This is reflected in the diagram, where the contour lines remain nearly horizontal. In contrast, warmer sites, particularly Liquan, exhibit a different pattern, with high temperatures during endodormancy causing a noticeable delay in bloom.\r\nThe warm end of the spectrum\r\nAll the temperature response curves examined so far, including those from the full temperature gradient in China, have featured relatively cool conditions. These locations fit well within the proposed temperature response hypothesis. However, the most critical predictions concern the warmest temperature extremes, where a net delay in spring phenology is expected with increasing temperatures.\r\nDelayed phenology is rarely discussed in the scientific literature, where the prevailing expectation is a continuous advancement of bloom and leaf emergence. Interestingly, even key references on phenology advances, such as Parmesan & Yohe (2003) include occasional records suggesting the opposite trend. While these could be measurement errors, they might also indicate an underlying phenomenon.\r\nTesting the warm end of the hypothetical temperature response curve requires phenology data from a very warm region. However, such locations rarely cultivate temperate fruit trees. An exception emerged in 2015 when Haïfa Benmoussafrom Tunisia, working with long-term phenology records for her PhD, provided data from a particularly warm growing region. Some of her results have been presented in earlier chapters, but the most intriguing aspect was the opportunity to analyze contour lines for trees in these warm conditions, potentially validating the temperature response hypothesis.\r\nThe following figure presents the findings for almonds grown in Central Tunisia:\r\nTemperature response of local almond cultivars in Sfax, Tunisia Benmoussa et al., 2017aThe almonds in this study are local cultivars, likely well adapted to the region’s warm temperature conditions. To further explore the effects of temperature, the analysis also includes ‘foreign’ cultivars imported from cooler climates.\r\nTemperature response of pistachios in Sfax, Tunisia, to temperatures during the chilling and forcing phases Benmoussa et al., 2017bThe predicted vertical contour lines are clearly visible in this analysis, aligning with the temperature response hypothesis. However, this does not constitute definitive proof. In science, unlike in mathematics, hypotheses can only be supported or rejected, not conclusively proven. To confidently establish this temperature response pattern, further evidence is needed from a wider range of species and locations—offering many opportunities for further research.\r\nImplications of our hypothesis\r\nIf the temperature response hypothesis is correct, phenology dynamics may not consistently align with the expectation of ever-advancing bloom and leaf emergence. Instead, strong advances should occur up to a certain threshold, beyond which further warming could lead to a slowdown. Eventually, phenology may stagnate, and with continued warming, noticeable delays could emerge. While some studies hint at this pattern, systematic research on the subject remains limited, presenting further opportunities for investigation.\r\nExercises on the relative importance of chill and heat\r\nDescribe the temperature response hypothesis outlined in this chapter.\r\nThe temperature response hypothesis suggests that the impact of temperature on tree phenology is not linear but follows a distinct pattern depending on climate conditions:\r\nCold-Winter Climates: In regions with cold winters, tree phenology is primarily influenced by temperatures during ecodormancy, when buds respond to heat. Chill accumulation is usually sufficient, and temperature variations during this period have little effect.\r\nModerate Warming: As winter temperatures rise, chill accumulation starts to play a more significant role. Warmer chilling periods begin to delay phenology, though forcing temperatures still have the dominant influence. This transition is reflected in contour lines that bend upward in temperature response diagrams.\r\nExcessive Warming – At higher temperatures, chill accumulation becomes the primary limiting factor. The usual expectation of advancing phenology with warming no longer holds. Instead, first stagnation and eventually delays in bloom and leaf emergence may occur, as seen in warmer study locations.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:24:44+01:00"
    },
    {
      "path": "chill_model.html",
      "title": "Chill model comparison",
      "author": [],
      "contents": "\r\nSafe Winter Chill\r\nSafe Winter Chill refers to the 10th percentile of chill distributions, representing the amount of chill exceeded in 90% of years. For growers, it provides a reliable chill requirement that is likely to be met in the majority of years. For instance, if a plant needs 800 Chill Hours, the Safe Winter Chill indicates that in 90% of years, enough chill will accumulate to break dormancy and trigger spring growth.\r\nThis concept is particularly useful for considering long-term trends and climate variability. It focuses on the chill that is consistently met across most years, eliminating the need to account for extreme years or outliers, offering a more dependable measure of how a plant will perform under typical conditions.\r\nChill metrics\r\nOne common method to visualize Safe Winter Chill is through heat maps. These maps allow for easy comparison of different chill models and how chill conditions fluctuate across years and regions. Heat maps provide a clear visual representation of which years meet the chill requirements and which do not, helping track long-term trends.\r\nA set of 13 chill metrics, compiled by Eduardo Fernandez (2020), is used to measure the cold accumulation required by plants. These metrics include various models for measuring chill, each designed to capture different aspects of cold exposure. A well-known tool for working with these metrics is the dormancyR package, available on GitHub.\r\n\r\n\r\n\r\n\r\n\r\nhourly_models <- list(Chilling_units = chilling_units,\r\n     Low_chill = low_chill_model,\r\n     Modified_Utah = modified_utah_model,\r\n     North_Carolina = north_carolina_model,\r\n     Positive_Utah = positive_utah_model,\r\n     Chilling_Hours = Chilling_Hours,\r\n     Utah_Chill_Units = Utah_Model,\r\n     Chill_Portions = Dynamic_Model)\r\n\r\ndaily_models <- list(Rate_of_Chill = rate_of_chill,\r\n                     Chill_Days = chill_days,\r\n                     Exponential_Chill = exponential_chill,\r\n                     # Triangular_Chill_Haninnen = triangular_chill_1,\r\n                     Triangular_Chill_Legave = triangular_chill_2)\r\n\r\nmetrics <- c(names(daily_models),\r\n             names(hourly_models))\r\n\r\nmodel_labels = c(\"Rate of Chill\",\r\n                 \"Chill Days\",\r\n                 \"Exponential Chill\",\r\n                 # \"Triangular Chill (Häninnen)\",\r\n                 \"Triangular Chill (Legave)\",\r\n                 \"Chilling Units\",\r\n                 \"Low-Chill Chill Units\",\r\n                 \"Modified Utah Chill Units\",\r\n                 \"North Carolina Chill Units\",\r\n                 \"Positive Utah Chill Units\",\r\n                 \"Chilling Hours\",\r\n                 \"Utah Chill Units\",\r\n                 \"Chill Portions\")\r\n\r\n\r\n\r\n\r\ndata.frame(Metric = model_labels, 'Function name' = metrics)\r\n\r\n\r\n\r\n\r\nMetric\r\n\r\n\r\nFunction.name\r\n\r\n\r\nRate of Chill\r\n\r\n\r\nRate_of_Chill\r\n\r\n\r\nChill Days\r\n\r\n\r\nChill_Days\r\n\r\n\r\nExponential Chill\r\n\r\n\r\nExponential_Chill\r\n\r\n\r\nTriangular Chill (Legave)\r\n\r\n\r\nTriangular_Chill_Legave\r\n\r\n\r\nChilling Units\r\n\r\n\r\nChilling_units\r\n\r\n\r\nLow-Chill Chill Units\r\n\r\n\r\nLow_chill\r\n\r\n\r\nModified Utah Chill Units\r\n\r\n\r\nModified_Utah\r\n\r\n\r\nNorth Carolina Chill Units\r\n\r\n\r\nNorth_Carolina\r\n\r\n\r\nPositive Utah Chill Units\r\n\r\n\r\nPositive_Utah\r\n\r\n\r\nChilling Hours\r\n\r\n\r\nChilling_Hours\r\n\r\n\r\nUtah Chill Units\r\n\r\n\r\nUtah_Chill_Units\r\n\r\n\r\nChill Portions\r\n\r\n\r\nChill_Portions\r\n\r\n\r\nThe next step involves applying all the chill models to the observed temperature record, as well as to historical and future temperature scenarios for Bonn. Since the temperature data has already been saved, there is no need to repeat the time-intensive processes of weather generation and climate data downloading. Instead, the data can simply be loaded. The past temperature data is loaded first, while future scenarios will be processed later using a loop.\r\n\r\n\r\nBonn_temps <- read_tab(\"data/Bonn_temps.csv\")\r\n\r\nTemps <- load_temperature_scenarios(\"data\",\r\n                                    \"Bonn_hist_scenarios\")\r\n\r\n\r\nThe chill models can now be applied. Some models require hourly temperature data, while others use daily data. Eduardo developed the tempResponse_list_daily function to apply similar procedures to daily data. Unlike the tempResponse_daily_list function from chillR, it doesn’t have a misstolerance parameter, so years with incomplete winter seasons must be manually excluded.\r\n\r\n\r\nStart_JDay <- 305\r\nEnd_JDay <- 59\r\n\r\ndaily_models_past_scenarios <-\r\n  tempResponse_list_daily(Temps,\r\n                          Start_JDay = Start_JDay,\r\n                          End_JDay = End_JDay,\r\n                          models=daily_models)\r\n\r\ndaily_models_past_scenarios <- lapply(\r\n  daily_models_past_scenarios,\r\n  function(x) x[which(x$Perc_complete>90),])\r\n\r\nhourly_models_past_scenarios<-\r\n  tempResponse_daily_list(Temps,\r\n                          latitude = 50.866,\r\n                          Start_JDay = Start_JDay,\r\n                          End_JDay = End_JDay,\r\n                          models = hourly_models,\r\n                          misstolerance = 10)\r\n\r\npast_scenarios <- daily_models_past_scenarios\r\n\r\n\r\npast_scenarios <- lapply(\r\n  names(past_scenarios),\r\n  function(x)\r\n    cbind(past_scenarios[[x]],\r\n          hourly_models_past_scenarios[[x]][,names(hourly_models)]))\r\n\r\nnames(past_scenarios) <- names(daily_models_past_scenarios)\r\n\r\ndaily_models_observed <-\r\n  tempResponse_daily(Bonn_temps,\r\n                     Start_JDay = Start_JDay,\r\n                     End_JDay = End_JDay,\r\n                     models = daily_models)\r\n\r\n\r\n\r\ndaily_models_observed <-\r\n  daily_models_observed[which(daily_models_observed$Perc_complete>90),]\r\n\r\nhourly_models_observed <-\r\n  tempResponse_daily_list(Bonn_temps,\r\n                          latitude= 50.866,\r\n                          Start_JDay = Start_JDay,\r\n                          End_JDay = End_JDay,\r\n                          models = hourly_models,\r\n                          misstolerance = 10)\r\n\r\npast_observed <- cbind(\r\n  daily_models_observed,\r\n  hourly_models_observed[[1]][,names(hourly_models)])\r\n\r\nsave_temperature_scenarios(past_scenarios,\r\n                           \"data/future_climate\",\r\n                           \"Bonn_multichill_305_59_historic\")\r\nwrite.csv(past_observed,\r\n          \"data/future_climate/Bonn_multichill_305_59_observed.csv\",\r\n          row.names=FALSE)\r\n\r\n\r\nThe same procedures are now applied to the future data. Given the presence of eight scenarios, it is efficient to implement this process within a loop.\r\n\r\n\r\nSSPs <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\n\r\nlist_ssp <-\r\n  strsplit(names(future_temps), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(future_temps), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(future_temps), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n  {\r\n    Temps <- future_temps[list_ssp == SSP & list_time == Time]\r\n    names(Temps) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    daily_models_future_scenarios <- tempResponse_list_daily(\r\n      Temps,\r\n      Start_JDay = Start_JDay,\r\n      End_JDay = End_JDay,\r\n      models = daily_models)\r\n    daily_models_future_scenarios<-lapply(\r\n      daily_models_future_scenarios,\r\n      function(x) x[which(x$Perc_complete>90),])\r\n    hourly_models_future_scenarios<-\r\n      tempResponse_daily_list(\r\n        Temps,\r\n        latitude = 50.866,\r\n        Start_JDay = Start_JDay,\r\n        End_JDay = End_JDay,\r\n        models=hourly_models,\r\n        misstolerance = 10)\r\n    \r\n    future_scenarios <- daily_models_future_scenarios\r\n    \r\n    future_scenarios <- lapply(\r\n      names(future_scenarios),\r\n      function(x)\r\n        cbind(future_scenarios[[x]],\r\n              hourly_models_future_scenarios[[x]][,names(hourly_models)]))\r\n    names(future_scenarios)<-names(daily_models_future_scenarios)\r\n    \r\n    chill<-future_scenarios\r\n    \r\n    save_temperature_scenarios(\r\n      chill,\r\n      \"data/future_climate\",\r\n      paste0(\"Bonn_multichill_305_59_\",Time,\"_\",SSP))\r\n  }\r\n\r\n\r\nAll the scenarios for processing and plotting have now been saved to disk. The next step is to load these scenarios and generate the corresponding chill projections.\r\n\r\n\r\nchill_past_scenarios <- load_temperature_scenarios(\r\n  \"data/future_climate\",\r\n  \"Bonn_multichill_305_59_historic\")\r\n\r\nchill_observed <- \r\n  read_tab(\"data/future_climate/Bonn_multichill_305_59_observed.csv\")\r\n\r\n\r\nchills <- make_climate_scenario(chill_past_scenarios,\r\n                                caption = \"Historic\",\r\n                                historic_data = chill_observed,\r\n                                time_series = TRUE)\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n  {\r\n    chill <- load_temperature_scenarios(\r\n      \"data/future_climate\",\r\n      paste0(\"Bonn_multichill_305_59_\",Time,\"_\",SSP))\r\n    if(SSP == \"ssp126\") SSPcaption <- \"SSP1\"\r\n    if(SSP == \"ssp245\") SSPcaption <- \"SSP2\"\r\n    if(SSP == \"ssp370\") SSPcaption <- \"SSP3\"    \r\n    if(SSP == \"ssp585\") SSPcaption <- \"SSP5\"    \r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- make_climate_scenario(chill,\r\n                                    caption = c(SSPcaption,\r\n                                                Time_caption),\r\n                                    add_to = chills)\r\n  }\r\n\r\n\r\nTo visualize Safe Winter Chill, a heatmap will be created using ggplot2 functions. This involves reshaping the data into a long-format data.frame. The 10% quantiles are also calculated from the distributions. Since the chill models use different units and have varying numeric values, all results are normalized by expressing them as changes relative to a 1980 baseline.\r\n\r\n\r\nfor(i in 1:length(chills))\r\n   {ch <- chills[[i]]\r\n   if(ch$caption[1] == \"Historic\")\r\n     {GCMs <- rep(\"none\",length(names(ch$data)))\r\n      SSPs <- rep(\"none\",length(names(ch$data)))\r\n      Years <- as.numeric(ch$labels)\r\n      Scenario <- rep(\"Historic\",\r\n                      length(names(ch$data)))} else\r\n                        {GCMs <- names(ch$data)\r\n                        SSPs <- rep(ch$caption[1],\r\n                                    length(names(ch$data)))\r\n                        Years <- rep(as.numeric(ch$caption[2]),\r\n                                     length(names(ch$data)))\r\n                        Scenario <- rep(\"Future\",\r\n                                        length(names(ch$data)))}\r\n   for(nam in names(ch$data))\r\n     {for(met in metrics)\r\n       {temp_res <-\r\n         data.frame(Metric = met,\r\n                    GCM = GCMs[which(nam == names(ch$data))],\r\n                    SSP = SSPs[which(nam == names(ch$data))],\r\n                    Year = Years[which(nam == names(ch$data))],\r\n                    Result = quantile(ch$data[[nam]][,met],0.1), \r\n                    Scenario = Scenario[which(nam == names(ch$data))])\r\n       if(i == 1 & nam == names(ch$data)[1] & met == metrics[1])\r\n         results <- temp_res else\r\n           results <- rbind(results,\r\n                            temp_res)\r\n         }\r\n     }\r\n   }\r\n\r\nfor(met in metrics)\r\n  results[which(results$Metric == met),\"SWC\"] <-\r\n    results[which(results$Metric == met),\"Result\"]/\r\n      results[which(results$Metric == met & results$Year == 1980),\r\n              \"Result\"]-1\r\n\r\n\r\nThe results are now ready for plotting. To ensure consistency across the plot’s panels, the full range of Safe Winter Chill change values must be captured. Once the range is established, the first heatmap plot can be created, focusing on future scenarios (excluding cases where the GCM is set to “none”). In the plot aesthetics, the factor function is used for the y-axis to ensure the metrics are displayed in the order specified by the levels parameter.\r\n\r\n\r\nrng = range(results$SWC)\r\n\r\np_future <- ggplot(results[which(!results$GCM == \"none\"),],\r\n                   aes(GCM,\r\n                       y = factor(Metric,\r\n                                  levels = metrics),\r\n                       fill = SWC)) +\r\n  geom_tile()\r\n\r\np_future\r\n\r\n\r\n\r\nThe current plot combines data for both SSPs and time points. To improve clarity, these aspects can be displayed in separate facets, allowing for a more detailed visualization.\r\n\r\n\r\np_future <-\r\n  p_future +\r\n  facet_grid(SSP ~ Year) \r\n\r\np_future\r\n\r\n\r\n\r\nThe design work begins by selecting a black-and-white color scheme and reducing the size of the axis text. This is done to accommodate the large amount of text on the axes, which would be difficult to display at full size.\r\n\r\n\r\np_future <-\r\n  p_future +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.text = element_text(size=6))\r\n\r\np_future\r\n\r\n\r\n\r\nThe current color scheme makes it difficult to distinguish subtle differences, so a red-to-blue color ramp will be used instead, with several intermediate values. The colorRamps library offers a variety of attractive schemes, including the matlab.like color ramp, from which 15 equally spaced values will be selected. The changes will be displayed in percentage terms, and the scale limits will be set to the full range of the data determined earlier.\r\n\r\n\r\np_future <-\r\n  p_future +\r\n  scale_fill_gradientn(colours = matlab.like(15),\r\n                       labels = scales::percent,\r\n                       limits = rng)\r\n\r\np_future\r\n\r\n\r\n\r\nA few additional cosmetic adjustments are needed. The angle of the x-axis labels will be changed to 75° to improve readability. A more descriptive title will be added to the legend (with \\n for line breaks). The automatic axis labels will be replaced with the customized labels designed earlier, and the y-axis label will be updated to “Chill metric” for clarity.\r\n\r\n\r\np_future <-\r\n  p_future  +\r\n  theme(axis.text.x = element_text(angle = 75, \r\n                                   hjust = 1,\r\n                                   vjust = 1)) +\r\n  labs(fill = \"Change in\\nSafe Winter Chill\\nsince 1980\") +\r\n  scale_y_discrete(labels = model_labels) +\r\n  ylab(\"Chill metric\")\r\n\r\np_future\r\n\r\n\r\n\r\nThe plot for the past scenarios can now be created. The procedure is mostly the same as for the future scenarios, so the details will not be repeated. One key difference is that the x-axis will be moved to the top using the scale_x_continuous(position = \"top\") command.\r\n\r\n\r\np_past<-\r\n  ggplot(results[which(results$GCM == \"none\"),],\r\n         aes(Year,\r\n             y = factor(Metric, \r\n                        levels=metrics),\r\n             fill = SWC)) +\r\n  geom_tile()\r\n\r\np_past<-\r\n  p_past +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.text = element_text(size = 6))\r\n\r\np_past<-\r\n  p_past +\r\n  scale_fill_gradientn(colours = matlab.like(15),\r\n                       labels = scales::percent,\r\n                       limits = rng)\r\n\r\np_past<-\r\n  p_past +\r\n  scale_x_continuous(position = \"top\") \r\n\r\np_past<-\r\n  p_past +\r\n  labs(fill = \"Change in\\nSafe Winter Chill\\nsince 1980\") +\r\n  scale_y_discrete(labels = model_labels) +\r\n  ylab(\"Chill metric\")\r\n\r\np_past\r\n\r\n\r\n\r\nThe plots can now be combined using the patchwork package. The layout differs slightly from before, with the plots stacked vertically (nrow = 2), and relative heights specified by heights = c(1,2). Changes to the facet strip are applied at this stage. This is convenient because it allows the adjustment to be made once, rather than for each subplot individually.\r\n\r\n\r\nchill_comp_plot<-\r\n  (p_past +\r\n     p_future +\r\n     plot_layout(guides = \"collect\",\r\n                 nrow = 2,\r\n                 heights = c(1,3))) &\r\n  theme(legend.position = \"right\",\r\n        strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\"))\r\n\r\nchill_comp_plot\r\n\r\n\r\n\r\nThis is the final result. Notable differences in the behavior of the various models can be observed. Some models suggest that Safe Winter Chill is increasing, while others indicate a decrease. This highlights the importance of model choice in the analysis.\r\nAn animated plot of relative changes in Safe Winter Chill\r\nFinally, the gganimate package will be explored to add an interesting feature to the figures created with ggplot2. To use this, a simpler representation of the Safe Winter Chill data will be produced by summarizing the results across all GCMs. Additionally, the historical data will be duplicated, with one copy assigned to each SSP scenario. This is necessary for easily plotting the full time series, broken down by SSP.\r\n\r\n\r\nhist_results <- results[which(results$GCM == \"none\"),]\r\nhist_results$SSP <- \"SSP1\"\r\nhist_results_2 <- hist_results\r\nhist_results_2$SSP <- \"SSP2\"\r\nhist_results_3 <- hist_results\r\nhist_results_3$SSP <- \"SSP3\"\r\nhist_results_4 <- hist_results\r\nhist_results_4$SSP <- \"SSP5\"\r\nhist_results <- rbind(hist_results,\r\n                      hist_results_2,\r\n                      hist_results_3,\r\n                      hist_results_4)\r\n\r\nfuture_results <- results[which(!results$GCM == \"none\"),]\r\n\r\nGCM_aggregate <- aggregate(\r\n  future_results$SWC,\r\n  by=list(future_results$Metric,\r\n          future_results$SSP,\r\n          future_results$Year),\r\n  FUN=mean)\r\n\r\ncolnames(GCM_aggregate) <- c(\"Metric\",\r\n                             \"SSP\",\r\n                             \"Year\",\r\n                             \"SWC\")\r\n\r\nSSP_Time_series<-rbind(hist_results[,c(\"Metric\",\r\n                                       \"SSP\",\r\n                                       \"Year\",\r\n                                       \"SWC\")],\r\n                       GCM_aggregate)\r\n\r\n\r\nThe next step is to create a line plot showing changes in Safe Winter Chill over the two SSP scenarios for all chill metrics. This procedure closely follows the previous steps, with the main difference being the use of the geom_line command instead of geom_tile. There are also a few other minor adjustments.\r\n\r\n\r\nSSP_Time_series$Year <- as.numeric(SSP_Time_series$Year)\r\n\r\nchill_change_plot<-\r\n  ggplot(data = SSP_Time_series,\r\n         aes(x = Year,\r\n             y = SWC,\r\n             col = factor(Metric,\r\n                          levels = metrics))) +\r\n  geom_line(lwd = 1.3) +\r\n  facet_wrap(~SSP,\r\n             nrow = 4) +\r\n  theme_bw(base_size = 10) +\r\n  labs(col = \"Change in\\nSafe Winter Chill\\nsince 1980\") +\r\n  scale_color_discrete(labels = model_labels) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  theme(strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\")) +\r\n  ylab(\"Safe Winter Chill\")\r\n\r\nchill_change_plot\r\n\r\n\r\n\r\nAs the final step, the plot will be converted into a moving picture using the transition_reveal function from the gganimate package. The resulting animation will then be saved.\r\n\r\n\r\nccp <-chill_change_plot +\r\n  transition_reveal(Year)\r\n\r\nanimate(ccp, fps = 10)\r\n\r\nanim_save(\"data/chill_comparison_animation.gif\",\r\n          animation = last_animation())\r\n\r\n\r\n\r\nThe animation illustrates the development of chill over time, relative to the 1980 baseline, and evaluated using 13 chill models.\r\nExercises on chill model comparison\r\nPerform a similar analysis for the location you’ve chosen for your exercises.\r\n\r\n\r\n# Load temperature data for Yakima\r\nYakima_temps <- read_tab(\"Yakima/Yakima_temps.csv\")\r\n\r\nTemps <- load_temperature_scenarios(\"Yakima\",\r\n                                    \"Yakima_hist_scenarios\")\r\n\r\n\r\n\r\n\r\n# Remove all years with incomplete winter seasons in past scenarios \r\nStart_JDay <- 305\r\nEnd_JDay <- 59\r\n\r\ndaily_models_past_scenarios <-\r\n  tempResponse_list_daily(Temps,\r\n                          Start_JDay = Start_JDay,\r\n                          End_JDay = End_JDay,\r\n                          models=daily_models)\r\n\r\ndaily_models_past_scenarios <- lapply(\r\n  daily_models_past_scenarios,\r\n  function(x) x[which(x$Perc_complete>90),])\r\n\r\nhourly_models_past_scenarios<-\r\n  tempResponse_daily_list(Temps,\r\n                          latitude = 46.6,\r\n                          Start_JDay = Start_JDay,\r\n                          End_JDay = End_JDay,\r\n                          models = hourly_models,\r\n                          misstolerance = 10)\r\n\r\npast_scenarios <- daily_models_past_scenarios\r\n\r\n\r\npast_scenarios <- lapply(\r\n  names(past_scenarios),\r\n  function(x)\r\n    cbind(past_scenarios[[x]],\r\n          hourly_models_past_scenarios[[x]][,names(hourly_models)]))\r\n\r\nnames(past_scenarios) <- names(daily_models_past_scenarios)\r\n\r\ndaily_models_observed <-\r\n  tempResponse_daily(Yakima_temps,\r\n                     Start_JDay = Start_JDay,\r\n                     End_JDay = End_JDay,\r\n                     models = daily_models)\r\n\r\n\r\n\r\ndaily_models_observed <-\r\n  daily_models_observed[which(daily_models_observed$Perc_complete>90),]\r\n\r\nhourly_models_observed <-\r\n  tempResponse_daily_list(Yakima_temps,\r\n                          latitude= 46.6,\r\n                          Start_JDay = Start_JDay,\r\n                          End_JDay = End_JDay,\r\n                          models = hourly_models,\r\n                          misstolerance = 10)\r\n\r\npast_observed <- cbind(\r\n  daily_models_observed,\r\n  hourly_models_observed[[1]][,names(hourly_models)])\r\n\r\nsave_temperature_scenarios(past_scenarios,\r\n                           \"Yakima/future_climate\",\r\n                           \"Yakima_multichill_305_59_historic\")\r\nwrite.csv(past_observed,\r\n          \"Yakima/future_climate/Yakima_multichill_305_59_observed.csv\",\r\n          row.names=FALSE)\r\n\r\n\r\n\r\n\r\n# Applying the same procedures to the future data\r\nSSPs <- c(\"ssp126\", \"ssp245\",\"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\n\r\nlist_ssp <- \r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\n\r\nfuture_temps <- load_temperature_scenarios(\"Yakima/future_climate\",\"Yakima_futuretemps\")\r\n\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n    {\r\n    Temps <- future_temps[list_ssp == SSP & list_time == Time]\r\n    names(Temps) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    daily_models_future_scenarios <- tempResponse_list_daily(\r\n      Temps,\r\n      Start_JDay = Start_JDay,\r\n      End_JDay = End_JDay,\r\n      models = daily_models)\r\n    daily_models_future_scenarios<-lapply(\r\n      daily_models_future_scenarios,\r\n      function(x) x[which(x$Perc_complete>90),])\r\n    hourly_models_future_scenarios<-\r\n      tempResponse_daily_list(\r\n        Temps,\r\n        latitude = 50.866,\r\n        Start_JDay = Start_JDay,\r\n        End_JDay = End_JDay,\r\n        models=hourly_models,\r\n        misstolerance = 10)\r\n\r\n    future_scenarios <- daily_models_future_scenarios\r\n    \r\n    future_scenarios <- lapply(\r\n      names(future_scenarios),\r\n      function(x)\r\n        cbind(future_scenarios[[x]],\r\n              hourly_models_future_scenarios[[x]][,names(hourly_models)]))\r\n    names(future_scenarios)<-names(daily_models_future_scenarios)\r\n    \r\n    chill<-future_scenarios\r\n    \r\n    save_temperature_scenarios(\r\n      chill,\r\n      \"Yakima/future_climate\",\r\n      paste0(\"Yakima_multichill_305_59_\",Time,\"_\",SSP))\r\n}\r\n\r\n\r\n\r\n\r\n# Load all scenarios and produce chill scenarios \r\nchill_past_scenarios <- load_temperature_scenarios(\r\n  \"Yakima/future_climate\",\r\n  \"Yakima_multichill_305_59_historic\")\r\n\r\nchill_observed <- \r\n  read_tab(\"Yakima/future_climate/Yakima_multichill_305_59_observed.csv\")\r\n\r\n\r\nchills <- make_climate_scenario(chill_past_scenarios,\r\n                                caption = \"Historic\",\r\n                                historic_data = chill_observed,\r\n                                time_series = TRUE)\r\n\r\n\r\n\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n    {\r\n    chill <- load_temperature_scenarios(\r\n      \"Yakima/future_climate\",\r\n      paste0(\"Yakima_multichill_305_59_\",Time,\"_\",SSP))\r\n    if(SSP == \"ssp126\") SSPcaption <- \"SSP1\"\r\n    if(SSP == \"ssp245\") SSPcaption <- \"SSP2\"\r\n    if(SSP == \"ssp370\") SSPcaption <- \"SSP3\"\r\n    if(SSP == \"ssp585\") SSPcaption <- \"SSP5\"    \r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- make_climate_scenario(chill,\r\n                                    caption = c(SSPcaption,\r\n                                                Time_caption),\r\n                                    add_to = chills)\r\n}\r\n\r\n\r\n\r\n\r\n# Reorganize the data into a long data.frame and normalize all results \r\nfor(i in 1:length(chills))\r\n   {ch <- chills[[i]]\r\n   if(ch$caption[1] == \"Historic\")\r\n     {GCMs <- rep(\"none\",length(names(ch$data)))\r\n      SSPs <- rep(\"none\",length(names(ch$data)))\r\n      Years <- as.numeric(ch$labels)\r\n      Scenario <- rep(\"Historic\",\r\n                      length(names(ch$data)))} else\r\n                        {GCMs <- names(ch$data)\r\n                        SSPs <- rep(ch$caption[1],\r\n                                    length(names(ch$data)))\r\n                        Years <- rep(as.numeric(ch$caption[2]),\r\n                                     length(names(ch$data)))\r\n                        Scenario <- rep(\"Future\",\r\n                                        length(names(ch$data)))}\r\n   for(nam in names(ch$data))\r\n     {for(met in metrics)\r\n       {temp_res <-\r\n         data.frame(Metric = met,\r\n                    GCM = GCMs[which(nam == names(ch$data))],\r\n                    SSP = SSPs[which(nam == names(ch$data))],\r\n                    Year = Years[which(nam == names(ch$data))],\r\n                    Result = quantile(ch$data[[nam]][,met],0.1), \r\n                    Scenario = Scenario[which(nam == names(ch$data))])\r\n       if(i == 1 & nam == names(ch$data)[1] & met == metrics[1])\r\n         results <- temp_res else\r\n           results <- rbind(results,\r\n                            temp_res)\r\n         }\r\n     }\r\n   }\r\n\r\nfor(met in metrics)\r\n  results[which(results$Metric == met),\"SWC\"] <-\r\n    results[which(results$Metric == met),\"Result\"]/\r\n      results[which(results$Metric == met & results$Year == 1980),\r\n              \"Result\"]-1\r\n\r\n\r\nMake a heat map illustrating past and future changes in Safe Winter Chill, relative to a past scenario, for the 13 chill models used here.\r\n\r\n\r\n# Plot future changes in Safe Winter Chill\r\nrng <- range(results$SWC)\r\n\r\np_future <- ggplot(results[results$GCM != \"none\", ], \r\n                   aes(x = GCM, \r\n                       y = factor(Metric, levels = metrics), \r\n                       fill = SWC)) +\r\n  geom_tile() +\r\n  facet_grid(SSP ~ Year) +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.text = element_text(size = 6),\r\n        axis.text.x = element_text(angle = 75, hjust = 1, vjust = 1)) +\r\n  scale_fill_gradientn(colours = matlab.like(15), \r\n                       labels = scales::percent, \r\n                       limits = rng) +\r\n  labs(fill = \"Change in\\nSafe Winter Chill\\nsince 1980\", \r\n       y = \"Chill metric\") +\r\n  scale_y_discrete(labels = model_labels)\r\n\r\np_future\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Plot past changes in Safe Winter Chill  \r\np_past <- ggplot(results[results$GCM == \"none\", ], \r\n                 aes(x = Year, \r\n                     y = factor(Metric, levels = metrics), \r\n                     fill = SWC)) +\r\n  geom_tile() +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.text = element_text(size = 6)) +\r\n  scale_fill_gradientn(colours = matlab.like(15), \r\n                       labels = scales::percent, \r\n                       limits = rng) +\r\n  scale_x_continuous(position = \"top\") +\r\n  labs(fill = \"Change in\\nSafe Winter Chill\\nsince 1980\", \r\n       y = \"Chill metric\") +\r\n  scale_y_discrete(labels = model_labels)\r\n\r\np_past\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Plot heat map with past and future changes in Safe Winter Chill\r\nchill_comp_plot<-\r\n  (p_past +\r\n     p_future +\r\n     plot_layout(guides = \"collect\",\r\n                 nrow = 2,\r\n                 heights = c(1,3))) &\r\n  theme(legend.position = \"right\",\r\n        strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\"))\r\n\r\nchill_comp_plot\r\n\r\n\r\n\r\n\r\n\r\nProduce an animated line plot of your results (summarizing Safe Winter Chill across all the GCMs).\r\n\r\n\r\nhist_results <- results[which(results$GCM == \"none\"),]\r\nhist_results$SSP <- \"SSP1\"\r\nhist_results_2 <- hist_results\r\nhist_results_2$SSP <- \"SSP2\"\r\nhist_results_3 <- hist_results\r\nhist_results_3$SSP <- \"SSP3\"\r\nhist_results_4 <- hist_results\r\nhist_results_4$SSP <- \"SSP5\"\r\nhist_results <- rbind(hist_results,\r\n                      hist_results_2,\r\n                      hist_results_3,\r\n                      hist_results_4)\r\n\r\nfuture_results <- results[which(!results$GCM == \"none\"),]\r\n\r\nGCM_aggregate <- aggregate(\r\n  future_results$SWC,\r\n  by=list(future_results$Metric,\r\n          future_results$SSP,\r\n          future_results$Year),\r\n  FUN=mean)\r\n\r\ncolnames(GCM_aggregate) <- c(\"Metric\",\r\n                             \"SSP\",\r\n                             \"Year\",\r\n                             \"SWC\")\r\n\r\nSSP_Time_series<-rbind(hist_results[,c(\"Metric\",\r\n                                       \"SSP\",\r\n                                       \"Year\",\r\n                                       \"SWC\")],\r\n                       GCM_aggregate)\r\n\r\n\r\n\r\n\r\nSSP_Time_series$Year <- as.numeric(SSP_Time_series$Year)\r\n\r\nchill_change_plot<-\r\n  ggplot(data = SSP_Time_series,\r\n         aes(x = Year,\r\n             y = SWC,\r\n             col = factor(Metric,\r\n                          levels = metrics))) +\r\n  geom_line(lwd = 1.3) +\r\n  facet_wrap(~SSP,\r\n             nrow = 4) +\r\n  theme_bw(base_size = 10) +\r\n  labs(col = \"Change in\\nSafe Winter Chill\\nsince 1980\") +\r\n  scale_color_discrete(labels = model_labels) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  theme(strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\")) +\r\n  ylab(\"Safe Winter Chill\")\r\n\r\n\r\n\r\n\r\nccp <-chill_change_plot +\r\n  transition_reveal(Year)\r\n\r\nanimate(ccp, fps = 10)\r\n\r\nanim_save(\"Yakima/chill_comparison_animation.gif\",\r\n          animation = last_animation())\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:25:52+01:00"
    },
    {
      "path": "climate_change.html",
      "title": "Climate Change and impact projection",
      "author": [],
      "contents": "\r\nClimate change refers to long-term changes in temperatures and weather patterns. While these changes can occur naturally — such as fluctuations in solar activity — since the 19th century, climate change has primarily been driven by human activities, particularly the burning of fossil fuels like coal, oil, and natural gas.\r\nBefore using chillR, there’s a brief overview of climate change, because the upcoming work will mainly focus on predicting how global warming might affect phenology-related metrics.\r\nThe drivers of climate change\r\nTo understand what’s happening to our planet, it’s important to know the main causes of climate change. This helps us spot false claims that things like the sun, cities, or natural changes in the climate are the main reasons for global warming. The truth is simple: human-made greenhouse gas emissions are heating up our planet, and the only way to stop this is to greatly reduce these emissions.\r\nThe video below, titled Climate Change 1 - Drivers of Climate Change, is the first in a series of four videos on the topic of climate change presented by Eike Lüdeling. It provides a comprehensive overview of the primary drivers of global climate change, such as greenhouse gases, aerosols, solar radiation, ozone, and others.\r\n\r\n\r\n\r\n\r\nWhat’s already known\r\nThe next video, Climate Change 2 - Recent Warming, explores climatic changes that have already occurred or for which there is substantial evidence. It demonstrates that the planet has experienced significant warming for several decades, almost globally.\r\n\r\n\r\n\r\n\r\nFuture scenarios\r\nWhen it comes to climate change, the most severe impacts are still ahead. This is largely due to the significantly higher rate of greenhouse gas emissions observed over the past few decades, with no signs of a slowdown in the near future. As a result, the human-induced ‘forcing’ effect on our climate has reached unprecedented levels, making it likely that future changes will occur even more rapidly than those we have already witnessed. The next video Climate Change 3 - Future scenarios introduces the methods that climate scientists employ to forecast future conditions and presents climate scenarios developed by these scientists, which researchers in other fields can use to project the impacts of climate change on ecological and agricultural systems.\r\n\r\n\r\n\r\n\r\nImpact projections approaches\r\nHaving robust climate scenarios is essential, but they only take us partway toward reliable assessments of climate change impacts. A potentially greater challenge lies in translating these climate scenarios into biological consequences. To achieve this, we need impact models or other methods to derive the impacts of climate change. The last video Climate change 4 - impact projection approaches introduces various methods for projecting climate impacts.\r\n\r\n\r\n\r\n\r\nExercises on climate change\r\nList the main drivers of climate change at the decade to century scale, and briefly explain the mechanism through which the currently most important driver affects our climate.\r\nThe main drivers of climate change on a decade-to-century scale include:\r\nGreenhouse Gases (GHGs): GHGs like carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N₂O) trap heat in the atmosphere, leading to the greenhouse effect, which raises Earth’s temperature. The increase in these gases is primarily due to human activities, such as burning fossil fuels, industrial processes, and deforestation\r\nAerosols: Particles in the atmosphere that can cool the climate by reflecting sunlight. They come from both natural sources (e.g. sea salt, dust, volcanic eruptions, fires) and human activities (e.g.power plants, cars, fires and cook stove). They are major climate driver in industrial centers (e.g. China)\r\nSun: Solar radiation heats the Earth, with minor fluctuations occurring over time due to cycles in solar activity, such as sunspots. Although these variations contribute only a small portion to the current climate changes, they play a significant role in driving climate change over geological timescales\r\nOzone: Ozone in the stratosphere protects Earth from UV-B radiation, while tropospheric ozone acts as a greenhouse gas and contributes to warming\r\nSurface albedo: The reflectivity of the Earth’s surface affects how much solar energy is absorbed. Light surfaces (like ice) reflect more energy, while dark surfaces (like forests or oceans) absorb more, influencing the planet’s heat balance. Changes in surface reflectivity, such as melting ice and snow, decrease the albedo effect, leading to more heat absorption and further warming\r\nThe currently most important driver of climate change is greenhouse gases, particularly CO₂. The mechanism through which CO₂ affects the climate involves the greenhouse effect: CO₂ molecules in the atmosphere absorb long-wave radiation emitted from the Earth’s surface and re-radiate it in all directions, including back toward the surface. This process traps heat and increases global temperatures, driving many of the changes we observe in climate patterns.\r\nExplain briefly what is special about temperature dynamics of recent decades, and why we have good reasons to be concerned.\r\nIn recent decades, global temperatures have been rising at a faster rate than at any other time in human history. This trend is evident from the fact that the hottest years on record have all occurred within the last few decades. One striking example is the extreme heat in Siberia in the spring of 2020, where temperatures were up to 8°C above the recent average. This trend is particularly concerning because it is mainly driven by human activities, especially the emission of greenhouse gases. Unlike previous climate changes, which took place slowly over long periods, today’s fast rise in temperatures increases the risk of triggering dangerous effects, like melting permafrost and losing ice cover, which could make global warming even worse. Even a small increase of 1.5°C could seriously upset the balance of our climate, showing how important it is to take action against these human-caused changes.\r\nWhat does the abbreviation ‘RCP’ stand for, how are RCPs defined, and what is their role in projecting future climates?\r\nRCP stands for Representative Concentration Pathways, which are essential scenarios used in climate modeling to project potential future greenhouse gas emissions and their impacts on the climate. RCPs are defined by the level of radiative forcing — measured in watts per square meter (W/m²) — that is expected by the end of the 21st century. Each pathway corresponds to a specific amount of greenhouse gas concentrations, which can significantly influence global temperatures. The role of RCPs is to serve as inputs for climate models, helping to produce future climate scenarios, which are essential for understanding the potential impacts of climate change and planning appropriate mitigation and adaptation strategies.\r\nBriefly describe the 4 climate impact projection methods described in the fourth video.\r\nThe four climate impact projection methods described in the fourth video are:\r\nStatistical models: These models establish relationships between climate parameters and impact measures, such as crop yield. They use historical data to explain past trends and project future climate impacts. Their primary limitation is that the statistical relationships may not remain valid under future climate conditions, and they may overlook important factors\r\nSpecies Distribution Modeling: Also known as ecological niche modeling, this method predicts the future distribution of species by relating current presence or absence data to climatic parameters. However, these models may assume species are in equilibrium with the climate, which is often not the case\r\nProcess based models: These models aim to represent all major system processes using equations, capturing the scientific knowledge of processes like crop growth, phenology or hydrology. However, they are limited by the lack of complete understanding of complex systems, and often require extensive parameterization or assumptions\r\nClimate Analogue models: This method identifies current locations with climates similar to those expected in the future at another site, offering real-world examples that can guide adaptation strategies. However, they may be limited by differences in non-climatic factors and lack of suitable data, making it difficult to draw clear conditions\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:25:58+01:00"
    },
    {
      "path": "cmip5.html",
      "title": "Making CMIP5 scenarios with the ClimateWizard",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nUsing ClimateWizard\r\nThe ClimateWizard provides access to future climate data from CMIP5 models for the RCP 4.5 and RCP 8.5 scenarios. Since the service is occasionally unavailable, malfunctions may occur. The following example shows how to retrieve data for a specific area:\r\n\r\n\r\ngetClimateWizardData(coordinates = c(longitude = 10.61, \r\n                                     latitude = 34.93),\r\n                     scenario = \"rcp45\",\r\n                     start_year = 2020,\r\n                     end_year = 2050,\r\n                     metric = c(\"CD18\", \"R02\"),\r\n                     GCMs = c(\"bcc-csm1-1\", \"BNU-ESM\"))\r\n\r\n\r\nThis code retrieves data for two climate models, an RCP scenario, and two climate metrics: cooling degree days above 18°C (“CD18”) and annual precipitation days over 0.2 mm/day (“R02”).\r\nAdaptation for Bonn\r\nInstead of the sample coordinates in Tunisia, data for Bonn should be used. All available climate models and both RCP scenarios (4.5 and 8.5) are considered. The focus is on minimum and maximum temperatures (monthly_min_max_temps). Since ClimateWizard cannot automatically download multiple scenarios, a loop is used:\r\n\r\n\r\nRCPs <- c(\"rcp45\", \r\n          \"rcp85\")\r\n\r\nTimes <- c(2050, \r\n           2085)\r\n\r\nfor(RCP in RCPs)\r\n  for(Time in Times) {\r\n    clim_scen <- getClimateWizardData(\r\n      coordinates = c(longitude = 7.143, \r\n                      latitude = 50.866),\r\n      scenario = RCP,\r\n      start_year = Time - 15,\r\n      end_year = Time + 15,\r\n      metric = \"monthly_min_max_temps\",\r\n      GCMs = \"all\")\r\n    save_temperature_scenarios(clim_scen, \"data/ClimateWizard\", paste0(\"Bonn_futures_\", Time, \"_\", RCP))\r\n  }\r\n\r\n\r\nBaseline Adjustment\r\nThe ClimateWizard database requires a 20-year baseline period between 1950 and 2005. Since weather data for Bonn is available from 1973 to 2019, the period 1975–2005 is chosen to allow a median year adjustment to 1990. This reflects current climate trends and meets database requirements.\r\nSince the observed weather data require a median year adjustment to 1996, a correction is applied:\r\n\r\n\r\nscenario_1990 <- Bonn_temps %>% temperature_scenario_from_records(1990)\r\nscenario_1996 <- Bonn_temps %>% temperature_scenario_from_records(1996)\r\n\r\nadjustment_scenario <- temperature_scenario_baseline_adjustment(\r\n  scenario_1996, \r\n  scenario_1990)\r\n\r\n\r\nTemperature Generation and Storage\r\nTo avoid repeated time-consuming data queries, the adjusted scenarios are saved.\r\n\r\n\r\nfor(RCP in RCPs)\r\n  for(Time in Times)\r\n  {\r\n    clim_scen <- load_ClimateWizard_scenarios(\r\n      \"data/climateWizard\",\r\n      paste0(\"Bonn_futures_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n    \r\n    clim_scen_adjusted <-\r\n      temperature_scenario_baseline_adjustment(\r\n        baseline_temperature_scenario = adjustment_scenario,\r\n        temperature_scenario = clim_scen)\r\n    \r\n    Temps <- temperature_generation(\r\n      weather = Bonn_temps, \r\n      years = c(1973,\r\n                2019),\r\n      sim_years = c(2001,\r\n                    2101),\r\n      temperature_scenario = clim_scen_adjusted)\r\n    \r\n    save_temperature_scenarios(\r\n      Temps,\r\n      \"data/Weather_ClimateWizard\",\r\n      paste0(\"Bonn_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n  }\r\n\r\n\r\nCreation of Historical Scenarios\r\nFor better analysis, historical temperature data for the years 1980, 1990, 2000, and 2010 are generated and stored:\r\n\r\n\r\nall_past_scenarios <- temperature_scenario_from_records(weather = Bonn_temps, \r\n                                                        year = c(1980, \r\n                                                                 1990, \r\n                                                                 2000, \r\n                                                                 2010))\r\n\r\nadjusted_scenarios <- temperature_scenario_baseline_adjustment(\r\n  baseline = scenario_1996, \r\n  temperature_scenario = all_past_scenarios)\r\n\r\nall_past_scenario_temps <- temperature_generation(\r\n  weather = Bonn_temps, \r\n  years = c(1973, \r\n            2019), \r\n  sim_years = c(2001, \r\n                2101), \r\n  temperature_scenario = adjusted_scenarios)\r\n\r\nsave_temperature_scenarios(all_past_scenario_temps, \"data/Weather_ClimateWizard\", \"Bonn_historic\")\r\n\r\n\r\nModeling and Calculation of Chill, Heat, and Frost Scenarios\r\nThree models are defined to calculate chill, heat, and frost hours:\r\n\r\n\r\nmodels <- list(Chill_Portions = Dynamic_Model, \r\n               GDH = GDH, \r\n               Frost_H = function(x) step_model(x, data.frame(lower=c(-1000,0),\r\n                                                              upper=c(0,1000),\r\n                                                              weight=c(1,0))))\r\n\r\n\r\nThe calculation is performed for future scenarios:\r\n\r\n\r\nchill_past_scenarios <- load_temperature_scenarios(\r\n  \"data/chill_ClimateWizard\",\r\n  \"Bonn_historic\")\r\n\r\nchill_observed <- load_temperature_scenarios(\r\n  \"data/chill_ClimateWizard\",\r\n  \"Bonn_observed\")\r\n\r\nchills <- make_climate_scenario(\r\n  chill_past_scenarios,\r\n  caption = \"Historic\",\r\n  historic_data = chill_observed,\r\n  time_series = TRUE)\r\n\r\nfor(RCP in RCPs)\r\n  for(Time in Times)\r\n    {\r\n    chill <- load_temperature_scenarios(\r\n      \"data/chill_ClimateWizard\",\r\n      paste0(\"Bonn_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n    if(RCP == \"rcp45\") RCPcaption <- \"RCP4.5\"\r\n    if(RCP == \"rcp85\") RCPcaption <- \"RCP8.5\"\r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- chill %>% \r\n      make_climate_scenario(\r\n        caption = c(RCPcaption,\r\n                    Time_caption),\r\n        add_to = chills)\r\n}\r\n\r\n\r\nVisualization of Results\r\nThe determined scenarios are displayed in graphs:\r\n\r\n\r\ninfo_chill <- \r\n  plot_climate_scenarios(climate_scenario_list = chills, \r\n                         metric = \"Chill_Portions\", \r\n                         metric_label = \"Chill (Chill Portions)\")\r\n\r\n\r\n\r\n\r\n\r\ninfo_heat <- \r\n  plot_climate_scenarios(climate_scenario_list = chills, \r\n                         metric = \"GDH\", \r\n                         metric_label = \"Heat (Growing Degree Hours)\")\r\n\r\n\r\n\r\n\r\n\r\ninfo_frost <- \r\n  plot_climate_scenarios(climate_scenario_list = chills, \r\n                         metric = \"Frost_H\", \r\n                         metric_label = \"Frost hours\")\r\n\r\n\r\n\r\nExercises on generating CMIP5 temperature scenarios\r\nAnalyze the historic and future impact of climate change on three agroclimatic metrics of your choice, for the location you’ve chosen for your earlier analyses.\r\n\r\n\r\n# Set baseline period and save temperature scenarios \r\nRCPs <- c(\"rcp45\",\r\n          \"rcp85\")\r\nTimes <- c(2050,\r\n           2085)\r\n\r\nfor(RCP in RCPs)\r\n  for(Time in Times)\r\n  {start_year <- Time - 15\r\n  end_year <- Time + 15\r\n  clim_scen <-\r\n    getClimateWizardData(\r\n      c(longitude = -120.539,\r\n        latitude = 46.569),\r\n      RCP,\r\n      start_year,\r\n      end_year,\r\n      temperature_generation_scenarios = TRUE,\r\n      baseline =c(1975, 2005),\r\n      metric = \"monthly_min_max_temps\",\r\n      GCMs = \"all\")\r\n  save_temperature_scenarios(clim_scen,\r\n                             \"Yakima/ClimateWizard\",\r\n                             paste0(\"Yakima_futures_\",Time,\"_\",RCP))}\r\n\r\n\r\n\r\n\r\n# Baseline adjustment\r\nscenario_1990 <- Yakima_temps %>%\r\n  temperature_scenario_from_records(1990)\r\nscenario_1998 <- Yakima_temps %>%\r\n  temperature_scenario_from_records(1998)\r\nadjustment_scenario <-\r\n  temperature_scenario_baseline_adjustment(scenario_1998,\r\n                                           scenario_1990)\r\n\r\n\r\n\r\n\r\n# Selecting RCPs and scenario years \r\nRCPs <- c(\"rcp45\",\r\n          \"rcp85\")\r\nTimes <- c(2050,\r\n           2085)\r\n\r\n\r\n\r\n\r\n# Temperature generation for future scenarios \r\nfor(RCP in RCPs)\r\n  for(Time in Times)\r\n  {\r\n    clim_scen <- load_ClimateWizard_scenarios(\r\n      \"Yakima/climateWizard\",\r\n      paste0(\"Yakima_futures_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n    \r\n    clim_scen_adjusted <-\r\n      temperature_scenario_baseline_adjustment(\r\n        baseline_temperature_scenario = adjustment_scenario,\r\n        temperature_scenario = clim_scen)\r\n    \r\n    Temps <- temperature_generation(\r\n      weather = Yakima_temps, \r\n      years = c(1973,\r\n                2023),\r\n      sim_years = c(2001,\r\n                    2101),\r\n      temperature_scenario = clim_scen_adjusted)\r\n    \r\n    save_temperature_scenarios(\r\n      Temps,\r\n      \"Yakima/Weather_ClimateWizard\",\r\n      paste0(\"Yakima_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n  }\r\n\r\n\r\n\r\n\r\n# Adding historic scenarios \r\nall_past_scenarios <- temperature_scenario_from_records(\r\n  weather = Yakima_temps,\r\n  year = c(1980,\r\n           1990,\r\n           2000,\r\n           2010))\r\n\r\nadjusted_scenarios <- temperature_scenario_baseline_adjustment(\r\n  baseline = scenario_1998,\r\n  temperature_scenario = all_past_scenarios)\r\n\r\nall_past_scenario_temps <- temperature_generation(\r\n  weather = Yakima_temps,\r\n  years = c(1973,\r\n            2023),\r\n  sim_years = c(2001,\r\n                2101),\r\n  temperature_scenario = adjusted_scenarios)\r\n\r\nsave_temperature_scenarios(\r\n  all_past_scenario_temps,\r\n  \"Yakima/Weather_ClimateWizard\",\r\n  \"Yakima_historic\")\r\n\r\n\r\n\r\n\r\n# Selection of models \r\nmodels <- list(Chill_Portions = Dynamic_Model, \r\n               GDH = GDH, \r\n               Frost_H = function(x) step_model(x, data.frame(lower=c(-1000,0),\r\n                                                              upper=c(0,1000),\r\n                                                              weight=c(1,0))))\r\n\r\n\r\n\r\n\r\n# Apply the models to historic data \r\nTemps <- load_temperature_scenarios(\"Yakima/Weather_ClimateWizard\",\r\n                                    \"Yakima_historic\")\r\n\r\nchill_past_scenarios <-\r\n  Temps %>%\r\n  tempResponse_daily_list(\r\n    latitude = 46.569,\r\n    Start_JDay = 305,\r\n    End_JDay = 59,\r\n    models = models,\r\n    misstolerance = 10)\r\n\r\nchill_observed <- \r\n  Yakima_temps %>%\r\n  tempResponse_daily_list(\r\n    latitude = 46.569,\r\n    Start_JDay = 305,\r\n    End_JDay = 59,\r\n    models = models,\r\n    misstolerance = 10)\r\n\r\nsave_temperature_scenarios(chill_past_scenarios,\r\n                           \"Yakima/chill_ClimateWizard\",\r\n                           \"Yakima_historic\")\r\nsave_temperature_scenarios(chill_observed,\r\n                           \"Yakima/chill_ClimateWizard\",\r\n                           \"Yakima_observed\")\r\n\r\n\r\n\r\n\r\nchill_past_scenarios <- load_temperature_scenarios(\r\n  \"Yakima/chill_ClimateWizard\",\r\n  \"Yakima_historic\")\r\nchill_observed <- load_temperature_scenarios(\r\n  \"Yakima/chill_ClimateWizard\",\r\n  \"Yakima_observed\")\r\n\r\nchills <- make_climate_scenario(\r\n  chill_past_scenarios,\r\n  caption = \"Historic\",\r\n  historic_data = chill_observed,\r\n  time_series = TRUE)\r\n\r\n# Plot climate scenarios \r\nplot_climate_scenarios(\r\n  climate_scenario_list = chills,\r\n  metric = \"Chill_Portions\",\r\n  metric_label = \"Chill (Chill Portions)\")\r\n\r\n\r\n[[1]]\r\n[1] \"time series labels\"\r\n\r\n\r\n\r\n# Add climate scenario to the chills object\r\nfor(RCP in RCPs)\r\n  for(Time in Times)\r\n    {\r\n    Temps <- load_temperature_scenarios(\r\n      \"Yakima/Weather_ClimateWizard\",\r\n      paste0(\"Yakima_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n    chill <- Temps %>% \r\n      tempResponse_daily_list(\r\n        latitude = 46.569,\r\n        Start_JDay = 305,\r\n        End_JDay = 59,\r\n        models = models,\r\n        misstolerance = 10)\r\n    save_temperature_scenarios(\r\n      chill,\r\n      \"Yakima/chill_ClimateWizard\",\r\n      paste0(\"Yakima_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n}\r\n\r\n\r\n\r\n\r\nfor(RCP in RCPs)\r\n  for(Time in Times)\r\n    {\r\n    chill <- load_temperature_scenarios(\r\n      \"Yakima/chill_ClimateWizard\",\r\n      paste0(\"Yakima_\",\r\n             Time,\r\n             \"_\",\r\n             RCP))\r\n    if(RCP == \"rcp45\") RCPcaption <- \"RCP4.5\"\r\n    if(RCP == \"rcp85\") RCPcaption <- \"RCP8.5\"\r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- chill %>% \r\n      make_climate_scenario(\r\n        caption = c(RCPcaption,\r\n                    Time_caption),\r\n        add_to = chills)\r\n}\r\n\r\n\r\n\r\n\r\n# Plot chill hours\r\ninfo_chill <-\r\n  plot_climate_scenarios(\r\n    climate_scenario_list = chills,\r\n    metric = \"Chill_Portions\",\r\n    metric_label = \"Chill (Chill Portions)\",\r\n    texcex = 1.5)\r\n\r\n\r\n\r\n\r\n\r\n# Plot Heat (Growing degree hours)\r\ninfo_heat <-\r\n  plot_climate_scenarios(\r\n    climate_scenario_list = chills,\r\n    metric = \"GDH\",\r\n    metric_label = \"Heat (Growing Degree Hours)\",\r\n    texcex = 1.5)\r\n\r\n\r\n\r\n\r\n\r\n# Plot frost hours\r\ninfo_frost <- \r\n  plot_climate_scenarios(  \r\n    climate_scenario_list=chills,\r\n    metric=\"Frost_H\",\r\n    metric_label=\"Frost hours\",\r\n    texcex=1.5)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:26:16+01:00"
    },
    {
      "path": "cmip6.html",
      "title": "Making CMIP6 scenarios",
      "author": [],
      "contents": "\r\nAccessing Gridded Climate Data from the Copernicus Climate Data Store\r\nFuture climate data is often provided in a gridded format, making it necessary to download large datasets to extract information for a single station. While CMIP5 data was accessible through ClimateWizard, a comparable solution for CMIP6 is not available.\r\nAccessing CMIP6 Data\r\nThe Copernicus Climate Data Store (CDS) provides access to CMIP6 climate projections. A free registration is required to obtain an API key for downloading data.\r\nThe function download_cmip6_ecmwfr() allows downloading climate model data for a specified region. For example, retrieving data for Bonn (7.1°E, 50.8°N):\r\n\r\n\r\nlocation = c(7.1, 50.8)\r\narea <- c(52, 6, 50, 8) # (max. Lat, min. Lon, min. Lat, max. Lon)\r\n\r\ndownload_cmip6_ecmwfr(\r\n  scenarios = 'ssp126', \r\n  area = area,\r\n  user = 'USER_ID',\r\n  key = 'API_KEY',\r\n  model = 'default',\r\n  frequency = 'monthly',\r\n  variable = c('Tmin', 'Tmax'),\r\n  year_start = 2015,\r\n  year_end = 2100)\r\n\r\n\r\nThe data is stored in a subfolder (cmip6_downloaded). Models that lack data for the selected parameters are automatically excluded.\r\nIt is possible to download multiple climate scenarios simultaneously:\r\n\r\ndownload_cmip6_ecmwfr(\r\n  scenarios = c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"), \r\n  area = area,\r\n  user = 'write user id here'\r\n  key = 'write key here',\r\n  model = 'default',\r\n  frequency = 'monthly',\r\n  variable = c('Tmin', 'Tmax'),\r\n  year_start = 2015,\r\n  year_end = 2100)\r\n\r\nGenerating Change Scenarios\r\nSince climate models use a coarse grid, it is useful to calculate temperature changes relative to a baseline period (1986-2014).\r\n\r\n\r\ndownload_baseline_cmip6_ecmwfr(\r\n  area = area,\r\n  user = 'USER_ID',\r\n  key = 'API_KEY',\r\n  model = 'match_downloaded',\r\n  frequency = 'monthly',\r\n  variable = c('Tmin', 'Tmax'),\r\n  year_start = 1986, \r\n  year_end = 2014)\r\n\r\n\r\nExtracting Data for a Specific Station\r\nThe extract_cmip6_data() function allows extracting climate data for a specific location.\r\n\r\n\r\nstation <- data.frame(\r\n  station_name = c(\"Bonn\"),\r\n  longitude = c(7.1),\r\n  latitude = c(50.8))\r\n\r\nextracted <- extract_cmip6_data(stations = station)\r\n\r\nwrite.csv(extracted$`ssp126_AWI-CM-1-1-MR`, \"data/extract_example_ssp126_AWI-CM-1-1-MR.csv\", row.names = FALSE)\r\n\r\n\r\nCalculating Relative Changes\r\nUsing the extracted data, relative climate change scenarios can be generated:\r\n\r\n\r\nchange_scenarios <- gen_rel_change_scenario(extracted)\r\nwrite.csv(change_scenarios, \"data/all_change_scenarios.csv\", row.names = FALSE)\r\n\r\n\r\nTo make the data usable for further analysis, it can be converted into a structured format:\r\n\r\n\r\nscen_list <- convert_scen_information(change_scenarios)\r\n\r\n\r\nAdjusting the Baseline for Weather Simulations\r\nIf the baseline period of the scenarios does not match observational records, an adjustment is needed.\r\nExample: Calculating the temperature trend in Bonn between 1996 and 2000:\r\n\r\n\r\ntemps_1996 <- temperature_scenario_from_records(Bonn_temps, 1996)\r\ntemps_2000 <- temperature_scenario_from_records(Bonn_temps, 2000)\r\n\r\nbase <- temperature_scenario_baseline_adjustment(temps_1996, temps_2000)\r\n\r\n\r\nNow, the baseline of the scenarios is adjusted:\r\n\r\n\r\nscen_list <- convert_scen_information(change_scenarios, give_structure = FALSE)\r\n\r\nadjusted_list <- temperature_scenario_baseline_adjustment(\r\n  base,\r\n  scen_list,\r\n  temperature_check_args = list(scenario_check_thresholds = c(-5, 15)))\r\n\r\n\r\nGenerating and Saving Climate Scenarios\r\nThe corrected climate scenarios can now be used in a weather generator:\r\n\r\n\r\ntemps <- temperature_generation(Bonn_temps, \r\n                                years = c(1973, 2019), \r\n                                sim_years = c(2001, 2100), \r\n                                adjusted_list)\r\n\r\nsave_temperature_scenarios(temps,\r\n                           \"data/future_climate\",\r\n                           \"Bonn_futuretemps\")\r\n\r\n\r\nIt’s important to save the data now to avoid waiting for the process to run again in the future. Temperature responses are calculated efficiently using the tempResponse_daily_list function, with three models: the Dynamic Model for chill accumulation, the GDH model for heat accumulation, and a simple model for frost hours.\r\n\r\n\r\nfrost_model <- function(x)\r\n  step_model(x,\r\n             data.frame(\r\n               lower = c(-1000, 0),\r\n               upper = c(0, 1000),\r\n               weight = c(1, 0)))\r\n\r\nmodels <- list(Chill_Portions = Dynamic_Model,\r\n               GDH = GDH,\r\n               Frost_H = frost_model)\r\n\r\n\r\nClimate scenarios are generated using the make_climate_scenario function and plotted. Historical and future climate scenarios are combined, and for each SSP and year (2050, 2085), the scenario is added.\r\n\r\n\r\nchill_future_scenario_list <- tempResponse_daily_list(temps,\r\n                                                    latitude = 50.8,\r\n                                                    Start_JDay = 305,\r\n                                                    End_JDay = 59,\r\n                                                    models = models)\r\n\r\nchill_future_scenario_list <- lapply(chill_future_scenario_list,\r\n                                     function(x) x %>%\r\n                                       filter(Perc_complete == 100))\r\n\r\nsave_temperature_scenarios(chill_future_scenario_list,\r\n                           \"data/future_climate\",\r\n                           \"Bonn_futurechill_305_59\")\r\n\r\n\r\nLoading Historical Data and Making Climate Scenarios\r\nHistorical climate data is loaded and used to create climate scenarios for both past and future conditions.\r\n\r\n\r\nchill_hist_scenario_list <- load_temperature_scenarios(\"data\",\r\n                                                     \"Bonn_hist_chill_305_59\")\r\n\r\nobserved_chill <- read_tab(\"data/Bonn_observed_chill_305_59.csv\")\r\n\r\nchills <- make_climate_scenario(\r\n  chill_hist_scenario_list,\r\n  caption = \"Historical\",\r\n  historic_data = observed_chill,\r\n  time_series = TRUE)\r\n\r\nplot_climate_scenarios(\r\n  climate_scenario_list = chills,\r\n  metric = \"Chill_Portions\",\r\n  metric_label = \"Chill (Chill Portions)\")\r\n\r\n\r\n\r\nProcessing Future Climate Scenarios by SSP and Time\r\nFor each SSP and time combination (2050, 2085), future climate scenarios are added to the climate scenarios object.\r\n\r\n\r\nSSPs <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\nlist_ssp <- \r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n    {\r\n    \r\n    # find all scenarios for the ssp and time\r\n    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]\r\n    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    if(SSP == \"ssp126\") SSPcaption <- \"SSP1\"\r\n    if(SSP == \"ssp245\") SSPcaption <- \"SSP2\"\r\n    if(SSP == \"ssp370\") SSPcaption <- \"SSP3\"\r\n    if(SSP == \"ssp585\") SSPcaption <- \"SSP5\"    \r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- chill %>% \r\n      make_climate_scenario(\r\n        caption = c(SSPcaption,\r\n                    Time_caption),\r\n        add_to = chills)\r\n}\r\n\r\n\r\nPlotting and Analyzing Climate Trends\r\nFinally, the trends for chill, heat, and frost hours are visualized, and additional information is stored for later analysis.\r\n\r\n\r\ninfo_chill <- plot_climate_scenarios(\r\n  climate_scenario_list = chills,\r\n  metric = \"Chill_Portions\",\r\n  metric_label = \"Chill (Chill Portions)\",\r\n  texcex = 1.5)\r\n\r\n\r\n\r\n\r\n\r\ninfo_heat <- plot_climate_scenarios(\r\n  climate_scenario_list = chills,\r\n  metric = \"GDH\",\r\n  metric_label = \"Heat (Growing Degree Hours)\",\r\n  texcex = 1.5)\r\n\r\n\r\n\r\n\r\n\r\ninfo_frost <- plot_climate_scenarios(\r\n  climate_scenario_list = chills,\r\n  metric = \"Frost_H\",\r\n  metric_label = \"Frost hours\",\r\n  texcex = 1.5)\r\n\r\n\r\n\r\nExercises on generating CMIP6 temperature scenarios\r\nAnalyze the historic and future impact of climate change on two agroclimatic metrics of your choice, for the location you’ve chosen for your earlier analyses.\r\n\r\n\r\n# Set location\r\nlocation = c(-120.5, 46.6)\r\narea <- c(48, -122 , 45, -119)\r\n\r\n# Download scenarios \r\ndownload_cmip6_ecmwfr(\r\n  scenarios = c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"), \r\n  area = c(49, -122 , 44, -118),\r\n  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',\r\n  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',\r\n  model = 'default',\r\n  frequency = 'monthly',\r\n  variable = c('Tmin', 'Tmax'),\r\n  year_start = 2015,\r\n  year_end = 2100)\r\n\r\n# Download baseline\r\ndownload_baseline_cmip6_ecmwfr(\r\n  area = c(49, -122 , 44, -118),\r\n  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',\r\n  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',\r\n  model = 'match_downloaded',\r\n  frequency = 'monthly',\r\n  variable = c('Tmin', 'Tmax'),\r\n  year_start = 1986, \r\n  year_end = 2014, \r\n  month = 1:12)\r\n\r\n# Extract data for specified location\r\nstation <- data.frame(\r\n  station_name = c(\"Yakima\"),\r\n  longitude = c(-120.5),\r\n  latitude = c(46.6))\r\n\r\nextracted <- extract_cmip6_data(stations = station,\r\n                                download_path = \"cmip6_downloaded/49_-122_44_-118\")\r\n\r\n\r\n\r\nUnzipping files\r\nExtracting downloaded CMIP6 files\r\n\r\n\r\n\r\n# Generate change scenarios\r\nchange_scenarios <- gen_rel_change_scenario(extracted)\r\n\r\n# Convert information into a list\r\nscen_list <- convert_scen_information(change_scenarios)\r\n\r\n# Calculate temperature between 1996 and 2000\r\ntemps_1996 <- temperature_scenario_from_records(Yakima_temps,\r\n                                                1996)\r\n\r\ntemps_2000 <- temperature_scenario_from_records(Yakima_temps,\r\n                                                2000)\r\n\r\n# Adjusts baseline based on observed temperature trends\r\nbase <- temperature_scenario_baseline_adjustment(temps_1996,\r\n                                                 temps_2000)\r\n\r\n# Convert scenarios \r\nscen_list <- convert_scen_information(change_scenarios, \r\n                                      give_structure = FALSE)\r\n\r\nadjusted_list <- temperature_scenario_baseline_adjustment(base,\r\n                                                          scen_list,\r\n                                                          temperature_check_args = list(scenario_check_thresholds = c(-5, 15)))\r\n\r\n\r\n\r\n\r\n# Generate temperatures \r\nfor(scen in 1:length(adjusted_list))\r\n{\r\n  if(!file.exists(paste0(\"Yakima/future_climate/Yakima_future_\",\r\n                         scen,\"_\",\r\n                         names(adjusted_list)[scen],\".csv\")) )\r\n  {temp_temp <- temperature_generation(Yakima_temps,\r\n                                       years = c(1973, 2019),\r\n                                       sim_years = c(2001, 2100),\r\n                                       adjusted_list[scen],  \r\n                                       temperature_check_args = \r\n                                         list( scenario_check_thresholds = c(-5, 15)))\r\n  write.csv(temp_temp[[1]],paste0(\"Yakima/future_climate/Yakima_future_\",scen,\"_\",names(adjusted_list)[scen],\".csv\"),\r\n            row.names=FALSE)\r\n  print(paste(\"Processed object\",scen,\"of\", length(adjusted_list)))\r\n  \r\n  \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Selection of models \r\nmodels <- list(Chill_Portions = Dynamic_Model, \r\n               GDH = GDH, \r\n               Frost_H = function(x) step_model(x, data.frame(lower=c(-1000,0),\r\n                                                              upper=c(0,1000),\r\n                                                              weight=c(1,0))))\r\n\r\n# Calculate temperature responses \r\ntemps <- load_temperature_scenarios(\"Yakima/future_climate\",\"Yakima_future_\")\r\n\r\nchill_future_scenario_list <- tempResponse_daily_list(temps,\r\n                                                      latitude = 46.6,\r\n                                                      Start_JDay = 305,\r\n                                                      End_JDay = 59,\r\n                                                      models = models)\r\n\r\nchill_future_scenario_list <- lapply(chill_future_scenario_list,\r\n                                     function(x) x %>%\r\n                                       filter(Perc_complete == 100))\r\n\r\nsave_temperature_scenarios(chill_future_scenario_list,\r\n                           \"Yakima/future_climate\",\r\n                           \"Yakima_futurechill_305_59\")\r\n\r\n\r\n\r\n\r\n# Generate climate scenarios \r\nobserved_chill <- read_tab(\"Yakima/Yakima_observed_chill_305_59.csv\")\r\nchill_hist_scenario_list <- load_temperature_scenarios(\"Yakima\",\r\n                                                       \"Yakima_hist_chill_305_59\")\r\n\r\nchills <- make_climate_scenario(\r\n  chill_hist_scenario_list,\r\n  caption = \"Historic\",\r\n  historic_data = observed_chill,\r\n  time_series = TRUE)\r\n\r\n# Plot historic climate scenarios \r\nplot_climate_scenarios(\r\n  climate_scenario_list = chills,\r\n  metric = \"Chill_Portions\",\r\n  metric_label = \"Chill (Chill Portions)\")\r\n\r\n\r\n[[1]]\r\n[1] \"time series labels\"\r\n\r\n\r\n\r\n# Identify data that belong to specific combinations of SSP and time \r\nSSPs <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\nlist_ssp <- \r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n  {\r\n    \r\n    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]\r\n    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    if(SSP == \"ssp126\") SSPcaption <- \"SSP1\"\r\n    if(SSP == \"ssp245\") SSPcaption <- \"SSP2\"\r\n    if(SSP == \"ssp370\") SSPcaption <- \"SSP3\"  \r\n    if(SSP == \"ssp585\") SSPcaption <- \"SSP5\"    \r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- chill %>% \r\n      make_climate_scenario(\r\n        caption = c(SSPcaption,\r\n                    Time_caption),\r\n        add_to = chills)\r\n  }\r\n\r\n\r\n\r\n\r\n# Plot chill hours\r\ninfo_chill <-\r\n  plot_climate_scenarios(\r\n    climate_scenario_list = chills,\r\n    metric = \"Chill_Portions\",\r\n    metric_label = \"Chill (Chill Portions)\",\r\n    texcex = 1.5)\r\n\r\n\r\n\r\n\r\n\r\n# Plot Heat (Growing degree hours)\r\ninfo_heat <-\r\n  plot_climate_scenarios(\r\n    climate_scenario_list = chills,\r\n    metric = \"GDH\",\r\n    metric_label = \"Heat (Growing Degree Hours)\",\r\n    texcex = 1.5)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:29:44+01:00"
    },
    {
      "path": "enhanced_PLS.html",
      "title": "Experimentally enhanced PLS",
      "author": [],
      "contents": "\r\nEnhanced phenology data\r\nIn Klein-Altendorf, limited temperature variation makes it difficult to analyze phenology responses to chill and heat. To enhance the data, a two-winter experiment was conducted at Campus Klein-Altendorf (Fernandez et al., 2021). Young potted trees were exposed to different controlled environments during winter. In the 2018/2019 season, three environments were used, and in 2019/2020, four additional environments were introduced, including three chambers with different materials and an outdoor setting at Campus Endenich, University of Bonn. By moving trees between environments, 66 experimental seasons for apples and 33 for pears were created.\r\nThe following plot provides a schematic (animated) representation of the experiment using ggplot2 and gganimate.\r\n\r\n\r\ndata <- read.csv(\"data/interactive_plot_PLS.csv\", sep = \";\")\r\n\r\n# This part is to re-code the different conditions\r\ndata[which(data$Final_Condition == \"Outside\"), \"Final_condition_2\"] <- 1\r\ndata[which(data$Final_Condition == \"Un-heated\"), \"Final_condition_2\"] <- 2\r\ndata[which(data$Final_Condition == \"Heated\"), \"Final_condition_2\"] <- 3\r\n\r\n# Implement the plot\r\nggplot(data, aes(Day, Final_condition_2, color = factor(Treatment, levels = c(1 : 33)))) +\r\n  geom_jitter(size = 4) +\r\n  geom_path(size = 1) +\r\n  scale_y_continuous(breaks = c(1, 2, 3),\r\n                     labels = c(\"Outside\", \"Un-Heated\", \"Heated\")) +\r\n  scale_x_continuous(breaks = as.numeric(levels(as.factor(data$Day))),\r\n                     labels = levels(as.factor(data$Day))) +\r\n  labs(x = \"Days of experiment\", y = \"Condition\", color = \"Treatment\") +\r\n  theme_bw() +\r\n  theme(axis.text.y = element_text(angle = 90, hjust = 0.5),\r\n        legend.position = \"none\") +\r\n  transition_reveal(Day)\r\n\r\nanim_save(\"data/interactive_experiment_plot.gif\",\r\n          animation = last_animation())\r\n\r\n\r\nIllustration of the tree movements in our PLS-enhancement experimentThe bloom dates of the trees were recorded following exposure to various winter temperature patterns, resulting in significant differences in bloom timing.\r\n\r\nThe plot above illustrates the mean temperature (solid line), the range of mean temperatures (sky blue shade), and the range of bloom dates (rectangles at the bottom) across different treatments over two winters.\r\nThe next step is to examine key findings from the study, which involves loading the weather files and the dataset containing flowering date observations for apples, saved in the data directory.\r\n\r\n\r\npheno_data <- read_tab(\"data/final_bio_data_S1_S2_apple.csv\")\r\nweather_data <- read_tab(\"data/final_weather_data_S1_S2.csv\")\r\n\r\n\r\nSeveral functions from previous chapters are required for the analysis:\r\nggplot_PLS (from Delineating temperature response phases with PLS regression)\r\nplot_PLS_chill_force (from PLS regression with agroclimatic metrics)\r\npheno_trend_ggplot (from Evaluating PLS outputs)\r\nChill_model_sensitivity and Chill_sensitivity_temps (both from Why PLS doesn’t always work)\r\nThese functions are being reloaded with suppressed output for clarity. Before conducting the PLS analysis, some preprocessing of the dataset is necessary, which will be explained in detail during the class.\r\n\r\n\r\nggplot_PLS <- function(PLS_results)\r\n{\r\n  library(ggplot2)\r\n  PLS_gg <- PLS_results$PLS_summary\r\n  PLS_gg[,\"Month\"]<-trunc(PLS_gg$Date/100)\r\n  PLS_gg[,\"Day\"]<-PLS_gg$Date-PLS_gg$Month*100\r\n  PLS_gg[,\"Date\"]<-ISOdate(2002,PLS_gg$Month,PLS_gg$Day)\r\n  PLS_gg[which(PLS_gg$JDay<=0),\"Date\"]<-ISOdate(2001,PLS_gg$Month[which(PLS_gg$JDay<=0)],PLS_gg$Day[which(PLS_gg$JDay<=0)])\r\n  PLS_gg[,\"VIP_importance\"]<-PLS_gg$VIP>=0.8\r\n  PLS_gg[,\"VIP_Coeff\"]<-factor(sign(PLS_gg$Coef)*PLS_gg$VIP_importance)\r\n  \r\n  VIP_plot<- ggplot(PLS_gg,aes(x=Date,y=VIP)) +\r\n  geom_bar(stat='identity',aes(fill=VIP>0.8)) +\r\n  scale_fill_manual(name=\"VIP\", \r\n                    labels = c(\"<0.8\", \">0.8\"), \r\n                    values = c(\"FALSE\"=\"grey\", \"TRUE\"=\"blue\")) +\r\n  theme_bw(base_size=15) +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank() )\r\n  \r\n  coeff_plot<- ggplot(PLS_gg,aes(x=Date,y=Coef)) +\r\n  geom_bar(stat='identity',aes(fill=VIP_Coeff)) +\r\n  scale_fill_manual(name=\"Effect direction\", \r\n                    labels = c(\"Advancing\", \"Unimportant\",\"Delaying\"), \r\n                    values = c(\"-1\"=\"red\", \"0\"=\"grey\",\"1\"=\"dark green\")) +\r\n  theme_bw(base_size=15) +\r\n  ylab(\"PLS coefficient\") +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank() )\r\n  \r\n  temp_plot<- ggplot(PLS_gg) +\r\n    geom_ribbon(aes(x=Date,ymin=Tmean-Tstdev,ymax=Tmean+Tstdev),fill=\"grey\") +\r\n    geom_ribbon(aes(x=Date,ymin=Tmean-Tstdev*(VIP_Coeff==-1),ymax=Tmean+Tstdev*(VIP_Coeff==-1)),fill=\"red\") +\r\n    geom_ribbon(aes(x=Date,ymin=Tmean-Tstdev*(VIP_Coeff==1),ymax=Tmean+Tstdev*(VIP_Coeff==1)),fill=\"dark green\") +\r\n    geom_line(aes(x=Date,y=Tmean)) +\r\n    theme_bw(base_size=15) +\r\n    ylab(expression(paste(T[mean],\" (°C)\")))\r\n\r\n  library(patchwork)\r\n  plot<- (VIP_plot +\r\n            coeff_plot +\r\n            temp_plot +\r\n            plot_layout(ncol=1,\r\n                        guides = \"collect\")\r\n          ) & theme(legend.position = \"right\",\r\n                    legend.text = element_text(size=8),\r\n                    legend.title = element_text(size=10),\r\n                    axis.title.x=element_blank())\r\n\r\nplot\r\n}\r\n\r\nplot_PLS_chill_force<-function(plscf,\r\n                               chill_metric=\"Chill_Portions\",\r\n                               heat_metric=\"GDH\",\r\n                               chill_label=\"CP\",\r\n                               heat_label=\"GDH\",\r\n                               chill_phase=c(-48,62),\r\n                               heat_phase=c(-5,105.5))\r\n{\r\n  PLS_gg<-plscf[[chill_metric]][[heat_metric]]$PLS_summary\r\n  PLS_gg[,\"Month\"]<-trunc(PLS_gg$Date/100)\r\n  PLS_gg[,\"Day\"]<-PLS_gg$Date-PLS_gg$Month*100\r\n  PLS_gg[,\"Date\"]<-ISOdate(2002,PLS_gg$Month,PLS_gg$Day)\r\n  PLS_gg[which(PLS_gg$JDay<=0),\"Date\"]<-ISOdate(2001,PLS_gg$Month[which(PLS_gg$JDay<=0)],PLS_gg$Day[which(PLS_gg$JDay<=0)])\r\n  PLS_gg[,\"VIP_importance\"]<-PLS_gg$VIP>=0.8\r\n  PLS_gg[,\"VIP_Coeff\"]<-factor(sign(PLS_gg$Coef)*PLS_gg$VIP_importance)\r\n  \r\n  chill_start_date<-ISOdate(2001,12,31)+chill_phase[1]*24*3600\r\n  chill_end_date<-ISOdate(2001,12,31)+chill_phase[2]*24*3600\r\n  heat_start_date<-ISOdate(2001,12,31)+heat_phase[1]*24*3600\r\n  heat_end_date<-ISOdate(2001,12,31)+heat_phase[2]*24*3600\r\n\r\n  \r\n  temp_plot<- ggplot(PLS_gg) +\r\n    annotate(\"rect\",\r\n             xmin = chill_start_date,\r\n             xmax = chill_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"blue\") +\r\n    annotate(\"rect\",\r\n             xmin = heat_start_date,\r\n             xmax = heat_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"red\") +\r\n    annotate(\"rect\",\r\n             xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600,\r\n             xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"black\") +\r\n    geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = \"dashed\") +\r\n    geom_ribbon(aes(x=Date,\r\n                    ymin=MetricMean - MetricStdev ,\r\n                    ymax=MetricMean + MetricStdev ),\r\n                fill=\"grey\") +\r\n    geom_ribbon(aes(x=Date,\r\n                    ymin=MetricMean - MetricStdev * (VIP_Coeff==-1),\r\n                    ymax=MetricMean + MetricStdev * (VIP_Coeff==-1)),\r\n                fill=\"red\") +\r\n    geom_ribbon(aes(x=Date,\r\n                    ymin=MetricMean - MetricStdev * (VIP_Coeff==1),\r\n                    ymax=MetricMean + MetricStdev * (VIP_Coeff==1)),\r\n                fill=\"dark green\") +\r\n    geom_line(aes(x=Date,y=MetricMean )) +\r\n    facet_wrap(vars(Type), scales = \"free_y\",\r\n               strip.position=\"left\",\r\n               labeller = labeller(Type = as_labeller(c(Chill=paste0(\"Chill (\",chill_label,\")\"),Heat=paste0(\"Heat (\",heat_label,\")\")) )) ) +\r\n    ggtitle(\"Daily chill and heat accumulation rates\") +\r\n    theme_bw(base_size=15) + \r\n    theme(strip.background = element_blank(),\r\n          strip.placement = \"outside\",\r\n          strip.text.y = element_text(size =12),\r\n          plot.title = element_text(hjust = 0.5),\r\n          axis.title.y=element_blank()\r\n          )\r\n  \r\n  VIP_plot<- ggplot(PLS_gg,aes(x=Date,y=VIP)) +\r\n    annotate(\"rect\",\r\n             xmin = chill_start_date,\r\n             xmax = chill_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"blue\") +\r\n    annotate(\"rect\",\r\n             xmin = heat_start_date,\r\n             xmax = heat_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"red\") +\r\n    annotate(\"rect\",\r\n             xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600,\r\n             xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"black\") +\r\n    geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = \"dashed\") +\r\n    geom_bar(stat='identity',aes(fill=VIP>0.8)) +\r\n    facet_wrap(vars(Type), scales=\"free\",\r\n               strip.position=\"left\",\r\n               labeller = labeller(Type = as_labeller(c(Chill=\"VIP for chill\",Heat=\"VIP for heat\") )) ) +\r\n    scale_y_continuous(limits=c(0,max(plscf[[chill_metric]][[heat_metric]]$PLS_summary$VIP))) +\r\n    ggtitle(\"Variable Importance in the Projection (VIP) scores\") +\r\n    theme_bw(base_size=15) + \r\n    theme(strip.background = element_blank(),\r\n          strip.placement = \"outside\",\r\n          strip.text.y = element_text(size =12),\r\n          plot.title = element_text(hjust = 0.5),\r\n          axis.title.y=element_blank()\r\n          ) +\r\n    scale_fill_manual(name=\"VIP\", \r\n                      labels = c(\"<0.8\", \">0.8\"), \r\n                      values = c(\"FALSE\"=\"grey\", \"TRUE\"=\"blue\")) +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank(),\r\n          axis.title.y = element_blank())\r\n  \r\n  coeff_plot<- ggplot(PLS_gg,aes(x=Date,y=Coef)) +\r\n    annotate(\"rect\",\r\n             xmin = chill_start_date,\r\n             xmax = chill_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"blue\") +\r\n    annotate(\"rect\",\r\n             xmin = heat_start_date,\r\n             xmax = heat_end_date,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"red\") +\r\n    annotate(\"rect\",\r\n             xmin = ISOdate(2001,12,31) + min(plscf$pheno$pheno,na.rm=TRUE)*24*3600,\r\n             xmax = ISOdate(2001,12,31) + max(plscf$pheno$pheno,na.rm=TRUE)*24*3600,\r\n             ymin = -Inf,\r\n             ymax = Inf,\r\n             alpha = .1,fill = \"black\") +\r\n    geom_vline(xintercept = ISOdate(2001,12,31) + median(plscf$pheno$pheno,na.rm=TRUE)*24*3600, linetype = \"dashed\") +\r\n    geom_bar(stat='identity',aes(fill=VIP_Coeff)) +\r\n    facet_wrap(vars(Type), scales=\"free\",\r\n               strip.position=\"left\",\r\n               labeller = labeller(Type = as_labeller(c(Chill=\"MC for chill\",Heat=\"MC for heat\") )) ) +\r\n    scale_y_continuous(limits=c(min(plscf[[chill_metric]][[heat_metric]]$PLS_summary$Coef),\r\n                                max(plscf[[chill_metric]][[heat_metric]]$PLS_summary$Coef))) +\r\n    ggtitle(\"Model coefficients (MC)\") +\r\n    theme_bw(base_size=15) + \r\n    theme(strip.background = element_blank(),\r\n          strip.placement = \"outside\",\r\n          strip.text.y = element_text(size =12),\r\n          plot.title = element_text(hjust = 0.5),\r\n          axis.title.y=element_blank()\r\n          ) +\r\n    scale_fill_manual(name=\"Effect direction\", \r\n                      labels = c(\"Advancing\", \"Unimportant\",\"Delaying\"), \r\n                      values = c(\"-1\"=\"red\", \"0\"=\"grey\",\"1\"=\"dark green\")) +\r\n    ylab(\"PLS coefficient\") +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank(),\r\n          axis.title.y = element_blank())\r\n  \r\n  library(patchwork)\r\n  \r\n  plot<- (VIP_plot +\r\n            coeff_plot +\r\n            temp_plot +\r\n            plot_layout(ncol=1,\r\n                        guides = \"collect\")\r\n          ) & theme(legend.position = \"right\",\r\n                    legend.text = element_text(size=8),\r\n                    legend.title = element_text(size=10),\r\n                    axis.title.x=element_blank())\r\n\r\nplot\r\n\r\n}\r\n\r\n\r\npheno_trend_ggplot<-function(temps,\r\n                             pheno,\r\n                             chill_phase,\r\n                             heat_phase,\r\n                             exclude_years=NA,\r\n                             phenology_stage=\"Bloom\")\r\n{\r\n  library(fields)\r\n  library(reshape2)\r\n  library(metR)\r\n  library(ggplot2)\r\n  library(colorRamps)\r\n  \r\n  # first, a sub-function (function defined within a function) to\r\n  # compute the temperature means\r\n  \r\n  mean_temp_period<-function(temps,\r\n                             start_JDay,\r\n                             end_JDay, \r\n                             end_season = end_JDay)\r\n    { temps_JDay<-make_JDay(temps)\r\n    temps_JDay[,\"Season\"]<-temps_JDay$Year\r\n    if(start_JDay>end_season)\r\n      temps_JDay$Season[which(temps_JDay$JDay>=start_JDay)]<-\r\n      temps_JDay$Year[which(temps_JDay$JDay>=start_JDay)]+1\r\n    if(start_JDay>end_JDay)\r\n      sub_temps<-subset(temps_JDay,JDay<=end_JDay|JDay>=start_JDay)\r\n    if(start_JDay<=end_JDay)\r\n      sub_temps<-subset(temps_JDay,JDay<=end_JDay&JDay>=start_JDay)\r\n    mean_temps<-aggregate(sub_temps[,c(\"Tmin\",\"Tmax\")],\r\n                          by=list(sub_temps$Season),\r\n                          FUN=function(x) mean(x, na.rm=TRUE))\r\n    mean_temps[,\"n_days\"]<-aggregate(sub_temps[,\"Tmin\"],\r\n                                     by=list(sub_temps$Season),\r\n                                     FUN=length)[,2]\r\n    mean_temps[,\"Tmean\"]<-(mean_temps$Tmin+mean_temps$Tmax)/2\r\n    mean_temps<-mean_temps[,c(1,4,2,3,5)]\r\n    colnames(mean_temps)[1]<-\"End_year\"\r\n    return(mean_temps)\r\n    }\r\n  \r\n  mean_temp_chill<-mean_temp_period(temps = temps,\r\n                                    start_JDay = chill_phase[1],\r\n                                    end_JDay = chill_phase[2],\r\n                                    end_season = heat_phase[2])\r\n  \r\n  mean_temp_heat<-mean_temp_period(temps = temps,\r\n                                   start_JDay = heat_phase[1],\r\n                                   end_JDay = heat_phase[2],\r\n                                   end_season = heat_phase[2])\r\n  \r\n  mean_temp_chill<-\r\n    mean_temp_chill[which(mean_temp_chill$n_days >= \r\n                            max(mean_temp_chill$n_days)-1),]\r\n  mean_temp_heat<-\r\n    mean_temp_heat[which(mean_temp_heat$n_days >= \r\n                           max(mean_temp_heat$n_days)-1),]\r\n  mean_chill<-mean_temp_chill[,c(\"End_year\",\"Tmean\")]\r\n  colnames(mean_chill)[2]<-\"Tmean_chill\"\r\n  mean_heat<-mean_temp_heat[,c(\"End_year\",\"Tmean\")]\r\n  colnames(mean_heat)[2]<-\"Tmean_heat\"\r\n  phase_Tmeans<-merge(mean_chill,mean_heat, by=\"End_year\")\r\n  \r\n  colnames(pheno)<-c(\"End_year\",\"pheno\")\r\n  Tmeans_pheno<-merge(phase_Tmeans,pheno, by=\"End_year\")\r\n  \r\n  if(!is.na(exclude_years[1]))\r\n    Tmeans_pheno<-Tmeans_pheno[which(!Tmeans_pheno$End_year %in% exclude_years),]\r\n  \r\n  # Kriging interpolation\r\n  k<-Krig(x=as.matrix(Tmeans_pheno[,c(\"Tmean_chill\",\"Tmean_heat\")]),\r\n          Y=Tmeans_pheno$pheno)\r\n  pred<-predictSurface(k)\r\n  predictions<-as.data.frame(pred$z)\r\n  \r\n  colnames(predictions) <- pred$y\r\n  predictions <- data.frame(Tmean_chill = pred$x, predictions)\r\n\r\n  melted<-melt(predictions,na.rm=TRUE,id.vars=\"Tmean_chill\")\r\n  colnames(melted)<-c(\"Tmean_chill\",\"Tmean_heat\",\"value\")\r\n  melted$Tmean_heat<-unique(pred$y)[as.numeric(melted$Tmean_heat)]\r\n\r\n  \r\n  ggplot(melted,aes(x=Tmean_chill,y=Tmean_heat,z=value)) +\r\n    geom_contour_fill(bins=60) +\r\n    scale_fill_gradientn(colours=alpha(matlab.like(15)),\r\n                         name=paste(phenology_stage,\"date \\n(day of the year)\")) +\r\n    geom_contour(col=\"black\") +\r\n    geom_text_contour(stroke = 0.2) +\r\n    geom_point(data=Tmeans_pheno,\r\n               aes(x=Tmean_chill,y=Tmean_heat,z=NULL),\r\n               size=0.7)  +\r\n    ylab(expression(paste(\"Forcing phase \", T[mean],\" (\",degree,\"C)\"))) +\r\n    xlab(expression(paste(\"Chilling phase \", T[mean],\" (\",degree,\"C)\"))) +\r\n    theme_bw(base_size=15)\r\n}\r\n\r\nChill_model_sensitivity<-function(latitude,\r\n                                  temp_models=list(Dynamic_Model=Dynamic_Model,GDH=GDH),\r\n                                  month_range=c(10,11,12,1,2,3),\r\n                                  Tmins=c(-10:20),\r\n                                  Tmaxs=c(-5:30))\r\n  {\r\n  mins<-NA\r\n  maxs<-NA\r\n  metrics<-as.list(rep(NA,length(temp_models)))\r\n  names(metrics)<-names(temp_models)\r\n  month<-NA\r\n \r\n  for(mon in month_range)\r\n    {\r\n    days_month<-as.numeric(difftime( ISOdate(2002,mon+1,1),\r\n                                      ISOdate(2002,mon,1) ))\r\n    if(mon==12) days_month<-31\r\n    weather<-make_all_day_table(data.frame(Year=c(2001,2002),\r\n                                           Month=c(mon,mon),\r\n                                           Day=c(1,days_month),\r\n                                           Tmin=c(0,0),Tmax=c(0,0)))\r\n\r\n    \r\n    for(tmin in Tmins)\r\n      for(tmax in Tmaxs)\r\n        if(tmax>=tmin)\r\n          {\r\n          weather$Tmin<-tmin\r\n          weather$Tmax<-tmax\r\n          hourtemps<-stack_hourly_temps(weather,\r\n                                        latitude=latitude)$hourtemps$Temp\r\n          for(tm in 1:length(temp_models))\r\n           metrics[[tm]]<-c(metrics[[tm]],do.call(temp_models[[tm]],\r\n                                                  list(hourtemps))[length(hourtemps)]/(length(hourtemps)/24))\r\n          mins<-c(mins,tmin)\r\n          maxs<-c(maxs,tmax)\r\n          month<-c(month,mon)\r\n        }\r\n    }\r\n  results<-cbind(data.frame(Month=month,Tmin=mins,Tmax=maxs),\r\n                 as.data.frame(metrics))\r\n  results<-results[!is.na(results$Month),]\r\n}\r\n\r\n\r\nChill_sensitivity_temps<-function(chill_model_sensitivity_table,\r\n                                  temperatures,\r\n                                  temp_model,\r\n                                  month_range=c(10,11,12,1,2,3),\r\n                                  Tmins=c(-10:20),\r\n                                  Tmaxs=c(-5:30),\r\n                                  legend_label=\"Chill/day (CP)\")\r\n{\r\n  library(ggplot2)\r\n  library(colorRamps)\r\n\r\n  cmst<-chill_model_sensitivity_table\r\n  cmst<-cmst[which(cmst$Month %in% month_range),]\r\n  cmst$Month_names<- factor(cmst$Month, levels=month_range,\r\n                            labels=month.name[month_range])  \r\n  \r\n  DM_sensitivity<-ggplot(cmst,aes_string(x=\"Tmin\",y=\"Tmax\",fill=temp_model)) +\r\n    geom_tile() +\r\n    scale_fill_gradientn(colours=alpha(matlab.like(15), alpha = .5),\r\n                         name=legend_label) +\r\n    xlim(Tmins[1],Tmins[length(Tmins)]) +\r\n    ylim(Tmaxs[1],Tmaxs[length(Tmaxs)])\r\n  \r\n  temperatures<-\r\n    temperatures[which(temperatures$Month %in% month_range),]\r\n  temperatures[which(temperatures$Tmax<temperatures$Tmin),\r\n               c(\"Tmax\",\"Tmin\")]<-NA\r\n  temperatures$Month_names <- factor(temperatures$Month,\r\n                                     levels=month_range, labels=month.name[month_range])  \r\n  \r\n  DM_sensitivity +\r\n    geom_point(data=temperatures,\r\n               aes(x=Tmin,y=Tmax,fill=NULL,color=\"Temperature\"),\r\n               size=0.2) +\r\n    facet_wrap(vars(Month_names)) +\r\n    scale_color_manual(values = \"black\",\r\n                       labels = \"Daily temperature \\nextremes (°C)\",\r\n                       name=\"Observed at site\" ) +\r\n    guides(fill = guide_colorbar(order = 1),\r\n           color = guide_legend(order = 2)) +\r\n    ylab(\"Tmax (°C)\") +\r\n    xlab(\"Tmin (°C)\") + \r\n    theme_bw(base_size=15)\r\n\r\n}\r\n\r\n\r\n\r\n\r\npheno_data$Year <- pheno_data$Treatment + 2000\r\n\r\nweather_data$Year[which(weather_data$Month < 6)] <-\r\n  weather_data$Treatment[which(weather_data$Month < 6)] + 2000\r\n\r\nweather_data$Year[which(weather_data$Month >= 6)]<-\r\n  weather_data$Treatment[which(weather_data$Month >= 6)] + 1999\r\n\r\nday_month_from_JDay <- function(year,\r\n                                JDay)\r\n{\r\n  fulldate <- ISOdate(year - 1,\r\n                      12,\r\n                      31) + JDay * 3600 * 24\r\n  return(list(day(fulldate),\r\n              month(fulldate)))\r\n}\r\n\r\nweather_data$Day <- day_month_from_JDay(weather_data$Year,\r\n                                        weather_data$JDay)[[1]]\r\n\r\nweather_data$Month <- day_month_from_JDay(weather_data$Year,\r\n                                          weather_data$JDay)[[2]]\r\n\r\n\r\nWith the necessary functions and dataset preparations in place, the PLS analysis can now be conducted.\r\n\r\n\r\npls_out <- PLS_pheno(weather_data = weather_data,\r\n                     bio_data = pheno_data)\r\n\r\nggplot_PLS(pls_out)\r\n\r\n\r\n\r\nThe results provide a much clearer picture compared to the previous analysis in the chapter on Delineating temperature response phases with PLS regression for this location.\r\nNext, the same analysis will be conducted using agroclimatic metrics, specifically Chill Portions and Growing Degree Hours, to further explore temperature influences on phenology.\r\n\r\n\r\ntemps_hourly <- stack_hourly_temps(weather_data,\r\n                                   latitude = 50.6)\r\n\r\ndaychill <- daily_chill(hourtemps = temps_hourly,\r\n                        running_mean = 1,\r\n                        models = list(Chilling_Hours = Chilling_Hours,\r\n                                      Utah_Chill_Units = Utah_Model,\r\n                                      Chill_Portions = Dynamic_Model,\r\n                                      GDH = GDH)\r\n    )\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame = pheno_data[!is.na(pheno_data$pheno), ],\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\",\r\n                         runn_means = 11)\r\n\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Chill_Portions\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CP\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(-76,\r\n                                     10),\r\n                     heat_phase = c(17,\r\n                                    97.5))\r\n\r\n\r\n\r\nThe results now clearly define the chilling and forcing phases, allowing for the calculation of mean chill and heat accumulation, which approximate their respective requirements.\r\nTo account for anomalies in some treatments that led to unusual bloom predictions, the median rather than the mean is used for chill and heat accumulation. Additionally, the 25% and 75% quantiles of the distributions serve as an estimate of uncertainty.\r\n\r\n\r\nchill_phase <- c(290,\r\n                 10)\r\nheat_phase <- c(17,\r\n                97.5)\r\n\r\nchill <- tempResponse(hourtemps = temps_hourly,\r\n                      Start_JDay = chill_phase[1],\r\n                      End_JDay = chill_phase[2],\r\n                      models = list(Chill_Portions = Dynamic_Model),\r\n                      misstolerance = 10)\r\n\r\nheat <- tempResponse(hourtemps = temps_hourly,\r\n                     Start_JDay = heat_phase[1],\r\n                     End_JDay = heat_phase[2],\r\n                     models = list(GDH = GDH))\r\nchill_requirement <- median(chill$Chill_Portions)\r\nchill_req_error <- quantile(chill$Chill_Portions, \r\n                            c(0.25,\r\n                              0.75))\r\n\r\nheat_requirement <- median(heat$GDH)\r\nheat_req_error <- quantile(heat$GDH,\r\n                           c(0.25,\r\n                             0.75))\r\n\r\n\r\nThe estimated chilling requirement for this cultivar is approximately 48.2 Chill Portions (CP), with a 50% confidence interval ranging from 34.4 to 59 CP. The heat requirement is estimated at 11.709 Growing Degree Hours (GDH), with a 50% confidence interval between 6.615 and 16.387 GDH.\r\nNext, the temperature range at this location will be examined in relation to the temperature sensitivity of the Dynamic Model to better understand its impact on chill accumulation.\r\n\r\n\r\nModel_sensitivities_CKA <-\r\n  Chill_model_sensitivity(latitude = 50.6,\r\n                          temp_models = list(Dynamic_Model = Dynamic_Model,\r\n                                             GDH = GDH),\r\n                          month_range = c(10:12,\r\n                                          1:5))\r\nwrite.csv(Model_sensitivities_CKA,\r\n          \"data/Model_sensitivities_CKA.csv\",\r\n          row.names = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_CKA,\r\n                        weather_data,\r\n                        temp_model = \"Dynamic_Model\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        legend_label = \"Chill per day \\n(Chill Portions)\") +\r\n  ggtitle(\"Chill model sensitivity at Klein-Altendorf on steroids\")\r\n\r\n\r\n\r\nThis pattern appears more promising, as the temperature data covers a broad range of model variation. This suggests that the PLS analysis may provide a better representation of dormancy dynamics compared to the naturally observed dataset from Klein-Altendorf.\r\nNext, the temperature response plot will be examined to determine if a clear pattern emerges.\r\n\r\n\r\npheno_trend_ggplot(temps = weather_data,\r\n                   pheno = pheno_data[ ,c(\"Year\",\r\n                                          \"pheno\")],\r\n                   chill_phase = chill_phase,\r\n                   heat_phase = heat_phase,\r\n                   exclude_years = pheno_data$Year[is.na(pheno_data$pheno)],\r\n                   phenology_stage = \"Bloom\")\r\n\r\n\r\n\r\nA relatively clear temperature response pattern for Klein-Altendorf is now visible. However, some data points deviate from expected trends. This may be due to certain treatments that introduced temperature conditions far from typical orchard environments. These unusual temperature curves likely contributed to the observed irregularities in the pattern.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:31:24+01:00"
    },
    {
      "path": "estimate.html",
      "title": "A robust method to estimate future frost risks",
      "author": [],
      "contents": "\r\nFrost Risk in Future Scenarios\r\nAssessing spring frost risk in temperate fruit trees is crucial but insufficient for planning future adaptation. A comprehensive approach must estimate both the probability and severity of future frost events under climate change.\r\nSpring frost risk depends on temperature thresholds affecting plant tissues and the temperature-driven progression of tree phenology. Sensitivity increases as trees develop, with timing varying by species, cultivar, and location. Reliable analysis requires robust temperature projections and accurate bloom date modeling. Advances like future temperature scenarios and the PhenoFlex model enhance predictions.\r\nHistorical data from Campus Klein-Altendorf is combined with simulated conditions to assess relevance for future climates. The following plot illustrates this approach:\r\n\r\n\r\n# Import the past weather dataset to build the hull plots for model validity domains\r\n# Note: data from 01.01.1958 to 20.06.2019. Not complete season for bloom in 1958 (missing Oct-Dec 1957). Barely passing the threshold for bloom in 2019 (need to set mrange to c(9, 5) in genSeasonList for PhenoFlex calibration)\r\npast_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\n# Add the column SSP_Time for further discrimination with future scenarios\r\npast_weather$SSP_Time <- \"Past\"\r\n\r\n# Summarize the data by month. Compute the mean of Tmin and Tmax by RCP_Time, Year, and Month\r\npast_months <- past_weather %>%\r\n  group_by(SSP_Time, Year, Month) %>%\r\n  summarize(Tmin = mean(Tmin, na.rm = TRUE),\r\n            Tmax = mean(Tmax, na.rm = TRUE))\r\n\r\n\r\nunstructured_future_temps <- load_temperature_scenarios(\"data/future_climate\",\r\n                                                        \"Bonn_futuretemps\")\r\n\r\nfuture_temps <-\r\n  data.frame(bind_rows(lapply(unstructured_future_temps, bind_rows),\r\n                       .id = \"List\"))\r\n\r\nfuture_temps <- future_temps %>%\r\n  mutate(GCM = strsplit(List, '\\\\.') %>%\r\n           map(3) %>%\r\n           unlist(),\r\n         SSP = strsplit(List, '\\\\.') %>%\r\n           map(2) %>%\r\n           unlist(),\r\n         Time = strsplit(List, '\\\\.') %>%\r\n           map(4) %>%\r\n           unlist()) %>%\r\n  select(DATE, Year, Month, Day, Tmin, Tmax, SSP, Time, GCM)\r\n\r\nfuture_temps[future_temps$SSP == \"ssp126\",\"SSP_Time\"] <-\r\n  paste(\"SSP1\", future_temps[future_temps$SSP == \"ssp126\",\"Time\"])\r\n\r\nfuture_temps[future_temps$SSP == \"ssp245\",\"SSP_Time\"] <-\r\n  paste(\"SSP2\", future_temps[future_temps$SSP == \"ssp245\",\"Time\"])\r\n\r\nfuture_temps[future_temps$SSP == \"ssp370\",\"SSP_Time\"] <-\r\n  paste(\"SSP3\", future_temps[future_temps$SSP == \"ssp370\",\"Time\"])\r\n\r\nfuture_temps[future_temps$SSP == \"ssp585\",\"SSP_Time\"] <-\r\n  paste(\"SSP5\", future_temps[future_temps$SSP == \"ssp585\",\"Time\"])\r\n\r\n\r\n# Summarize the data by month. Compute the mean of Tmin and Tmax by RCP_Time, Year, and Month\r\nfuture_months <- future_temps %>%\r\n  group_by(SSP_Time, Year, Month) %>%\r\n  summarize(Tmin = mean(Tmin, na.rm = TRUE),\r\n            Tmax = mean(Tmax, na.rm = TRUE))\r\n  \r\n\r\n# Merge the past and future months to plot them together\r\nall_months <- rbind(past_months,\r\n                    future_months)\r\n\r\n# Add a column for the name of the month \r\nall_months$month_name <- factor(all_months$Month,\r\n                                levels = c(6 : 12, 1 : 5), \r\n                                labels = month.name[c(6 : 12, 1 : 5)])\r\n\r\n# Calculate the hulls for each group\r\nhull_temps <- all_months %>% \r\n  group_by(SSP_Time, month_name) %>%\r\n  slice(chull(Tmin, Tmax))\r\n\r\n# Load the weather data from the experimental seasons to generate an \"Enhanced\" temps category\r\nenhanced <- read_tab(\"data/final_weather_data_S1_S2_pear_hourly.csv\")\r\n\r\n# Summarize the data. Compute the minimum and maximum records in a daily basis\r\nenhanced <- enhanced %>% group_by(YEARMODA, Treatment, Year, Month ) %>% \r\n  summarize(Tmin = min(Temp, na.rm = TRUE),\r\n            Tmax = max(Temp, na.rm = TRUE))\r\n\r\n# Summarize the data by month now. Compute mean across minimum and maximum records\r\nenhanced <- enhanced %>% group_by(Treatment, Month) %>% summarize(Tmin = mean(Tmin),\r\n                                                                  Tmax = mean(Tmax))\r\n\r\n# Re-format the column Year and add the column RCP_Time\r\nenhanced$Year <- enhanced$Treatment\r\nenhanced$SSP_Time <- \"Past_enhanced\"\r\n\r\n# Combining the past and the enhanced temps\r\npast_months$SSP_Time <- \"Past combined\"\r\nenhanced$SSP_Time <- \"Past combined\"\r\n\r\n# Merge all the data. Simulated scenarios, observed scenarios, and enhanced temps\r\nall_months_both <- rbind(enhanced, past_months, future_months)\r\n\r\n# Add the labels for the month no\r\nall_months_both$month_name <- factor(all_months_both$Month, levels = c(6 : 12, 1 : 5), labels = month.name[c(6 : 12, 1 : 5)])\r\n\r\n# Create the hull\r\nhull_temps_both <- all_months_both %>% group_by(SSP_Time, month_name) %>% slice(chull(Tmin, Tmax))\r\n\r\n# Remove the temperature for the treatments excluded from the analysis. These treatments may be a bit unrealistic and therefore\r\n# difficult to be explained by the model\r\nall_months_both_conference <- filter(all_months_both, !(Year %in% c(3, 8, 9, 12, 13, 14, 15, 19,\r\n                                                                    23, 25, 26, 27, 28, 29, 32)))\r\n# Create the hull again\r\nhull_temps_both_conference <- all_months_both_conference %>%\r\n  group_by(SSP_Time,month_name) %>%\r\n  slice(chull(Tmin, Tmax))\r\n\r\nwrite.csv(hull_temps_both,\"data/hull_temps_both.csv\",row.names = FALSE)\r\nwrite.csv(hull_temps_both_conference, \"data/hull_temps_both_conference.csv\",row.names = FALSE)\r\n\r\n\r\n\r\n\r\nhull_temps_both <- read_tab(\"data/hull_temps_both.csv\")\r\n\r\nhull_temps_both$month_name <-\r\n  factor(hull_temps_both$Month,\r\n         levels = c(6:12, 1:5),\r\n         labels = month.name[c(6:12, 1:5)])\r\n\r\n# Implement the plot showing the overlap among conditions\r\nggplot(hull_temps_both[which(hull_temps_both$Month %in% c(10,11,12,1,2,3)),],\r\n       aes(Tmin, Tmax, fill = factor(SSP_Time))) +\r\n  geom_polygon() +\r\n  facet_wrap(vars(month_name)) +\r\n  scale_fill_manual(name=\"Scenario\",\r\n                    breaks=c(\"Past combined\",\r\n                             \"SSP1 2050\",\r\n                             \"SSP1 2085\",\r\n                             \"SSP2 2050\",\r\n                             \"SSP2 2085\",\r\n                             \"SSP3 2050\",\r\n                             \"SSP3 2085\",\r\n                             \"SSP5 2050\",\r\n                             \"SSP5 2085\"),\r\n                    values=c(\"black\",\r\n                             alpha(\"light green\",0.4),\r\n                             alpha(\"dark green\",0.4),\r\n                             alpha(\"coral\",0.4),\r\n                             alpha(\"dark red\",0.4),\r\n                             alpha(\"yellow\",0.4),\r\n                             alpha(\"orange\",0.4),                 \r\n                             alpha(\"light blue\",0.4),\r\n                             alpha(\"dark blue\",0.4))) +\r\n  xlab(\"Mean daily minimum temperature (°C)\") +\r\n  ylab(\"Mean daily maximum temperature (°C)\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nThis convex-hull plot shows the temperature coverage at CKA from 1958 to 2019, alongside all experimental seasons analyzed in the Experimentally Enhanced PLS chapter.\r\nTo integrate historical and experimental data into phenology modeling, records from the same species and cultivar (pear cultivar ‘Conference’) are required. Historical full bloom records for this cultivar exist from 1958 to 2019. However, to avoid distortions from extreme warming treatments, the experimental data used in the convex-hull plot may need refinement by excluding unusually warm seasons.\r\nThe following plot presents an updated version without these extreme conditions:\r\n\r\n\r\n\r\nThe coverage remains incomplete, as data for October and November is missing. Filling this gap is challenging since warmer conditions were not included in the experiments during these months.\r\nThe analysis will continue with the available data, anticipating that future identification of ‘normal seasons’ in the experimental dataset will improve the modeling domain’s validity.\r\nNext, the PhenoFlex dormancy modeling framework will be implemented by integrating historical and experimental records. The following code chunk imports and formats the data for model calibration.\r\n\r\n\r\n# Import the phenology data for the historical period 1958-2019\r\n# Note that we are selecting only the Year and full bloom columns\r\nhistoric_pheno_conference <- \r\n  read_tab(\"data/Pheno_pear_conference_1958_2019.csv\")[c(\"Year\",\r\n                                         \"Full_bloom\")]\r\n\r\n# Remove missing years\r\nhistoric_pheno_conference <- \r\n  historic_pheno_conference[which(!historic_pheno_conference$Full_bloom==\"\"),]\r\n\r\n# Add a column for the JDay\r\nhistoric_pheno_conference$Full_bloom <- \r\n  dormancyR::date_to_JDay(date = as.Date(historic_pheno_conference$Full_bloom,\r\n                                         format = \"%d.%m.%Y\"),\r\n                          format = \"%Y-%m-%d\")\r\n\r\n# Rename the columns\r\ncolnames(historic_pheno_conference) <- c(\"Year\", \"pheno\")\r\n\r\n# Do the same for the weather data\r\npast_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\n# Create hourly temps for compatibility with the experimental data set\r\npast_weather <- stack_hourly_temps(weather = past_weather, \r\n                                   latitude = 50.4)[[\"hourtemps\"]]\r\n\r\n\r\n# Load the experimental data\r\n# Load the data from the folder\r\nexp_weather <- read_tab(\"data/final_weather_data_S1_S2_pear_hourly.csv\")\r\n\r\n# Generate a new column (Year_2) to simulate the year and comply with the format of PhenoFlex functions\r\nexp_weather[\"Year_2\"] <- exp_weather$Treatment + exp_weather$Year + 3\r\n\r\n# Since this experiment was conducted during two consecutive seasons, the next step will fix a small continuity issue\r\n# generated during the season 2\r\nexp_weather[exp_weather$Treatment >= 17, \"Year_2\"] <-\r\n  exp_weather[exp_weather$Treatment >= 17, \"Year_2\"] - 1\r\n\r\n# For further compatibility, I will now select the columns needed and will drop \"Year\" (the original one)\r\nexp_weather <- exp_weather[c(\"YEARMODA\",\r\n                             \"Year_2\", \r\n                             \"Month\", \r\n                             \"Day\",\r\n                             \"Hour\", \r\n                             \"JDay\", \r\n                             \"Temp\")]\r\n\r\n# To replace the missing \"Year\" column, I will now change the name of the column\r\ncolnames(exp_weather)[which(colnames(exp_weather) == \"Year_2\")] <- \"Year\"\r\n\r\n\r\n# Import the phenology data from the repository\r\nexp_pheno <- read_tab(\"data/final_bio_data_S1_S2_pear.csv\")\r\n\r\nexp_pheno[\"Treatment\"] <- exp_pheno$Treatment + 2019 + 3\r\n\r\n# Remove conflictive treatments\r\nexp_pheno <- exp_pheno[!(exp_pheno$Treatment %in% c(2025, 2030, 2031, 2034, 2035, 2036, 2037, 2041,\r\n                                                    2045, 2047, 2048, 2049, 2050, 2051, 2054)),\r\n                       c(\"Treatment\", \"pheno\")]\r\n\r\n# Rename the columns to match the names of the historical dataset\r\ncolnames(exp_pheno) <- c(\"Year\", \"pheno\")\r\n\r\n\r\n# Merge the historical and experimental phenology data\r\npheno_merged <- bind_rows(filter(historic_pheno_conference, Year != 1958),\r\n                          exp_pheno)\r\n\r\n\r\n# Merge the historical and experimental phenology data\r\nweather_merged <- bind_rows(past_weather[, colnames(past_weather) %in% names(exp_weather)],\r\n                            exp_weather)\r\n\r\n\r\nThe historical and experimental data for phenology and weather records are now combined into a single dataset. The same procedure used in the chapter Can we improve the performance of PhenoFlex? can be applied to fit the PhenoFlex parameters to the data.\r\n\r\n\r\n# Define the season used for calibration and validation in the PhenoFlex modelling approach\r\ncalibration_seasons <- sort(sample(pheno_merged$Year, 50, replace = FALSE))\r\nvalidation_seasons <- sort(pheno_merged[!(pheno_merged$Year %in% calibration_seasons), \"Year\"])\r\n\r\n# Define the list of seasons (weather data)\r\nweather_season_list <- genSeasonList(weather_merged, mrange = c(9, 5), years = calibration_seasons)\r\n\r\n\r\nThe PhenoFlex model fitting procedure can now be applied, following the approach outlined in The PhenoFlex Model chapter. Initially, wide parameter ranges—particularly for yc and zc—are used to let the model determine optimal estimates. The fitted parameters and predicted bloom dates are then saved in the data folder.\r\n\r\n\r\n# Set the initial parameters (wide ranges)\r\n#          yc,  zc,  s1, Tu,     E0,      E1,     A0,          A1,   Tf, Tc, Tb, slope\r\nlower <- c(20, 100, 0.1,  0, 3000.0,  9000.0, 6000.0,       5.e13,    0,  0,  0,  0.05)\r\npar   <- c(40, 190, 0.5, 25, 3372.8,  9900.3, 6319.5, 5.939917e13,    4, 36,  4,  1.60)\r\nupper <- c(80, 500, 1.0, 30, 4000.0, 10000.0, 7000.0,       6.e13,   10, 40, 10, 50.00)\r\n\r\n# Run the fitter\r\npheno_fit <- phenologyFitter(par.guess = par,\r\n                             modelfn = PhenoFlex_GDHwrapper,\r\n                             bloomJDays = pheno_merged[pheno_merged$Year %in%\r\n                                                         calibration_seasons, \"pheno\"],\r\n                             SeasonList = weather_season_list,\r\n                             lower = lower,\r\n                             upper = upper,\r\n                             control = list(smooth = FALSE,\r\n                                            verbose = FALSE,\r\n                                            maxit = 2000,\r\n                                            nb.stop.improvement = 20))\r\n\r\n# Save the resulting parameters to folder (to avoid having to run the phenology fitter again)\r\nwrite.csv(pheno_fit$par, \"data/PhenoFlex_hist_exp_pear.csv\", row.names = FALSE)\r\n\r\n# Save the results of the predicted phenology for the calibration seasons\r\nwrite.csv(data.frame(pheno_merged[pheno_merged$Year %in% calibration_seasons, ],\r\n                     \"Predicted\" = pheno_fit$pbloomJDays), \"data/PhenoFlex_hist_exp_predicted_bloom_pear.csv\",\r\n          row.names = FALSE)\r\n\r\n\r\nThe results of the fitting procedure can now be examined. By using the fitted parameters in the model, the prediction error can be assessed by comparing the predicted bloom dates with the observed ones.\r\n\r\n\r\n# Read the parameters\r\nparams <- read.csv(\"data/PhenoFlex_hist_exp_pear.csv\")[[1]]\r\n\r\n# Generate a data set to collect the outputs of the fitting for the calibration data \r\nout_df <- read_tab(\"data/PhenoFlex_hist_exp_predicted_bloom_pear.csv\")\r\n\r\n# Compute the error (observed - predicted)\r\nout_df[[\"Error\"]] <- out_df$pheno - out_df$Predicted\r\n\r\n\r\n\r\n\r\n\r\nModel performance metrics can now be computed based on the estimated prediction errors. While not crucial during calibration, these metrics provide insight into the success of the calibration attempt.\r\n\r\n\r\n\r\nAlthough there is room for improvement—considering that only 10 iterations of the fitting procedure were used—the calibration still results in a relatively small RMSEP. A plot of the results will help visualize the overall performance.\r\n\r\n\r\n# Plot the results to see the overall fitting\r\nggplot(out_df, aes(pheno, Predicted)) +\r\n  geom_point() +\r\n  geom_abline(intercept = 0, slope = 1) +\r\n  labs(x = \"Observed\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nThis is a good initial calibration, but it may be worthwhile to test different par.guess values and adjust other arguments within phenologyFitter() to improve the model calibration.\r\nValidation\r\nNext, the model’s ability to predict bloom dates for seasons outside the calibration dataset should be evaluated. This requires extracting the model parameters and using the PhenoFlex_GDHwrapper() function to perform the predictions.\r\n\r\n\r\n# Generate a validation data set with phenology data\r\nvalid_df <- pheno_merged[pheno_merged$Year %in% validation_seasons, ]\r\n\r\n# Generate a list of seasons with weather data for the validation procedure\r\nvalid_season_list <- genSeasonList(weather_merged, mrange = c(9, 7), \r\n                                   years = validation_seasons)\r\n\r\n# Estimate the bloom dates with PhenoFlexGDHwrapper\r\nfor (i in 1 : nrow(valid_df)) {\r\n  \r\n  valid_df[i, \"Predicted\"] <- PhenoFlex_GDHwrapper(valid_season_list[[i]],\r\n                                                   params)\r\n}\r\n\r\n# Compute the error (observed - predicted)\r\nvalid_df[[\"Error\"]] <- valid_df$pheno - valid_df$Predicted\r\n\r\n\r\nSince the prediction error (the difference between observed and predicted values) in the validation dataset is known, model performance metrics such as RMSEP and RPIQ can be estimated. The RMSEP() and RPIQ() functions from the chillR package can be used for this purpose.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMetric\r\nCalibration\r\nValidation\r\nRMSEP\r\n4.72\r\n5.34\r\nRPIQ\r\n4.50\r\n4.12\r\n\r\nThe table results suggest that the PhenoFlex model performs well in predicting pear tree bloom dates under future conditions. To enhance robustness, multiple PhenoFlex fittings should be conducted to lower RMSEP values. Additionally, reducing the number of experimental seasons may help minimize the influence of extreme values and narrow the range of observed conditions in the warmer part of the model’s validity domain.\r\nFurther errors may stem from merging datasets collected using different methodologies, such as mature orchard trees versus young potted trees, or variations in observers across experiments. The fitting outcomes are analyzed below, with labels marking seasons where the prediction error exceeds 10 days.\r\n\r\n\r\n# Plot the validation and calibration results (highlight seasons with 10 or more days of error)\r\nggplot() +\r\n  geom_point(data = out_df, aes(pheno, Predicted, color = \"Calibration\")) +\r\n  geom_point(data = valid_df, aes(pheno, Predicted, color = \"Validation\")) + \r\n  ggrepel::geom_label_repel(aes(pheno, Predicted, label = Year),\r\n                            data = filter(out_df, abs(Error) > 10), nudge_y = 2, nudge_x = 6) +\r\n  ggrepel::geom_label_repel(aes(pheno, Predicted, label = Year),\r\n                            data = filter(valid_df, abs(Error) > 10), nudge_y = -1, nudge_x = 9, force = 4) +\r\n  scale_color_manual(values = c(\"cadetblue\", \"firebrick\")) +\r\n  geom_abline(intercept = 0, slope = 1) +\r\n  labs(x = \"Observed\",\r\n       color = \"Dataset\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nIf the validation approach is deemed satisfactory, the fitted parameters from previous model runs can be used to predict bloom dates under future scenarios. This involves utilizing the data from the Future Temperature Scenarios chapter, which has already been loaded into R for generating the convex-hull plots.\r\nThe function PhenoFlex_GDHwrapper() will be applied to estimate bloom dates under future conditions. The initial steps within the for loop ensure proper data formatting by deriving hourly temperature records with stack_hourly_temps() and defining the seasons using genSeasonList().\r\n\r\n\r\n# Create a primer data frame to allocate the results for future bloom\r\nfuture_bloom <- future_temps %>% \r\n  group_by(SSP_Time, \r\n           GCM, \r\n           Year) %>% \r\n  summarise(Pheno = NA)\r\n\r\n# Define the scenarios to be used in the for loop\r\nscenarios <- unique(future_temps$SSP_Time)\r\n\r\n# Define the climate models to be used in the for loop\r\nclimate_models <- unique(future_temps$GCM)\r\n\r\n# Implement the for loop\r\nfor (scenario in scenarios){\r\n  scen_file <- future_temps %>% \r\n    filter(SSP_Time == scenario)\r\n  \r\n  climate_models <- unique(scen_file$GCM)\r\n  \r\n  for (climate_model in climate_models){\r\n    \r\n    # Subset a temporary data frame according to scenario and climate model\r\n    # and generate hourly temperatures\r\n    temp_df <- filter(scen_file,\r\n                      GCM == climate_model) %>%\r\n      stack_hourly_temps(latitude = 50.4)\r\n    \r\n    # Define the saeasons to be used for predicting the phenology\r\n    temp_seasons_list <- genSeasonList(temp_df$hourtemps, \r\n                                       mrange = c(9, 7),\r\n                                       years = c(2002 : 2101))\r\n    \r\n    # Change the names of the list of seasons to be used as index in the next for loop\r\n    names(temp_seasons_list) <- 2002 : 2101\r\n    \r\n    # Implement a for loop that runs over the list of seasons to estimate the bloom date using the parameters fitted by the\r\n    # model\r\n    for (i in 1 : length(temp_seasons_list)){\r\n      \r\n      # Add the bloom date to the primer data set\r\n      future_bloom[future_bloom$SSP_Time == scenario & \r\n                     future_bloom$GCM == climate_model & \r\n                     future_bloom$Year == names(temp_seasons_list)[i], \r\n                   \"Pheno\"] <-\r\n        PhenoFlex_GDHwrapper(temp_seasons_list[[i]],\r\n                             params)\r\n    }\r\n  }\r\n}\r\n\r\n\r\n\r\n\r\n\r\nThe same procedure can now be applied to estimate bloom dates for historically simulated scenarios.\r\n\r\n\r\n# Bloom for past simulated scenarios ####\r\n# Load the historical simulated scenarios\r\ntemps_past_scenarios <- load_temperature_scenarios(\"data\",\r\n                                    \"Bonn_hist_scenarios\")\r\n\r\n# Create a single dataset for all the simulated years\r\ntemps_past_scenarios <- bind_rows(temps_past_scenarios,\r\n                                  .id = \"Scen_year\")\r\n\r\n# Make a primer dataset to allocate the results of the bloom projection\r\nsimulated_bloom <- temps_past_scenarios %>% \r\n  group_by(Scen_year, Year) %>% \r\n  summarise(Pheno = NA)\r\n\r\n# Define the scenario years to be used in the loop\r\nscen_years <- unique(simulated_bloom$Scen_year)\r\n\r\n# Implement the for loop\r\nfor (scen_year in scen_years){\r\n  \r\n  # Subset a temporary data frame according to scenario and climate model\r\n  temp_df <- filter(temps_past_scenarios, \r\n                    Scen_year == scen_year)\r\n  \r\n  # Generate hourly temperatures in the temporary data frame\r\n  temp_df <- stack_hourly_temps(temp_df,\r\n                                latitude = 50.4)\r\n  \r\n  # Define the saeasons to be used for predicting the phenology\r\n  temp_seasons_list <- genSeasonList(temp_df$hourtemps, \r\n                                     mrange = c(9, 7), \r\n                                     years = c(2002 : 2101))\r\n  \r\n  # Change the names of the list of seasons to be used as index in the next for loop\r\n  names(temp_seasons_list) <- 2002 : 2101\r\n  \r\n  # Implement a for loop that runs over the list of seasons to estimate the bloom date using the parameters fitted by the\r\n  # model\r\n  for (i in 1 : length(temp_seasons_list)){\r\n    \r\n    # Add the bloom date to the primer data set\r\n    simulated_bloom[simulated_bloom$Scen_year == scen_year & \r\n                      simulated_bloom$Year == names(temp_seasons_list)[i], \r\n                    \"Pheno\"] <- \r\n      PhenoFlex_GDHwrapper(temp_seasons_list[[i]],\r\n                                                                                                            params)\r\n  }\r\n}\r\n\r\nwrite.csv(simulated_bloom, \"data/frost_simulated_bloom.csv\", row.names = FALSE)\r\n\r\n\r\nA plot will now be created to illustrate likely bloom dates for historical and future scenarios, incorporating calibration data for the PhenoFlex model. Since the two datasets have different axes, the plotting process is complex and follows these steps:\r\nGenerate a violin plot for simulated bloom data from past scenarios.\r\nExtract x and y axis limits using layer_scales().\r\nCreate a density plot of calibration data (distinguishing between historical and experimental data) and extract its data using ggplot_build().\r\nConvert the extracted density data into numeric values that align with the year axis from the first plot.\r\nAdd this density data to the initial plot using geom_ribbon().\r\nPlot future bloom data separately.\r\nMerge both plots using the patchwork package.\r\nSave the final plot as an image with ggsave().\r\nDespite necessary adjustments and challenges in aligning both datasets, the resulting visualization effectively presents the intended comparison.\r\n\r\n\r\n\r\n\r\n\r\n# Plot all bloom prediction results\r\n\r\n# Plot showing the violins for historical simulated scenarios\r\npast_simulated_plot <-\r\n  ggplot(data = simulated_bloom) +\r\n  geom_violin(\r\n    aes(x = Scen_year,\r\n        group = Scen_year, \r\n        y = Pheno),\r\n    linewidth = 0.35,\r\n    draw_quantiles = c(0.25, 0.5, 0.75),\r\n    alpha = 0.5) +\r\n  facet_grid(~ \"Historical\") +\r\n  theme_bw() +\r\n  theme(axis.title.x=element_blank(),\r\n        axis.title.y=element_blank(),\r\n        plot.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid = element_blank(),\r\n        aspect.ratio = 4)\r\n\r\npast_xlim <- layer_scales(past_simulated_plot)$x$range$range\r\npast_ylim <- layer_scales(past_simulated_plot)$y$range$range\r\n\r\n# Make a density plot to show the distribution of the seasons used for calibration of the PhenoFlex modelling framework\r\n# Create a dataframe with columns to be used in the plot\r\nobserved_bloom_calibration <-\r\n  data.frame(out_df,\r\n             Facet = \"Past scenarios\",\r\n             Dataset = if_else(out_df$Year > 2010, \r\n                               \"Experimental\", \r\n                               \"Historical\"))\r\n\r\n# Make a density plot of the bloom dates included in the calibration dataset\r\ngg_density_plot <-   \r\n  ggplot() +\r\n  geom_density(data = observed_bloom_calibration,\r\n               aes(y = pheno, \r\n                   fill = Dataset)\r\n  )\r\n\r\n# Extract data from the plot\r\ndensity_data <- \r\n  gg_density_plot %>% \r\n  ggplot_build() %>%\r\n  pluck(1,1) %>% \r\n  select(x:y,group)\r\n\r\n\r\n# Scale data so that they can be plotted on the Year axis\r\ndensity_data[density_data$group == 1, \"Dataset\"] <- \"Experimental\"\r\ndensity_data[density_data$group == 2, \"Dataset\"] <- \"Historical\"\r\ndensity_data <- \r\n  density_data %>%\r\n  mutate(density_scaled = x / max(x),\r\n         density_scaled = density_scaled * (past_xlim[2] - \r\n                                              past_xlim[1]) * 0.9 + past_xlim[1])\r\n\r\n# merge the two plots (and plot the violin plot again, so that it's placed\r\n# on top)\r\n\r\npast_observed_plot <- \r\n  past_simulated_plot +\r\n  geom_ribbon(data = density_data,\r\n            stat = \"identity\",\r\n            position = \"identity\",\r\n            aes(xmin = past_xlim[1],\r\n                xmax = density_scaled,\r\n                y = y,\r\n                fill = Dataset),\r\n         alpha = 0.75) +\r\n  geom_violin(data = simulated_bloom,\r\n    aes(x = Scen_year,\r\n        group = Scen_year, \r\n        y = Pheno),\r\n    size = 0.35,\r\n    draw_quantiles = c(0.25, 0.5, 0.75),\r\n    alpha = 0.5) +\r\n  scale_y_continuous(limits = c(10,160),\r\n                     labels = function (x) \r\n                       format(dormancyR::JDay_to_date(x, \r\n                                                      2001,\r\n                                                      na.rm = TRUE),\r\n                              \"%b %d\")) +\r\n  theme(legend.position = c(0.5, 0.1),\r\n        axis.text.x = element_text(size = 8, \r\n                                   angle = 45,\r\n                                   vjust = .4, \r\n                                   hjust = .6),\r\n        legend.text = element_text(size = 6),\r\n        legend.title = element_text(size = 8))\r\n\r\n\r\n# Create a violin plot to show future bloom dates\r\nfuture_bloom_plot <- ggplot(na.omit(future_bloom),\r\n                            aes(GCM, Pheno, fill = GCM)) +\r\n  geom_violin(size = 0.35,\r\n              draw_quantiles = c(0.25, 0.75)) +\r\n  stat_summary(fun = \"median\", \r\n               geom = \"point\",\r\n               shape = 4,\r\n               size = 0.8) +\r\n  scale_y_continuous(limits = c(10, 160),\r\n                     labels = function (x)\r\n                       format(dormancyR::JDay_to_date(x,\r\n                                                      2001,\r\n                                                      na.rm = TRUE), \r\n                              \"%b %d\"))  +\r\n  facet_grid(~ SSP_Time) +\r\n  theme_bw() +\r\n  theme(legend.position = \"bottom\",\r\n        legend.key.size = unit(0.4, \"cm\"),\r\n        legend.text = element_text(size = 7),\r\n        axis.title = element_blank(),\r\n        axis.text = element_blank(),\r\n        axis.ticks = element_blank()\r\n       )\r\n\r\n# Combine plots\r\nlibrary(patchwork)\r\n\r\npast_observed_plot +\r\n             future_bloom_plot\r\n\r\n\r\n\r\n\r\n# Save the plot to the folder\r\nggsave(\"images/future_pheno_pear.png\", \r\n       device = \"png\", \r\n       width = 19, \r\n       height = 15,\r\n       units = \"cm\", \r\n       dpi = 600)\r\n\r\n\r\n\r\nThe results indicate that bloom in pear cultivar ‘Conference’ is expected to occur slightly earlier under future climate scenarios. The next step is to assess the probability of frost events during the flowering period. To do this, a bloom period of 10 days around the predicted full bloom date for each simulated year will be considered, and temperature data during this period will be analyzed.\r\n\r\n\r\n# Frost risk part ####\r\n# Add the \"buffer\" around the estimated bloom dates to compute the number of hours below 0 °C in this period. Hypothetically, this represents the\r\n# period of bloom in all scenarios\r\n\r\n# Define the buffer only once\r\nbloom_buffer <- 5\r\n\r\n# Mutate the future bloom dataset to add the beggining and end for bloom\r\nfuture_bloom <- future_bloom %>% \r\n  mutate(Beginning = Pheno - bloom_buffer,\r\n         End = Pheno + bloom_buffer)\r\n\r\n# Do the same with the observed bloom dataset (i.e. calibration)\r\nobserved_bloom_calibration <- observed_bloom_calibration %>% \r\n  mutate(Beginning = pheno - bloom_buffer,\r\n         End = pheno + bloom_buffer)\r\n\r\n# Same with the past simulated data from historical scenarios\r\nsimulated_bloom <- simulated_bloom %>%\r\n  mutate(Beginning = Pheno - bloom_buffer,\r\n         End = Pheno + bloom_buffer)\r\n\r\n\r\nThe number of frost hours expected under future and historical simulated scenarios will now be computed using multiple for loops. This approach will allow for a detailed assessment of frost risk during the bloom period across different climate conditions.\r\n\r\n\r\nfor (i in 1 : nrow(observed_bloom_calibration)){\r\n  \r\n  # Extract the beginning and end date for observed bloom\r\n  beg_bloom <- observed_bloom_calibration[i, \"Beginning\"]\r\n  \r\n  end_bloom <- observed_bloom_calibration[i, \"End\"]\r\n  \r\n  # Subset a temporary dataframe of weather records for the year of interest\r\n  temp_df <- filter(weather_merged, \r\n                    Year == observed_bloom_calibration[i, \"Year\"])\r\n  \r\n  # Add the Julian date\r\n  temp_df <- make_JDay(temp_df)\r\n  \r\n  # Filter only the period for bloom\r\n  temp_df <- filter(temp_df,\r\n                    JDay %in% c(beg_bloom : end_bloom))\r\n  \r\n  # Calculate the number of hours below 0 °C\r\n  frost_hours <- max(dormancyR::frost_risk(temp_df$Temp, \r\n                                           threshold = 0))\r\n  \r\n  # Add the number of hours to the original dataframe\r\n  observed_bloom_calibration[i, \"Frost\"] <- frost_hours\r\n  \r\n}\r\n\r\n\r\nThe same procedure is applied to both past simulated and future scenarios.\r\n\r\n\r\n# Make a primer dataset to allocate the results of the frost projections\r\nsimulated_frost <- temps_past_scenarios %>% \r\n  group_by(Scen_year, Year) %>% \r\n  summarise(Frost = NA)\r\n\r\n# Implement the loop over scenario years and simulated years \r\nfor (scen_year in scen_years)\r\n  for (year in c(2002 : 2100)){\r\n    \r\n    if(!is.na(filter(simulated_bloom, \r\n                     Scen_year == scen_year & \r\n                     Year == year)[[\"Pheno\"]]   ))\r\n      {\r\n      \r\n      # Extract the beginning and end of the blooming period\r\n      beg_bloom <- filter(na.omit(simulated_bloom), \r\n                          Scen_year == scen_year & \r\n                            Year == year)[[\"Beginning\"]]\r\n      \r\n      beg_bloom <- trunc(beg_bloom)\r\n      \r\n      end_bloom <- filter(na.omit(simulated_bloom), \r\n                          Scen_year == scen_year, \r\n                          Year == year)[[\"End\"]]\r\n      \r\n      end_bloom <- trunc(end_bloom)\r\n      \r\n      # Filter the weather data according to the relevant scenario year and simulated year\r\n      \r\n      temp_df <- filter(temps_past_scenarios, \r\n                        Scen_year == scen_year &\r\n                          Year == year)\r\n      \r\n      # Derive hourly temperatures based on the latitude of Campus Klein-Altendorf\r\n      temp_df <- stack_hourly_temps(temp_df,\r\n                                    latitude = 50.4)[[\"hourtemps\"]]\r\n      \r\n      # Filter the relevant period when bloom is likely to occurs\r\n      \r\n      temp_df <- filter(temp_df, \r\n                        JDay %in% c(beg_bloom : end_bloom))\r\n      \r\n      # Compute the number of frost hours\r\n      frost_hours <- max(dormancyR::frost_risk(temp_df$Temp, \r\n                                               threshold = 0))\r\n      } else frost_hours <- 0\r\n      \r\n      # Add the number of frost events to the primer dataset\r\n      \r\n      simulated_frost[simulated_frost$Scen_year == scen_year & \r\n                      simulated_frost$Year == year, \"Frost\"] <- frost_hours\r\n\r\n}\r\n\r\n# Remove NA cells since the first year has no bloom\r\nsimulated_frost <- na.omit(simulated_frost)\r\n\r\n\r\n\r\n\r\n\r\nNext, the future scenarios are processed using the same methodology.\r\n\r\n\r\n# Create a primer data frame to allocate the results for future bloom\r\nfuture_frost <- future_temps %>%\r\n  group_by(SSP_Time, GCM, Year) %>% \r\n  summarise(Frost = NA)\r\n\r\n# Implement the for loop\r\nfor (scenario in scenarios)\r\n    {scen_file <- future_bloom %>% \r\n      filter(SSP_Time == scenario)\r\n    climate_models <- unique(scen_file$GCM)\r\n    \r\n    for (climate_model in climate_models)\r\n      for (year in c(2002 : 2100)){\r\n      \r\n        if(!is.na(filter(scen_file,\r\n                         GCM == climate_model & \r\n                         Year == year)[[\"Pheno\"]]   ))\r\n        {\r\n          beg_bloom <- filter(na.omit(scen_file),\r\n                              GCM == climate_model &\r\n                                Year == year)[[\"Beginning\"]]\r\n          \r\n          beg_bloom <- trunc(beg_bloom)\r\n          \r\n          end_bloom <- filter(na.omit(scen_file),\r\n                              GCM == climate_model &\r\n                                Year == year)[[\"End\"]]\r\n          \r\n          end_bloom <- trunc(end_bloom)\r\n          \r\n          # Derive hourly temperatures based on the latitude of Campus Klein-Altendorf\r\n          \r\n          temp_df <- filter(future_temps,\r\n                            SSP_Time == scenario & \r\n                              GCM == climate_model &\r\n                              Year == year)\r\n          \r\n          temp_df <- stack_hourly_temps(temp_df,\r\n                                        latitude = 50.4)[[\"hourtemps\"]]\r\n          \r\n          temp_df <- filter(temp_df,\r\n                            JDay %in% c(beg_bloom : end_bloom))\r\n          \r\n          frost_hours <- max(dormancyR::frost_risk(temp_df$Temp,\r\n                                                   threshold = 0))\r\n          } else frost_hours <- 0\r\n          \r\n          future_frost[future_frost$SSP_Time == scenario & \r\n                         future_frost$GCM == climate_model &\r\n                         future_frost$Year == year, \"Frost\"] <- frost_hours\r\n      }\r\n    }\r\n# Remove NA cells since the first year has no bloom\r\nfuture_frost <- na.omit(future_frost)\r\n\r\n\r\n\r\n\r\n\r\nNow, the results are visualized using a standard approach for displaying agro-climatic projections. Two separate plots are generated—one for past simulated scenarios and one for future projections—and then combined using the patchwork package to provide a comprehensive overview of frost risk trends.\r\n\r\n\r\n\r\n\r\n\r\n# Plots\r\npast_frost_plot <- ggplot(simulated_frost,\r\n                          aes(Scen_year, \r\n                              Frost,\r\n                              group = Scen_year)) +\r\n  geom_boxplot(fill = \"cadetblue\",\r\n               outlier.size = 0.8, \r\n               outlier.alpha = 0.5, \r\n               outlier.shape = 1) +\r\n  #geom_point(data = observed_bloom_calibration, aes(as.character(Year), Frost)) +\r\n  scale_y_continuous(limits = c(0, 90), \r\n                     expand = expansion(mult = 0)) +\r\n  labs(x = \"Year\",\r\n       y = \"Spring frost (hours <0 °C)\") +\r\n  facet_grid(~ \"Historical\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(size = 7, \r\n                                   angle = 60, \r\n                                   hjust = 1))\r\n\r\nfuture_frost_plot <- ggplot(future_frost,\r\n                            aes(GCM, \r\n                                Frost, \r\n                                fill = GCM)) +\r\n  geom_boxplot(outlier.size = 0.8, \r\n               outlier.alpha = 0.5, \r\n               outlier.shape = 1) +\r\n  scale_y_continuous(limits = c(0, 90),\r\n                     expand = expansion(mult = 0)) +\r\n  facet_grid(~ SSP_Time) +\r\n  theme_bw() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        legend.position = \"bottom\",\r\n        legend.text = element_text(size = 7.5),\r\n        legend.key.size = unit(0.3, \"cm\"))\r\n\r\npast_frost_plot + \r\n  future_frost_plot + \r\n  plot_layout(widths = c(0.25, 1))\r\n\r\n\r\n\r\nAn interesting observation is the high number of outliers present in each simulated scenario. Despite projections indicating an earlier bloom period for pear cv. Conference under future climate scenarios, the overall trend suggests a slight decline in the risk of spring frost events.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:33:25+01:00"
    },
    {
      "path": "examples.html",
      "title": "Examples of PLS regression with agroclimatic metrics",
      "author": [],
      "contents": "\r\nPLS regression across species and agroclimatic contexts\r\nSince 2012, PLS regression with agroclimatic metrics (chill and heat) has been applied in various contexts. While some researchers have adopted this approach, this chapter focuses on studies involving the author.\r\nChestnut, jujube and apricot in Beijing\r\nOne of the coldest locations where this approach was applied is Beijing, where research led by Guo Liang used bloom data for Chinese chestnut (Castanea mollissima) and jujube (Ziziphus jujuba) to define chilling and forcing periods (Guo Liang et al., 2014). Another study analyzed datasets for apricot (Prunus armeniaca) and mountain peach (Prunus davidiana), yielding the following results:\r\nResults of a PLS analysis based on the relationship between daily chill (quantified with the Dynamic Model) and heat (quantified with the GDH model) accumulation and bloom of Chinese chestnut (Castanea mollissima) in Beijing, China (Guo et al., 2014a)Results of a PLS analysis based on the relationship between daily chill (quantified with the Dynamic Model) and heat (quantified with the GDH model) accumulation and bloom of jujube (Ziziphus jujuba) in Beijing, China (Guo et al., 2014a)Results of a PLS analysis based on the relationship between daily chill (quantified with the Dynamic Model) and heat (quantified with the GDH model) accumulation and bloom of mountain peach (Prunus davidiana) in Beijing, China (Guo et al., 2014b)Results of a PLS analysis based on the relationship between daily chill (quantified with the Dynamic Model) and heat (quantified with the GDH model) accumulation and bloom of mountain peach (Prunus davidiana) in Beijing, China (Guo et al., 2014b)Results of a PLS analysis based on the relationship between daily chill (quantified with the Dynamic Model) and heat (quantified with the GDH model) accumulation and bloom of apricot (Prunus armeniaca) in Beijing, China (Guo et al., 2014b)For apricots, PLS regressions were conducted using multiple chill metrics, including Chilling Hours, the Utah Model (Chill Units), and the Dynamic Model (Chill Portions):\r\nResults of a PLS analysis based on the relationship between daily chill and heat accumulation and bloom of apricot (Prunus armeniaca) in Beijing, China. Coefficients for heat are not shown here (they are similar to what’s shown in the previous figure). Chill accumulation was quantified with the Chilling Hours Model (left), the Utah Model (middle) and the Dynamic Model (right) (Guo et al., 2015b)In all analyses of phenology records from Beijing, the forcing period was easy to delineate, but the chilling period was difficult to see.\r\nApples in Shaanxi Province, China\r\nGuo Liang also led a study on apple phenology in Shaanxi, one of China’s main apple growing provinces:\r\nResults of a PLS analysis of the relationship between chill (in Chill Portions) and heat (in GDH) and bloom dates of apple in Shaanxi, China (Guo et al., 2019)Also here the chilling phase was visible but difficult to delineate.\r\nCherries in Klein-Altendorf\r\nWinters in Beijing and Shaanxi are quite cold. A slightly warmer location was analyzed by studying cherries in Klein-Altendorf.\r\nResults of a PLS analysis of bloom dates of cherries ‘Schneiders späte Knorpelkirsche’ in Klein-Altendorf, Germany, based on chill (in Chill Portions) and heat (in GDH) accumulation (Luedeling et al., 2013a)Again, it’s pretty difficult to see the chilling period.\r\nApricots in the UK\r\nFor apricots in the UK National Fruit Collection at Brogdale Farm, Faversham, a clear chill response phase was observed in January and February. However, this period begins later than the typical expected start of chill accumulation.\r\nResults of a PLS analysis of apricot bloom in the southern UK, based on chill accumulation (in Chill Portions) and heat accumulation (in GDH) (Martı́nez-Lüscher et al., 2017)Grapevine in Croatia\r\nGrapes also have chilling requirements. Johann Johann Martínez-Lüscher, who led the UK apricot study, analyzed the temperature response of grapes grown in Croatia.\r\nResults of a PLS analysis of flowering dates of grapevine (cv. ‘Riesling Italico’) in Mandicevac, Croatia. Chill was quantified with the Dynamic Model, heat with the Growing Degree Hours Model (Martı́nez-Lüscher et al., 2016)In Croatia, where winters are warmer and chill accumulation rates are more variable, the chilling period is more distinct. Bloom response to chill is particularly strong in December and January. With a broader interpretation, the chilling period could extend from late September to February.\r\nWalnuts in California\r\nIn California, where winters are even warmer, the chilling period for walnuts was evident both from raw temperature data and when using agroclimatic metrics.\r\nResults of a PLS analysis of leaf emergence dates of walnuts (cv. ‘Payne’) in Davis, California. Chill was quantified with the Dynamic Model, heat with the Growing Degree Hours Model (Luedeling et al., 2013a)The analysis again reveals a distinct chilling period, seemingly divided into two phases. The reason for this split remains unclear and may be worth further investigation. However, a clear bloom response to high chill accumulation rates is observed between mid-October and late December.\r\nAlmonds in Tunisia\r\nSfax, in central Tunisia, represents an even warmer climate near the cultivation limit for temperate nut trees. A study led by Haïfa Benmoussa analyzed bloom data from 37 almond cultivars, successfully identifying both the chilling and forcing periods in nearly all cases. The following figures illustrate these findings.\r\nPLS results for almond cultivars near Sfax, Tunisia - part 1 (Benmoussa et al., 2017a)PLS results for almond cultivars near Sfax, Tunisia - part 2 (Benmoussa et al., 2017a)PLS results for almond cultivars near Sfax, Tunisia - part 3 (Benmoussa et al., 2017a)Pistachios in Tunisia\r\nPistachio data from the same experimental station in Sfax, Tunisia, was also analyzed. The results revealed a long chilling period with strong responses to chill accumulation rates. However, the forcing period was less distinct.\r\nPLS results for pistachios near Sfax, Tunisia (Benmoussa et al., 2017b)Exercises on examples of PLS regression with agroclimatic metrics\r\nLook across all the PLS results presented above. Can you detect a pattern in where chilling and forcing periods could be delineated clearly, and where this attempt failed?\r\nLooking at the PLS results across different locations and crops, a clear pattern emerges regarding the delineation of chilling and forcing periods:\r\nChilling Period Identification:\r\nIn colder regions like Beijing, Shaanxi, and Croatia, chilling periods were generally well-defined, often spanning from late autumn to mid-winter.\r\nIn moderately cold locations like Klein-Altendorf and Brogdale Farm (UK), chilling phases were also visible, but in some cases, they appeared later than expected.\r\nIn warm locations like Sfax (Tunisia) and California, chilling periods could still be detected, but they sometimes appeared fragmented or extended over a longer time frame.\r\n\r\nForcing Period Identification:\r\nIn most locations, forcing periods were clearly visible and followed the expected seasonal pattern.\r\nHowever, in pistachios from Sfax, the forcing phase was difficult to identify, suggesting that temperature alone may not be the primary driver of bud development in this case.\r\n\r\nThink about possible reasons for the success or failure of PLS analysis based on agroclimatic metrics. Write down your thoughts.\r\nReasons for Success or Failure of PLS Analysis:\r\nTemperature Variability:\r\nIn regions with distinct seasonal temperature changes (e.g., Beijing, Croatia), PLS was effective in identifying chilling and forcing phases\r\nIn warmer areas with mild winters (e.g., Sfax, California), chill accumulation was more gradual, leading to less distinct responses\r\n\r\nChilling Model Accuracy:\r\nDifferent crops may respond to chilling in unique ways, and the effectiveness of PLS depends on how well the selected agroclimatic metric captures the true physiological response of the plants\r\nThe Dynamic Model often provided better results than simpler models like Chilling Hours or the Utah Model\r\n\r\nThreshold Effects and Physiological Responses:\r\nIn some cases, the chilling phase appeared to consist of two parts, suggesting that additional factors (e.g., photoperiod, water availability) may influence dormancy release\r\nThe inability to detect a forcing period in pistachios suggests that heat accumulation alone might not fully explain bud development in this crop\r\n\r\nClimatic Extremes:\r\nExtremely cold winters (e.g., Beijing, Shaanxi) might lead to periods where chill accumulation is excessive, making further responses difficult to detect\r\nIn very warm climates (e.g., Sfax), chill accumulation might be insufficient in some years, causing irregular dormancy release patterns\r\n\r\n",
      "last_modified": "2025-03-15T09:33:29+01:00"
    },
    {
      "path": "frost_risk.html",
      "title": " Frost risk analysis",
      "author": [],
      "contents": "\r\nSpring frost\r\nThe focus of this chapter is on assessing spring frost risk during flowering. After budbreak, trees become highly vulnerable to frost, with emerging flowers particularly susceptible to damage from freezing temperatures. Since all tree fruits originate from flowers, severe frost damage can result in significant yield losses or even complete crop failure.\r\nIn April 2017, a severe spring frost event, with multiple nights reaching around -5°C, caused significant damage to flowers, leading to drastically reduced yields. This event created major challenges for fruit growers. While such occurrences are not unprecedented, it raised concerns about whether climate change is increasing the risk of frost damage.\r\nUnusual weather events are often attributed to climate change, but natural climate variability has always played a role. The key question is whether 2017 was an isolated extreme or an indication of a shifting climate pattern, where spring frosts become more frequent and problematic for growers.\r\nUnderstanding this risk is crucial for two major decisions in fruit production:\r\nFrost protection investments: Growers can mitigate frost damage using protection strategies like sprinklers, candles, or wind machines. However, these require significant investments, which are only justified if frost events occur frequently. Recent research by Schmitz et al. (2025) evaluated various frost protection measures using a probabilistic decision model.\r\nCultivar selection: Late-flowering cultivars are generally less exposed to frost risk than early-flowering ones. However, they may have disadvantages, such as lower market prices at harvest or other agronomic trade-offs. A clear understanding of frost risk would help optimize these choices.\r\nThis analysis will focus on the experimental station of the University of Bonn at Klein-Altendorf:\r\n\r\n\r\n\r\n\r\n\r\nleaflet() %>%\r\n  setView(lng = 6.99,\r\n          lat = 50.625,\r\n          zoom = 12) %>%\r\n  addTiles() %>%\r\n  addMarkers(lng = 6.99,\r\n             lat = 50.625,\r\n             popup = \"Campus Klein-Altendorf\")\r\n\r\n\r\n\r\n\r\nFigure 1: Location of Campus Klein-Altendorf, an experimental station of the University of Bonn\r\n\r\n\r\n\r\nA key advantage of this experimental station is its long history of phenology data collection, maintained by multiple generations of technical staff since the 1950s. Additionally, high-quality weather data is available for the entire period. This provides a unique opportunity to conduct a comprehensive frost risk assessment using historical data.\r\nLeveraging this dataset and prior knowledge from this module, the goal is to develop a state-of-the-art frost risk analysis. The first step involves loading long-term weather and bloom datasets into R.\r\n\r\n\r\nCKA_Alexander_Lucas <- read_tab(\"data/Alexander_Lucas_bloom_1958_2019.csv\")\r\nCKA_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\n\r\nPhenology trend analysis\r\nHere’s what the bloom dataset CKA_Alexander_Lucas looks like:\r\n\r\n\r\nhead(CKA_Alexander_Lucas)\r\n\r\n\r\n\r\n\r\nPheno_year\r\n\r\n\r\nFirst_bloom\r\n\r\n\r\nFull_bloom\r\n\r\n\r\nLast_bloom\r\n\r\n\r\n1958\r\n\r\n\r\n19580502\r\n\r\n\r\n19580503\r\n\r\n\r\n19580507\r\n\r\n\r\n1959\r\n\r\n\r\n19590408\r\n\r\n\r\n19590413\r\n\r\n\r\n19590421\r\n\r\n\r\n1960\r\n\r\n\r\n19600410\r\n\r\n\r\n19600415\r\n\r\n\r\n19600421\r\n\r\n\r\n1961\r\n\r\n\r\n19610330\r\n\r\n\r\n19610408\r\n\r\n\r\n19610415\r\n\r\n\r\n1962\r\n\r\n\r\n19620427\r\n\r\n\r\n19620505\r\n\r\n\r\n19620510\r\n\r\n\r\n1963\r\n\r\n\r\n19630428\r\n\r\n\r\n19630504\r\n\r\n\r\n19630513\r\n\r\n\r\nTo prepare the data for visualization with ggplot, the pivot_longer function is used to reshape the data.frame into the appropriate format. Additionally, Year, Month, and Day columns are calculated. The make_JDay function from chillR is then applied to add the Julian date, which represents the day of the year (e.g., 1 for January 1st, 2 for January 2nd, and 32 for February 1st).\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nAlexander_Lucas <- \r\n  CKA_Alexander_Lucas %>%\r\n  pivot_longer(cols = \"First_bloom\":\"Last_bloom\",\r\n               names_to = \"variable\",\r\n               values_to=\"YEARMODA\") %>%\r\n  mutate(Year = as.numeric(substr(YEARMODA, 1, 4)),\r\n         Month = as.numeric(substr(YEARMODA, 5, 6)),\r\n         Day = as.numeric(substr(YEARMODA, 7, 8))) %>%\r\n  make_JDay() \r\n\r\n\r\n\r\n\r\nhead(Alexander_Lucas)\r\n\r\n\r\n\r\n\r\nPheno_year\r\n\r\n\r\nvariable\r\n\r\n\r\nYEARMODA\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nJDay\r\n\r\n\r\n1958\r\n\r\n\r\nFirst_bloom\r\n\r\n\r\n19580502\r\n\r\n\r\n1958\r\n\r\n\r\n5\r\n\r\n\r\n2\r\n\r\n\r\n122\r\n\r\n\r\n1958\r\n\r\n\r\nFull_bloom\r\n\r\n\r\n19580503\r\n\r\n\r\n1958\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n123\r\n\r\n\r\n1958\r\n\r\n\r\nLast_bloom\r\n\r\n\r\n19580507\r\n\r\n\r\n1958\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n127\r\n\r\n\r\n1959\r\n\r\n\r\nFirst_bloom\r\n\r\n\r\n19590408\r\n\r\n\r\n1959\r\n\r\n\r\n4\r\n\r\n\r\n8\r\n\r\n\r\n98\r\n\r\n\r\n1959\r\n\r\n\r\nFull_bloom\r\n\r\n\r\n19590413\r\n\r\n\r\n1959\r\n\r\n\r\n4\r\n\r\n\r\n13\r\n\r\n\r\n103\r\n\r\n\r\nNow, the historic bloom dates can be visualized using ggplot:\r\n\r\n\r\nggplot(data = Alexander_Lucas,\r\n       aes(Pheno_year,\r\n           JDay,\r\n           col = variable)) +\r\n  geom_line() +\r\n  theme_bw(base_size = 15) +\r\n  scale_color_discrete(\r\n    name = \"Phenological event\",\r\n    labels = c(\"First bloom\",\r\n               \"Full bloom\",\r\n               \"Last bloom\")) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\")\r\n\r\n\r\n\r\nTo better visualize trends in bloom dates, a regression line with a standard error can be added using ggplot.\r\n\r\n\r\nggplot(data = Alexander_Lucas,\r\n       aes(Pheno_year,\r\n           JDay,\r\n           col = variable)) +\r\n  geom_line() +\r\n  theme_bw(base_size = 15) +\r\n  scale_color_discrete(name = \"Phenological event\",\r\n                       labels = c(\"First bloom\",\r\n                                  \"Full bloom\", \r\n                                  \"Last bloom\")) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\") +\r\n  geom_smooth(method = \"lm\")\r\n\r\n\r\n\r\nThe geom_smooth method can be used to display the data.\r\n\r\n\r\nggplot(data=Alexander_Lucas,aes(Pheno_year,JDay,col=variable)) +\r\n  geom_smooth() +\r\n  theme_bw(base_size=15) +\r\n  scale_color_discrete(\r\n    name = \"Phenological event\",\r\n    labels = c(\"First bloom\", \"Full bloom\", \"Last bloom\")) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\")\r\n\r\n\r\n\r\nThe Kendall test is an effective method for detecting trends in time series data. It helps determine whether the observed changes in bloom dates are statistically significant or just due to random variation.\r\n\r\n\r\nrequire(Kendall)\r\nKendall_first <-\r\n  Kendall(x = Alexander_Lucas$Pheno_year[\r\n            which(Alexander_Lucas$variable == \"First_bloom\")],\r\n          y = Alexander_Lucas$JDay[\r\n            which(Alexander_Lucas$variable == \"First_bloom\")])\r\n\r\nKendall_full <- \r\n  Kendall(x = Alexander_Lucas$Pheno_year[\r\n            which(Alexander_Lucas$variable == \"Full_bloom\")],\r\n          y = Alexander_Lucas$JDay[\r\n            which(Alexander_Lucas$variable == \"Full_bloom\")])\r\n\r\nKendall_last <- \r\n  Kendall(x = Alexander_Lucas$Pheno_year[\r\n            which(Alexander_Lucas$variable == \"Last_bloom\")],\r\n          y = Alexander_Lucas$JDay[\r\n            which(Alexander_Lucas$variable == \"Last_bloom\")])\r\n\r\nKendall_first\r\n\r\ntau = -0.186, 2-sided pvalue =0.03533\r\n\r\n\r\n\r\nKendall_full\r\n\r\ntau = -0.27, 2-sided pvalue =0.0021401\r\n\r\n\r\n\r\nKendall_last\r\n\r\ntau = -0.233, 2-sided pvalue =0.0083237\r\n\r\nThe Kendall test identifies a significant trend in bloom dates, with p-values below 0.05. The negative tau value indicates an advancing bloom trend over time. However, since the test does not quantify the trend’s strength, a linear model is commonly used to estimate the rate of change in bloom dates across the years.\r\n\r\n\r\nlinear_trend_first <- lm(\r\n  Alexander_Lucas$JDay[\r\n    which(Alexander_Lucas$variable == \"First_bloom\")]~\r\n    Alexander_Lucas$Pheno_year[\r\n      which(Alexander_Lucas$variable == \"First_bloom\")])\r\n\r\nlinear_trend_full <- lm(\r\n  Alexander_Lucas$JDay[\r\n    which(Alexander_Lucas$variable == \"Full_bloom\")]~\r\n    Alexander_Lucas$Pheno_year[\r\n      which(Alexander_Lucas$variable == \"First_bloom\")])\r\n\r\nlinear_trend_last <- lm(\r\n  Alexander_Lucas$JDay[\r\n    which(Alexander_Lucas$variable == \"Last_bloom\")]~\r\n    Alexander_Lucas$Pheno_year[\r\n      which(Alexander_Lucas$variable == \"First_bloom\")])\r\n\r\nlinear_trend_first\r\n\r\n\r\nCall:\r\nlm(formula = Alexander_Lucas$JDay[which(Alexander_Lucas$variable == \r\n    \"First_bloom\")] ~ Alexander_Lucas$Pheno_year[which(Alexander_Lucas$variable == \r\n    \"First_bloom\")])\r\n\r\nCoefficients:\r\n                                                                 (Intercept)  \r\n                                                                    429.1662  \r\nAlexander_Lucas$Pheno_year[which(Alexander_Lucas$variable == \"First_bloom\")]  \r\n                                                                     -0.1618  \r\n\r\n\r\n\r\nlinear_trend_full\r\n\r\n\r\nCall:\r\nlm(formula = Alexander_Lucas$JDay[which(Alexander_Lucas$variable == \r\n    \"Full_bloom\")] ~ Alexander_Lucas$Pheno_year[which(Alexander_Lucas$variable == \r\n    \"First_bloom\")])\r\n\r\nCoefficients:\r\n                                                                 (Intercept)  \r\n                                                                    569.6720  \r\nAlexander_Lucas$Pheno_year[which(Alexander_Lucas$variable == \"First_bloom\")]  \r\n                                                                     -0.2305  \r\n\r\n\r\n\r\nlinear_trend_last\r\n\r\n\r\nCall:\r\nlm(formula = Alexander_Lucas$JDay[which(Alexander_Lucas$variable == \r\n    \"Last_bloom\")] ~ Alexander_Lucas$Pheno_year[which(Alexander_Lucas$variable == \r\n    \"First_bloom\")])\r\n\r\nCoefficients:\r\n                                                                 (Intercept)  \r\n                                                                    485.9785  \r\nAlexander_Lucas$Pheno_year[which(Alexander_Lucas$variable == \"First_bloom\")]  \r\n                                                                     -0.1841  \r\n\r\nThe second model coefficient provides the rate of change in bloom dates: -0.16 for first bloom, -0.23 for full bloom, and -0.18 for last bloom. To express these shifts per decade, the values are multiplied by 10. This allows for a clearer interpretation of long-term trends in bloom timing.\r\n\r\n\r\nphenology_trends <-\r\n  data.frame(Stage = c(\"First bloom\",\r\n                       \"Full bloom\", \r\n                       \"Last bloom\"),\r\n             Kendall_tau = c(round(Kendall_first[[1]][1],3),\r\n                             round(Kendall_full[[1]][1],3),\r\n                             round(Kendall_last[[1]][1],3)),\r\n             Kendall_p = c(round(Kendall_first[[2]][1],3),\r\n                           round(Kendall_full[[2]][1],3),\r\n                           round(Kendall_last[[2]][1],3)),\r\n             Linear_trend_per_decade =\r\n               c(round(linear_trend_first[[1]][2],2) * 10,\r\n                 round(linear_trend_full[[1]][2],2) * 10,\r\n                 round(linear_trend_last[[1]][2],2) * 10)\r\n             )\r\n\r\n\r\n\r\n\r\nphenology_trends\r\n\r\n\r\n\r\n\r\nStage\r\n\r\n\r\nKendall_tau\r\n\r\n\r\nKendall_p\r\n\r\n\r\nLinear_trend_per_decade\r\n\r\n\r\nFirst bloom\r\n\r\n\r\n-0.186\r\n\r\n\r\n0.035\r\n\r\n\r\n-1.6\r\n\r\n\r\nFull bloom\r\n\r\n\r\n-0.270\r\n\r\n\r\n0.002\r\n\r\n\r\n-2.3\r\n\r\n\r\nLast bloom\r\n\r\n\r\n-0.233\r\n\r\n\r\n0.008\r\n\r\n\r\n-1.8\r\n\r\n\r\nFrost risk\r\nTo assess frost risk, a simple frost model will be used, counting any hour with temperatures below 0°C as a frost hour. While bud sensitivity to frost changes over time—dormant buds can withstand deep freezes, but sensitivity increases as development progresses—this model will not account for these complexities. Given previous experience with temperature models, implementing this should be straightforward. The help file for step_model can provide guidance if needed.\r\n\r\n\r\nfrost_df = data.frame(\r\n  lower = c(-1000, 0),\r\n  upper = c(0, 1000),\r\n  weight = c(1, 0))\r\n\r\nfrost_model <- function(x) step_model(x,\r\n                                      frost_df)\r\n\r\n\r\nThe next step is to apply the frost model to historical data. Before doing so, the temperature records need to be converted to hourly values:\r\n\r\n\r\nhourly <- stack_hourly_temps(CKA_weather,\r\n                             latitude = 50.625)\r\n\r\nfrost <- tempResponse(hourly,\r\n                      models = c(frost = frost_model))\r\n\r\nggplot(frost,\r\n       aes(End_year,\r\n           frost)) +\r\n  geom_smooth() +\r\n  geom_point() +\r\n  ylim(c(0, NA)) +\r\n  ylab(\"Frost hours per year\") +\r\n  xlab(\"Year\")\r\n\r\n\r\n\r\nThe data indicates a significant decline in the number of frost hours at Klein-Altendorf. This trend can be quantified using statistical measures such as the Kendall test for trend detection and a linear regression model to estimate the rate of change over time.\r\n\r\n\r\nKendall(x = frost$End_year,\r\n        y = frost$frost)\r\n\r\ntau = -0.179, 2-sided pvalue =0.041862\r\n\r\n\r\n\r\nlm(frost$frost ~ frost$End_year)\r\n\r\n\r\nCall:\r\nlm(formula = frost$frost ~ frost$End_year)\r\n\r\nCoefficients:\r\n   (Intercept)  frost$End_year  \r\n      11172.57           -5.19  \r\n\r\nOn average, assuming a linear decline, there has been a reduction of approximately -5.2 frost hours per year. While historic scenarios of frost hour numbers could be created using previous methods, the focus here is on assessing the impact of spring frost on pear trees. Instead of aggregating frost hours for the entire year, daily frost occurrence data is needed to compare with bloom dates. This requires a modified version of the frost model.\r\n\r\n\r\nfrost_model_no_summ <- \r\n  function(x) step_model(x, \r\n                         frost_df,\r\n                         summ=FALSE)\r\n\r\nhourly$hourtemps[, \"frost\"] <- frost_model_no_summ(hourly$hourtemps$Temp)\r\n\r\nDaily_frost_hours <- aggregate(hourly$hourtemps$frost,\r\n                               by = list(hourly$hourtemps$YEARMODA),\r\n                               FUN = sum)\r\n\r\nDaily_frost <- make_JDay(CKA_weather)\r\n\r\nDaily_frost[, \"Frost_hours\"] <- Daily_frost_hours$x\r\n\r\n\r\nTo determine whether an individual hour experienced frost, a frost model is needed that does not automatically sum up all frost hours. This requires modifying the step_model function by setting the summ parameter to FALSE, allowing the model to be applied directly to hourly temperatures.\r\nInstead of plotting data by hour, it is more practical to summarize it by day. This can be done using the aggregate function, which sums values for specific data.frame columns that meet certain criteria. The make_JDay function is then used to add Julian dates to the dataset, and daily frost hour data is stored in a new column.\r\nFor visualization, the bloom and frost data should be plotted on the same axes: Year and Julian Day (JDay). Since the number of frost hours needs to be represented as well, dots of varying sizes can be used. To avoid displaying days with zero frost hours, these values should first be set to NA before plotting with ggplot.\r\n\r\n\r\nDaily_frost$Frost_hours[which(Daily_frost$Frost_hours == 0)] <- NA\r\n\r\nggplot(data = Daily_frost,\r\n       aes(Year,\r\n           JDay,\r\n           size = Frost_hours)) +\r\n  geom_point(col = \"light blue\",\r\n             alpha = 0.8) + \r\n  scale_size(range = c(0, 3),\r\n             breaks = c(1, 5, 10, 15, 20),\r\n             labels = c(\"1\", \"5\", \"10\", \"15\", \"20\"),\r\n             name = \"Frost hours\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nBloom dates and frost hours are plotted together, with dot sizes indicating frost hours:\r\n\r\n\r\nggplot(data = Alexander_Lucas,\r\n       aes(Pheno_year,\r\n           JDay,\r\n           col = variable)) +\r\n  geom_line() +\r\n  theme_bw(base_size = 15) +\r\n  scale_color_discrete(\r\n    name = \"Phenological event\",\r\n    labels = c(\"First bloom\",\r\n               \"Full bloom\",\r\n               \"Last bloom\")) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\") +\r\n  geom_point(data = Daily_frost,\r\n             aes(Year,\r\n                 JDay,\r\n                 size = Frost_hours),\r\n             col = \"light blue\",\r\n             alpha = 0.8) + \r\n  scale_size(range = c(0, 3),\r\n             breaks = c(1, 5, 10, 15, 20),\r\n             labels = c(\"1\", \"5\", \"10\", \"15\", \"20\"),\r\n             name = \"Frost hours\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nTo improve clarity, only spring data will be displayed in the plot:\r\n\r\n\r\nggplot(data = Alexander_Lucas,\r\n       aes(Pheno_year,\r\n           JDay,\r\n           col = variable)) +\r\n  geom_line() +\r\n  theme_bw(base_size = 15) +\r\n  scale_color_discrete(\r\n    name = \"Phenological event\",\r\n    labels = c(\"First bloom\",\r\n               \"Full bloom\",\r\n               \"Last bloom\")) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\") +\r\n  geom_point(data = Daily_frost,\r\n             aes(Year,\r\n                 JDay,\r\n                 size = Frost_hours),\r\n             col = \"light blue\",\r\n             alpha = 0.8) + \r\n  scale_size(range = c(0, 3),\r\n             breaks = c(1, 5, 10, 15, 20),\r\n             labels = c(\"1\", \"5\", \"10\", \"15\", \"20\"),\r\n             name = \"Frost hours\") +\r\n  theme_bw(base_size = 15) +\r\n  ylim(c(75, 140))\r\n\r\n\r\n\r\nTo enhance visibility, a ribbon will represent the total bloom duration, while a line will indicate full bloom. Some data restructuring is required for this adjustment:\r\n\r\n\r\nRibbon_Lucas <-\r\n  Alexander_Lucas %>%\r\n  select(Pheno_year, variable, JDay) %>%\r\n  pivot_wider(names_from = \"variable\", values_from = \"JDay\")\r\n\r\n\r\nggplot(data = Ribbon_Lucas,\r\n       aes(Pheno_year)) +\r\n  geom_ribbon(aes(ymin = First_bloom,\r\n                  ymax = Last_bloom),\r\n              fill = \"light gray\") +\r\n  geom_line(aes(y = Full_bloom)) +\r\n  theme_bw(base_size = 15) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\") +\r\n  geom_point(data = Daily_frost,\r\n             aes(Year,\r\n                 JDay,\r\n                 size = Frost_hours),\r\n             col = \"light blue\",\r\n             alpha = 0.8) + \r\n  scale_size(range = c(0, 3),\r\n             breaks = c(1, 5, 10, 15, 20),\r\n             labels = c(\"1\", \"5\", \"10\", \"15\", \"20\"),\r\n             name = \"Frost hours\") +\r\n  theme_bw(base_size = 15) +\r\n  ylim(c(75, 140))\r\n\r\n\r\n\r\nTo improve visibility, color will be used to highlight instances where frost events coincided with bloom.\r\n\r\n\r\n# identify frost events that overlap with bloom\r\nlookup_dates <- Ribbon_Lucas\r\n\r\nrow.names(lookup_dates) <- lookup_dates$Pheno_year\r\n\r\nDaily_frost[, \"First_bloom\"]<-\r\n  lookup_dates[as.character(Daily_frost$Year),\r\n               \"First_bloom\"]\r\n\r\nDaily_frost[, \"Last_bloom\"]<-\r\n  lookup_dates[as.character(Daily_frost$Year),\r\n               \"Last_bloom\"]\r\n\r\nDaily_frost[which(!is.na(Daily_frost$Frost_hours)),\r\n            \"Bloom_frost\"] <-\r\n  \"Before bloom\"\r\n\r\nDaily_frost[which(Daily_frost$JDay >= Daily_frost$First_bloom),\r\n            \"Bloom_frost\"]<-\r\n  \"During bloom\"\r\n\r\nDaily_frost[which(Daily_frost$JDay > Daily_frost$Last_bloom),\r\n            \"Bloom_frost\"]<-\r\n  \"After bloom\"\r\n\r\nDaily_frost[which(Daily_frost$JDay > 180),\r\n            \"Bloom_frost\"]<-\r\n  \"Before bloom\"\r\n\r\nggplot(data = Ribbon_Lucas,\r\n       aes(Pheno_year)) +\r\n  geom_ribbon(aes(ymin = First_bloom, \r\n                  ymax = Last_bloom),\r\n              fill = \"light gray\") +\r\n  geom_line(aes(y = Full_bloom)) +\r\n  theme_bw(base_size = 15) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\") +\r\n  geom_point(data = Daily_frost,\r\n             aes(Year,\r\n                 JDay,\r\n                 size = Frost_hours,\r\n                 col = Bloom_frost),\r\n             alpha = 0.8) + \r\n  scale_size(range = c(0, 5),\r\n             breaks = c(1, 5, 10, 15, 20),\r\n             labels = c(\"1\", \"5\", \"10\", \"15\", \"20\"),\r\n             name = \"Frost hours\") +\r\n  scale_color_manual(\r\n    breaks = c(\"Before bloom\",\r\n               \"During bloom\",\r\n               \"After bloom\"),\r\n    values = c(\"light green\",\r\n               \"red\",\r\n               \"light blue\"),\r\n    name = \"Frost timing\") +\r\n  theme_bw(base_size = 15) +\r\n  ylim(c(75, 140))\r\n\r\n\r\n\r\nThe analysis now focuses on the long-term trend in frost hours during the bloom period:\r\n\r\n\r\nBloom_frost_trend <- \r\n  aggregate(\r\n    Daily_frost$Frost_hours,\r\n    by = list(Daily_frost$Year,\r\n              Daily_frost$Bloom_frost),\r\n    FUN = function(x) sum(x,\r\n                          na.rm = TRUE))\r\n\r\ncolnames(Bloom_frost_trend) <- c(\"Year\",\r\n                                 \"Frost_timing\",\r\n                                 \"Frost_hours\")\r\n\r\nDuringBloom<-\r\n  Bloom_frost_trend[\r\n    which(Bloom_frost_trend$Frost_timing == \"During bloom\"),]\r\n\r\nggplot(data = DuringBloom,\r\n       aes(Year,\r\n           Frost_hours)) +\r\n  geom_col() +\r\n  ylab(\"Frost hours\")\r\n\r\n\r\n\r\nTo determine if there is a trend in frost hours during bloom, statistical analysis is needed:\r\n\r\n\r\nKendall(x = DuringBloom$Year,\r\n        y = DuringBloom$Frost_hours)\r\n\r\ntau = 0.0834, 2-sided pvalue =0.3797\r\n\r\n\r\n\r\nlm(DuringBloom$Frost_hours ~ DuringBloom$Year)\r\n\r\n\r\nCall:\r\nlm(formula = DuringBloom$Frost_hours ~ DuringBloom$Year)\r\n\r\nCoefficients:\r\n     (Intercept)  DuringBloom$Year  \r\n      -159.07879           0.08302  \r\n\r\nThe regression slope suggests an average annual increase of 0.08 frost hours during bloom. However, the Kendall test does not indicate a significant trend.\r\nExercises on frost risk analysis\r\nDownload the phenology dataset for the apple cultivar Roter Boskoop from Klein-Altendorf.\r\n\r\n\r\nCKA_Roter_Boskoop <- read_tab(\"data/Roter_Boskoop_bloom_1958_2019.csv\")\r\n\r\n\r\nRoter_Boskoop <- CKA_Roter_Boskoop %>%\r\n  pivot_longer(cols = \"First_bloom\":\"Last_bloom\",\r\n               names_to = \"variable\",\r\n               values_to=\"YEARMODA\") %>%\r\n  mutate(Year = as.numeric(substr(YEARMODA, 1, 4)),\r\n         Month = as.numeric(substr(YEARMODA, 5, 6)),\r\n         Day = as.numeric(substr(YEARMODA, 7, 8))) %>%\r\n  make_JDay() \r\n\r\n\r\nIllustrate the development of the bloom period over the duration of the weather record. Use multiple ways to show this - feel free to be creative.\r\n\r\n\r\n# Line chart with trend line\r\nggplot(data = Roter_Boskoop,aes(Pheno_year, JDay, col = variable)) +\r\n  geom_smooth() +\r\n  theme_bw(base_size=15) +\r\n  scale_color_discrete(\r\n    name = \"Phenological event\",\r\n    labels = c(\"First bloom\", \"Full bloom\", \"Last bloom\")) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\")\r\n\r\n\r\n\r\nTrend analysis of phenological events for ‘Roter Boskoop’ over time. The x-axis represents the phenological year, while the y-axis shows the Julian day of blooming events. The colored lines indicate smoothed trends for the three key phenological stages: first bloom, full bloom, and last bloom.\r\n\r\n\r\n# Boxplots of Bloom Period by Decade\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(forcats)\r\n\r\n# Define colors for boxplots\r\ncolors <- c(\"#b2182b\", \"#d6604d\", \"#f4a582\", \"#fdae61\", \"#92c5de\", \"#4393c3\", \"#2166ac\")\r\n\r\n\r\n\r\n# Calculate decades\r\nRoter_Boskoop <- Roter_Boskoop %>%\r\n  mutate(Decade = cut(Year, breaks = seq(1950, 2020, by = 10), labels = seq(1950, 2010, by = 10)))\r\n\r\n# Create boxplots\r\nggplot(Roter_Boskoop, aes(x = as.factor(Decade), y = JDay, fill = Decade)) +\r\n  stat_boxplot(geom = \"errorbar\", width = 0.4, lwd = 0.5) +  \r\n  geom_boxplot(outlier.color = \"black\", \r\n               outlier.shape = 16, \r\n               outlier.size = 1.5,  \r\n               lwd = 0.6,  \r\n               fatten = 1.2) +  \r\n  scale_fill_manual(values = colors) +  \r\n  labs(x = \"Decade\", \r\n       y = \"Julian date (day of the year)\",\r\n       fill = \"Decade\") +\r\n  theme_bw(base_size = 15, base_family = \"Arial\") + \r\n  theme(\r\n    legend.position = \"none\", \r\n    panel.border = element_rect(color = \"black\", fill = NA, size = 0.5)) \r\n\r\n\r\n\r\nBoxplot of the blooming period of ‘Roter Boskoop’ across decades. The x-axis represents different decades from 1950 to 2010, while the y-axis shows the Julian day of blooming events. Each boxplot visualizes the distribution of blooming dates within a decade, including the median, interquartile range, and potential outliers.\r\n\r\n\r\n# Heatmap for Bloom density \r\nggplot(Roter_Boskoop, aes(x = Year, y = JDay)) +\r\n  geom_bin2d(bins = 30) +  \r\n  scale_fill_gradientn(colors = c(\"yellow\", \"orange\", \"red\"), name = \"Density\") +  \r\n  labs(x = \"Year\",\r\n       y = \"Julian Day\") +\r\n  theme_bw(base_size = 15) +\r\n  theme(panel.border = element_rect(color = \"black\", fill = NA, size = 0.5))\r\n\r\n\r\n\r\nHeatmap of bloom density for ‘Roter Boskoop’ over the years. The x-axis represents the year, while the y-axis shows the Julian day on which blooming events were observed. The color scale from yellow to red indicates the density of observations, with red areas representing a higher frequency of blooming events.\r\nEvaluate the occurrence of frost events at Klein-Altendorf since 1958. Illustrate this in a plot.\r\n\r\n\r\n# Generating model for frost hours (temperatures < 0°C)\r\nfrost_df = data.frame(\r\n  lower = c(-1000, 0),\r\n  upper = c(0, 1000),\r\n  weight = c(1, 0))\r\n\r\nfrost_model <- function(x) step_model(x,\r\n                                      frost_df)\r\n\r\n# Convert temperature record to hourly values\r\nhourly <- stack_hourly_temps(CKA_weather,\r\n                             latitude = 50.625)\r\n\r\nfrost <- tempResponse(hourly,\r\n                      models = c(frost = frost_model))\r\n\r\n# Plot number of frost hours\r\nggplot(frost,\r\n       aes(End_year,\r\n           frost)) +\r\n  geom_smooth() +\r\n  geom_point() +\r\n  ylim(c(0, NA)) +\r\n  ylab(\"Frost hours per year\") +\r\n  xlab(\"Year\")\r\n\r\n\r\n\r\nProduce an illustration of the relationship between spring frost events and the bloom period of ‘Roter Boskoop’.\r\n\r\n\r\n# Ribbon for total bloom duration\r\nRibbon_Boskoop <-\r\n  Roter_Boskoop %>%\r\n  select(Pheno_year, variable, JDay) %>%\r\n  pivot_wider(names_from = \"variable\", values_from = \"JDay\")\r\n\r\n# Identify frost events that overlap with bloom \r\nlookup_dates <- Ribbon_Boskoop\r\n\r\nrow.names(lookup_dates) <- lookup_dates$Pheno_year\r\n\r\nDaily_frost[, \"First_bloom\"]<-\r\n  lookup_dates[as.character(Daily_frost$Year),\r\n               \"First_bloom\"]\r\n\r\nDaily_frost[, \"Last_bloom\"] <-\r\n  lookup_dates[as.character(Daily_frost$Year),\r\n               \"Last_bloom\"]\r\n\r\nDaily_frost[which(!is.na(Daily_frost$Frost_hours)),\r\n            \"Bloom_frost\"] <-\r\n  \"Before bloom\"\r\n\r\nDaily_frost[which(Daily_frost$JDay >= Daily_frost$First_bloom),\r\n            \"Bloom_frost\"] <-\r\n  \"During bloom\"\r\n\r\nDaily_frost[which(Daily_frost$JDay > Daily_frost$Last_bloom),\r\n            \"Bloom_frost\"] <-\r\n  \"After bloom\"\r\n\r\nDaily_frost[which(Daily_frost$JDay > 180),\r\n            \"Bloom_frost\"] <-\r\n  \"Before bloom\"\r\n\r\n\r\n\r\n\r\n# Plot spring frost events that coincided with bloom\r\nggplot(data = Ribbon_Boskoop,\r\n       aes(Pheno_year)) +\r\n  geom_ribbon(aes(ymin = First_bloom, \r\n                  ymax = Last_bloom),\r\n              fill = \"light gray\") +\r\n  geom_line(aes(y = Full_bloom)) +\r\n  theme_bw(base_size = 15) +\r\n  xlab(\"Phenological year\") +\r\n  ylab(\"Julian date (day of the year)\") +\r\n  geom_point(data = Daily_frost,\r\n             aes(Year,\r\n                 JDay,\r\n                 size = Frost_hours,\r\n                 col = Bloom_frost),\r\n             alpha = 0.8) + \r\n  scale_size(range = c(0, 5),\r\n             breaks = c(1, 5, 10, 15, 20),\r\n             labels = c(\"1\", \"5\", \"10\", \"15\", \"20\"),\r\n             name = \"Frost hours\") +\r\n  scale_color_manual(\r\n    breaks = c(\"Before bloom\",\r\n               \"During bloom\",\r\n               \"After bloom\"),\r\n    values = c(\"light green\",\r\n               \"red\",\r\n               \"light blue\"),\r\n    name = \"Frost timing\") +\r\n  theme_bw(base_size = 15) +\r\n  ylim(c(75, 140))\r\n\r\n\r\n\r\nEvaluate how the risk of spring frost for this cultivar has changed over time. Has there been a significant trend?\r\n\r\n\r\n# Investigate if there is a long-term trend in the number of frost hours during bloom\r\nBloom_frost_trend <- \r\n  aggregate(\r\n    Daily_frost$Frost_hours,\r\n    by = list(Daily_frost$Year,\r\n              Daily_frost$Bloom_frost),\r\n    FUN = function(x) sum(x,\r\n                          na.rm = TRUE))\r\n\r\ncolnames(Bloom_frost_trend) <- c(\"Year\",\r\n                                 \"Frost_timing\",\r\n                                 \"Frost_hours\")\r\n\r\nDuringBloom <-\r\n  Bloom_frost_trend[\r\n    which(Bloom_frost_trend$Frost_timing == \"During bloom\"),]\r\n\r\nggplot(data = DuringBloom,\r\n       aes(Year,\r\n           Frost_hours)) +\r\n  geom_col() +\r\n  ylab(\"Frost hours\")\r\n\r\n\r\n\r\n\r\n\r\n# Performing Kendall test\r\nKendall(x = DuringBloom$Year,\r\n        y = DuringBloom$Frost_hours)\r\n\r\ntau = 0.0176, 2-sided pvalue =0.86236\r\n\r\n\r\n\r\n# Performing linear model\r\nlm(DuringBloom$Frost_hours ~ DuringBloom$Year)\r\n\r\n\r\nCall:\r\nlm(formula = DuringBloom$Frost_hours ~ DuringBloom$Year)\r\n\r\nCoefficients:\r\n     (Intercept)  DuringBloom$Year  \r\n       -75.87112           0.03996  \r\n\r\nThe slope of the regression suggests an average increase of 0.04 frost hours during bloom per year. However, the Kendall test (τ = 0.0176, p = 0.862) indicates that this trend is not statistically significant.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:35:01+01:00"
    },
    {
      "path": "future.html",
      "title": "Future temperature scenarios",
      "author": [],
      "contents": "\r\nHuman-induced climate change is already causing significant impacts on the climate, ecosystems, and agriculture. The situation is expected to worsen, driven by high levels of greenhouse gases and ongoing emissions (about 40 Gt CO2-equivalents annually). Ecosystem degradation further weakens the planet’s resilience. Future outcomes are uncertain, but climate change adaptation research aims to reduce uncertainties by focusing on exposure, sensitivity, and adaptive capacity.\r\nFramework for Climate Vulnerability and Adaptation\r\nThe conceptual framework for evaluating climate vulnerability and adaptation involves:\r\nExposure: The anticipated future climate conditions.\r\nSensitivity: How ecosystems or systems respond to those conditions.\r\nAdaptive Capacity: The ability of systems to adapt to climate change.\r\nCombining exposure and sensitivity helps estimate potential impacts, while adaptation efforts work to reduce sensitivity (e.g., using resilient cultivars) or enhance adaptive capacity (e.g., flexible management strategies). Current research mainly focuses on exposure, particularly in areas like orchards. While some models account for tree sensitivity, these analyses are largely centered around exposure factors.\r\nFuture Climate Scenarios and Modeling\r\nDeveloping future climate scenarios requires precise climate model setups, including Global Climate Models (GCMs), Regional Climate Models (RCMs), and downscaling, with no single “correct” approach. The chillR package can use both high-quality and less accurate climate scenarios, facilitating access to reliable databases even for users without expert knowledge.\r\nBackground on Climate Models and Warming Pathways\r\nUp until November 2023, ClimateWizard was the best source for climate data in chillR, providing point-specific climate data from various models. ClimateWizard simplifies data retrieval for specific locations, bypassing large-scale datasets that could otherwise create bottlenecks. However, its data is based on older climate models (CMIP5) and outdated Representative Concentration Pathways (RCPs), which are becoming less reliable. The newer CMIP6 models and Shared Socioeconomic Pathways (SSPs), released in 2021, provide a more modern approach for accurate climate projections and are now the recommended scenarios for climate change modeling. While ClimateWizard still supports CMIP5 and RCP scenarios, transitioning to CMIP6 and SSP scenarios is crucial for the most up-to-date and accurate climate change projections.\r\nExercises on future temperature scenarios\r\nBriefly describe the differences between the RCPs and the SSPs (you may have to follow some of the links provided above).\r\nThe Representative Concentration Pathways (RCPs) and the Shared Socioeconomic Pathways (SSPs) are both scenario frameworks used for modeling future climate developments, but they differ in their methodology and focus.\r\nRCPs (Representative Concentration Pathways):\r\nDeveloped for the IPCC’s 5th Assessment Report (AR5, published in 2014).\r\nDescribe different possible levels of radiative forcing (W/m²) by 2100, meaning the direct effect of greenhouse gas emissions on the climate.\r\nFour main pathways: RCP2.6 (strong emissions reduction), RCP4.5 and RCP6.0 (moderate emissions), RCP8.5 (very high emissions, often referred to as “business as usual”).\r\nA purely climate-science-based approach without considering socioeconomic developments.\r\n\r\nSSPs (Shared Socioeconomic Pathways):\r\nDeveloped for the IPCC’s 6th Assessment Report (AR6, published in 2021).\r\nCombine socioeconomic developments with emission pathways to create different future scenarios.\r\nFive main scenarios (SSP1 to SSP5), representing various societal, economic, and political trajectories, such as sustainable development (SSP1) or continued reliance on fossil fuels (SSP5).\r\nCan be combined with different radiative forcing levels (e.g., SSP1-2.6 for sustainable development with low emissions or SSP5-8.5 for high emissions and fossil fuel dependence).\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:35:07+01:00"
    },
    {
      "path": "gaps.html",
      "title": "Filling gaps in temperature records",
      "author": [],
      "contents": "\r\nWeather data often contains gaps due to equipment malfunctions, power outages, or storage problems. These gaps create challenges for modeling agroclimatic conditions, requiring effective gap-filling methods.\r\nFilling Short Gaps in Daily Records\r\nFor short gaps (2-3 days), linear interpolation estimates missing values by averaging the last known and first known values around the gap. The chillR package provides the interpolate_gaps() function for this:\r\n\r\n\r\nweather <- KA_weather %>% make_all_day_table()\r\n\r\nTmin_int <- interpolate_gaps(weather[,\"Tmin\"])\r\nweather <- weather %>% mutate(Tmin = Tmin_int$interp, Tmin_interpolated = Tmin_int$missing)\r\n\r\nTmax_int <- interpolate_gaps(weather[,\"Tmax\"])\r\nweather <- weather %>% mutate(Tmax = Tmax_int$interp, Tmax_interpolated = Tmax_int$missing)\r\n\r\nKA_weather_gap <- rbind(KA_weather, c(Year = 2011,\r\n                                      Month = 3,\r\n                                      Day = 3,\r\n                                      Tmax = 26,\r\n                                      Tmin = 14)) \r\n\r\n\r\nThe fix_weather() function can also be used to fill gaps:\r\n\r\n\r\nfixed_winter_days <- KA_weather_gap %>% fix_weather(start_year = 2000, \r\n                                                    end_year = 2011, \r\n                                                    start_date = 300, \r\n                                                    end_date = 100)\r\nfixed_all_days <- KA_weather_gap %>% fix_weather()\r\n\r\n\r\nThe function returns a weather dataframe with interpolated data and a QC object summarizing interpolation quality:\r\n\r\n\r\nfixed_winter_days$QC\r\n\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nMissing_Tmin\r\n\r\n\r\nMissing_Tmax\r\n\r\n\r\nIncomplete_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\n1999/2000\r\n\r\n\r\n2000\r\n\r\n\r\n166\r\n\r\n\r\n100\r\n\r\n\r\n66\r\n\r\n\r\n66\r\n\r\n\r\n66\r\n\r\n\r\n60.2\r\n\r\n\r\n2000/2001\r\n\r\n\r\n2001\r\n\r\n\r\n167\r\n\r\n\r\n167\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2001/2002\r\n\r\n\r\n2002\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2002/2003\r\n\r\n\r\n2003\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2003/2004\r\n\r\n\r\n2004\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2004/2005\r\n\r\n\r\n2005\r\n\r\n\r\n167\r\n\r\n\r\n167\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2005/2006\r\n\r\n\r\n2006\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2006/2007\r\n\r\n\r\n2007\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2008/2009\r\n\r\n\r\n2009\r\n\r\n\r\n167\r\n\r\n\r\n167\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2009/2010\r\n\r\n\r\n2010\r\n\r\n\r\n166\r\n\r\n\r\n166\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2010/2011\r\n\r\n\r\n2011\r\n\r\n\r\n166\r\n\r\n\r\n128\r\n\r\n\r\n165\r\n\r\n\r\n165\r\n\r\n\r\n165\r\n\r\n\r\n0.6\r\n\r\n\r\n\r\n\r\n\r\nfixed_all_days$QC\r\n\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nMissing_Tmin\r\n\r\n\r\nMissing_Tmax\r\n\r\n\r\nIncomplete_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\n1997/1998\r\n\r\n\r\n1998\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n1998/1999\r\n\r\n\r\n1999\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n1999/2000\r\n\r\n\r\n2000\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2000/2001\r\n\r\n\r\n2001\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2001/2002\r\n\r\n\r\n2002\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2002/2003\r\n\r\n\r\n2003\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2003/2004\r\n\r\n\r\n2004\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2004/2005\r\n\r\n\r\n2005\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2005/2006\r\n\r\n\r\n2006\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2006/2007\r\n\r\n\r\n2007\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2008/2009\r\n\r\n\r\n2009\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100.0\r\n\r\n\r\n2009/2010\r\n\r\n\r\n2010\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n214\r\n\r\n\r\n214\r\n\r\n\r\n214\r\n\r\n\r\n41.4\r\n\r\n\r\n2010/2011\r\n\r\n\r\n2011\r\n\r\n\r\n365\r\n\r\n\r\n62\r\n\r\n\r\n364\r\n\r\n\r\n364\r\n\r\n\r\n364\r\n\r\n\r\n0.3\r\n\r\n\r\n\r\nA plot illustrates the effect of gap length on interpolation accuracy:\r\n\r\n\r\ngap_weather <- KA_weather[200:305, ]\r\ngap_weather[ ,\"Tmin_observed\"] <- gap_weather$Tmin\r\ngap_weather$Tmin[c(2, 4:5, 7:9, 11:14, 16:20, 22:27, 29:35, \r\n                   37:44, 46:54, 56:65, 67:77, 79:90, 92:104)] <- NA\r\nfixed_gaps <- fix_weather(gap_weather)$weather\r\n\r\nggplot(data = fixed_gaps, aes(DATE, Tmin_observed)) +\r\n  geom_line(lwd = 1.3) +\r\n  xlab(\"Date\") +\r\n  ylab(\"Daily minimum temperature (°C)\") +\r\n  geom_line(data = fixed_gaps, aes(DATE, Tmin), col = \"red\", lwd = 1.3)\r\n\r\n\r\n\r\nInterpolation errors increase with gap size:\r\n\r\n\r\nfixed_gaps[,\"error\"] <- abs(fixed_gaps$Tmin - fixed_gaps$Tmin_observed)\r\n\r\nggplot(data = fixed_gaps, aes(DATE, error)) +\r\n  geom_line(lwd = 1.3) +\r\n  xlab(\"Date\") +\r\n  ylab(\"Error introduced by interpolation (°C)\") +\r\n  geom_point(data = fixed_gaps[which(!fixed_gaps$no_Tmin),], aes(DATE, error), col = \"red\", cex = 3)\r\n\r\n\r\n\r\nFilling Long Gaps in Daily Records\r\nFor long gaps, data from nearby weather stations is used. The patch_weather() function in chillR helps with this:\r\n\r\n\r\n\r\n\r\n\r\nstation_list <- handle_gsod(action = \"list_stations\",\r\n                            location = c(7.10, 50.73),\r\n                            time_interval = c(1990, 2020))\r\n\r\n\r\nRelevant stations are downloaded:\r\n\r\n\r\n\r\n\r\n\r\npatch_weather <- \r\n  handle_gsod(action = \"download_weather\", \r\n              location = as.character(station_list$chillR_code[c(2, 3, 6)]), \r\n              time_interval = c(1990, 2020)) %>% \r\n  handle_gsod()\r\n\r\n\r\nGaps are filled using patch_daily_temperatures():\r\n\r\n\r\npatched <- patch_daily_temperatures(weather = Bonn, patch_weather = patch_weather)\r\n\r\n\r\nPatch statistics are examined:\r\n\r\n\r\npatched$statistics[[1]]\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean_bias\r\n\r\n\r\nstdev_bias\r\n\r\n\r\nfilled\r\n\r\n\r\ngaps_remain\r\n\r\n\r\nTmin\r\n\r\n\r\n-0.307\r\n\r\n\r\n1.304\r\n\r\n\r\n2146\r\n\r\n\r\n1\r\n\r\n\r\nTmax\r\n\r\n\r\n0.202\r\n\r\n\r\n1.154\r\n\r\n\r\n2146\r\n\r\n\r\n1\r\n\r\n\r\n\r\n\r\npatched$statistics[[2]]\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean_bias\r\n\r\n\r\nstdev_bias\r\n\r\n\r\nfilled\r\n\r\n\r\ngaps_remain\r\n\r\n\r\nTmin\r\n\r\n\r\n-1.871\r\n\r\n\r\n2.080\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nTmax\r\n\r\n\r\n1.466\r\n\r\n\r\n1.427\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n\r\n\r\npatched$statistics[[3]]\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean_bias\r\n\r\n\r\nstdev_bias\r\n\r\n\r\nfilled\r\n\r\n\r\ngaps_remain\r\n\r\n\r\nTmin\r\n\r\n\r\n-0.546\r\n\r\n\r\n1.186\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nTmax\r\n\r\n\r\n1.314\r\n\r\n\r\n1.089\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nTo improve accuracy, mean bias and standard deviation bias limits are set:\r\n\r\n\r\npatched <- patch_daily_temperatures(weather = Bonn, \r\n                                    patch_weather = patch_weather, \r\n                                    max_mean_bias = 1, \r\n                                    max_stdev_bias = 2)\r\n\r\n\r\nFinal gaps are identified:\r\n\r\n\r\npost_patch_stats <- fix_weather(patched)$QC\r\n\r\n\r\nRemaining short gaps are filled with interpolation:\r\n\r\n\r\nBonn_weather <- fix_weather(patched)\r\n\r\n\r\nFor seasonally adjusted bias correction, patch_daily_temps() is used:\r\n\r\n\r\npatched_monthly <- patch_daily_temps(weather = Bonn, \r\n                                     patch_weather = patch_weather, \r\n                                     max_mean_bias = 1, \r\n                                     max_stdev_bias = 2, \r\n                                     time_interval = \"month\")\r\n\r\n\r\nThis function allows for interval-based bias corrections:\r\n\r\n\r\npatched_2weeks <- patch_daily_temps(weather = Bonn, \r\n                                    patch_weather = patch_weather, \r\n                                    max_mean_bias = 1, \r\n                                    max_stdev_bias = 2, \r\n                                    time_interval = \"2 weeks\")\r\n\r\n\r\nUsing finer time intervals improves bias correction accuracy, but requires sufficient data for reliability.\r\nExercises on filling gaps\r\nUse chillR functions to find out how many gaps you have in your dataset (even if you have none, please still follow all further steps)\r\n\r\n\r\nYakima <- read.csv(\"Yakima/Yakima_chillR_weather.csv\")\r\nYakima_QC <- fix_weather(Yakima)$QC\r\n\r\n\r\n\r\n\r\n\r\nSeason\r\n\r\n\r\nEnd_year\r\n\r\n\r\nSeason_days\r\n\r\n\r\nData_days\r\n\r\n\r\nMissing_Tmin\r\n\r\n\r\nMissing_Tmax\r\n\r\n\r\nIncomplete_days\r\n\r\n\r\nPerc_complete\r\n\r\n\r\n1989/1990\r\n\r\n\r\n1990\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1990/1991\r\n\r\n\r\n1991\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1991/1992\r\n\r\n\r\n1992\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1992/1993\r\n\r\n\r\n1993\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1993/1994\r\n\r\n\r\n1994\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1994/1995\r\n\r\n\r\n1995\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1995/1996\r\n\r\n\r\n1996\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1996/1997\r\n\r\n\r\n1997\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1997/1998\r\n\r\n\r\n1998\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1998/1999\r\n\r\n\r\n1999\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n1999/2000\r\n\r\n\r\n2000\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2000/2001\r\n\r\n\r\n2001\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2001/2002\r\n\r\n\r\n2002\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2002/2003\r\n\r\n\r\n2003\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2003/2004\r\n\r\n\r\n2004\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2004/2005\r\n\r\n\r\n2005\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2005/2006\r\n\r\n\r\n2006\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2006/2007\r\n\r\n\r\n2007\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2007/2008\r\n\r\n\r\n2008\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2008/2009\r\n\r\n\r\n2009\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2009/2010\r\n\r\n\r\n2010\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2010/2011\r\n\r\n\r\n2011\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2011/2012\r\n\r\n\r\n2012\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2012/2013\r\n\r\n\r\n2013\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2013/2014\r\n\r\n\r\n2014\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2014/2015\r\n\r\n\r\n2015\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2015/2016\r\n\r\n\r\n2016\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2016/2017\r\n\r\n\r\n2017\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2017/2018\r\n\r\n\r\n2018\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2018/2019\r\n\r\n\r\n2019\r\n\r\n\r\n365\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n2019/2020\r\n\r\n\r\n2020\r\n\r\n\r\n366\r\n\r\n\r\n366\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n100\r\n\r\n\r\n\r\nCreate a list of the 25 closest weather stations using the handle_gsod function\r\n\r\n\r\nstation_list_Yakima <- handle_gsod(action = \"list_stations\",\r\n                                   location = c(long = -120.50, lat = 46.60),\r\n                                   time_interval = c(1990, 2020))\r\n\r\n\r\n\r\n\r\n\r\nchillR_code\r\n\r\n\r\nSTATION.NAME\r\n\r\n\r\nCTRY\r\n\r\n\r\nLat\r\n\r\n\r\nLong\r\n\r\n\r\nBEGIN\r\n\r\n\r\nEND\r\n\r\n\r\nDistance\r\n\r\n\r\nOverlap_years\r\n\r\n\r\nPerc_interval_covered\r\n\r\n\r\n72781024243\r\n\r\n\r\nYAKIMA AIR TERMINAL/MCALSR FIELD AP\r\n\r\n\r\nUS\r\n\r\n\r\n46.564\r\n\r\n\r\n-120.535\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n4.82\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924243\r\n\r\n\r\nYAKIMA AIR TERMINAL\r\n\r\n\r\nUS\r\n\r\n\r\n46.568\r\n\r\n\r\n-120.543\r\n\r\n\r\n19480101\r\n\r\n\r\n19721231\r\n\r\n\r\n4.85\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72781399999\r\n\r\n\r\nVAGABOND AAF / YAKIMA TRAINING CENTER WASHINGTON USA\r\n\r\n\r\nUS\r\n\r\n\r\n46.667\r\n\r\n\r\n-120.454\r\n\r\n\r\n20030617\r\n\r\n\r\n20081110\r\n\r\n\r\n8.25\r\n\r\n\r\n5.40\r\n\r\n\r\n17\r\n\r\n\r\n72056299999\r\n\r\n\r\nRANGE OP 13 / YAKIMA TRAINING CENTER\r\n\r\n\r\nUS\r\n\r\n\r\n46.800\r\n\r\n\r\n-120.167\r\n\r\n\r\n20080530\r\n\r\n\r\n20170920\r\n\r\n\r\n33.79\r\n\r\n\r\n9.31\r\n\r\n\r\n30\r\n\r\n\r\n72788399999\r\n\r\n\r\nBOWERS FLD\r\n\r\n\r\nUS\r\n\r\n\r\n47.033\r\n\r\n\r\n-120.531\r\n\r\n\r\n20000101\r\n\r\n\r\n20031231\r\n\r\n\r\n48.26\r\n\r\n\r\n4.00\r\n\r\n\r\n13\r\n\r\n\r\n72788324220\r\n\r\n\r\nBOWERS FIELD AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.034\r\n\r\n\r\n-120.531\r\n\r\n\r\n19880106\r\n\r\n\r\n20250304\r\n\r\n\r\n48.37\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924220\r\n\r\n\r\nELLENSBURG BOWERS FI\r\n\r\n\r\nUS\r\n\r\n\r\n47.034\r\n\r\n\r\n-120.530\r\n\r\n\r\n19480601\r\n\r\n\r\n19550101\r\n\r\n\r\n48.37\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72784094187\r\n\r\n\r\nHANFORD AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n46.567\r\n\r\n\r\n-119.600\r\n\r\n\r\n20060101\r\n\r\n\r\n20130326\r\n\r\n\r\n68.96\r\n\r\n\r\n7.23\r\n\r\n\r\n23\r\n\r\n\r\n72784099999\r\n\r\n\r\nHANFORD\r\n\r\n\r\nUS\r\n\r\n\r\n46.567\r\n\r\n\r\n-119.600\r\n\r\n\r\n19730101\r\n\r\n\r\n19971231\r\n\r\n\r\n68.96\r\n\r\n\r\n8.00\r\n\r\n\r\n26\r\n\r\n\r\n72782594239\r\n\r\n\r\nPANGBORN MEMORIAL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.397\r\n\r\n\r\n-120.201\r\n\r\n\r\n20000101\r\n\r\n\r\n20250304\r\n\r\n\r\n91.58\r\n\r\n\r\n21.00\r\n\r\n\r\n68\r\n\r\n\r\n72782599999\r\n\r\n\r\nPANGBORN MEM\r\n\r\n\r\nUS\r\n\r\n\r\n47.399\r\n\r\n\r\n-120.207\r\n\r\n\r\n19730101\r\n\r\n\r\n19971231\r\n\r\n\r\n91.69\r\n\r\n\r\n8.00\r\n\r\n\r\n26\r\n\r\n\r\n72788499999\r\n\r\n\r\nRICHLAND AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n46.306\r\n\r\n\r\n-119.304\r\n\r\n\r\n19810203\r\n\r\n\r\n20250303\r\n\r\n\r\n97.39\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n72781524237\r\n\r\n\r\nSTAMPASS PASS FLTWO\r\n\r\n\r\nUS\r\n\r\n\r\n47.277\r\n\r\n\r\n-121.337\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n98.63\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924237\r\n\r\n\r\nSTAMPEDE PASS\r\n\r\n\r\nUS\r\n\r\n\r\n47.277\r\n\r\n\r\n-121.337\r\n\r\n\r\n19480101\r\n\r\n\r\n19721231\r\n\r\n\r\n98.63\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72790024141\r\n\r\n\r\nEPHRATA MUNICIPAL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.308\r\n\r\n\r\n-119.516\r\n\r\n\r\n20050101\r\n\r\n\r\n20250304\r\n\r\n\r\n108.64\r\n\r\n\r\n16.00\r\n\r\n\r\n52\r\n\r\n\r\n72782624141\r\n\r\n\r\nEPHRATA MUNICIPAL\r\n\r\n\r\nUS\r\n\r\n\r\n47.308\r\n\r\n\r\n-119.515\r\n\r\n\r\n19420101\r\n\r\n\r\n19971231\r\n\r\n\r\n108.69\r\n\r\n\r\n8.00\r\n\r\n\r\n26\r\n\r\n\r\n99999924141\r\n\r\n\r\nEPHRATA AP FCWOS\r\n\r\n\r\nUS\r\n\r\n\r\n47.308\r\n\r\n\r\n-119.515\r\n\r\n\r\n19480101\r\n\r\n\r\n19550101\r\n\r\n\r\n108.69\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72782724110\r\n\r\n\r\nGRANT COUNTY INTL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.193\r\n\r\n\r\n-119.315\r\n\r\n\r\n19430610\r\n\r\n\r\n20250304\r\n\r\n\r\n111.73\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n72782799999\r\n\r\n\r\nMOSES LAKE/GRANT CO\r\n\r\n\r\nUS\r\n\r\n\r\n47.200\r\n\r\n\r\n-119.317\r\n\r\n\r\n20000101\r\n\r\n\r\n20031231\r\n\r\n\r\n112.06\r\n\r\n\r\n4.00\r\n\r\n\r\n13\r\n\r\n\r\n72784524163\r\n\r\n\r\nTRI-CITIES AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n46.270\r\n\r\n\r\n-119.118\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n112.21\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n72784599999\r\n\r\n\r\nTRI CITIES\r\n\r\n\r\nUS\r\n\r\n\r\n46.267\r\n\r\n\r\n-119.117\r\n\r\n\r\n20000101\r\n\r\n\r\n20031231\r\n\r\n\r\n112.40\r\n\r\n\r\n4.00\r\n\r\n\r\n13\r\n\r\n\r\n99999924163\r\n\r\n\r\nPASCO NAS\r\n\r\n\r\nUS\r\n\r\n\r\n46.267\r\n\r\n\r\n-119.117\r\n\r\n\r\n19450401\r\n\r\n\r\n19460601\r\n\r\n\r\n112.40\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72698824219\r\n\r\n\r\nMUNICIPAL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n45.619\r\n\r\n\r\n-121.166\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n120.70\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924219\r\n\r\n\r\nTHE DALLES MUNICIPAL ARPT\r\n\r\n\r\nUS\r\n\r\n\r\n45.619\r\n\r\n\r\n-121.166\r\n\r\n\r\n19480101\r\n\r\n\r\n19650101\r\n\r\n\r\n120.70\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72688399999\r\n\r\n\r\nHERMISTON MUNI\r\n\r\n\r\nUS\r\n\r\n\r\n45.828\r\n\r\n\r\n-119.259\r\n\r\n\r\n19980514\r\n\r\n\r\n20051231\r\n\r\n\r\n128.55\r\n\r\n\r\n7.64\r\n\r\n\r\n25\r\n\r\n\r\n\r\nIdentify suitable weather stations for patching gaps\r\nDownload weather data for promising stations, convert them to chillR format and compile them in a list\r\n\r\n\r\n\r\n\r\n\r\npatch_weather <-\r\n  handle_gsod(action = \"download_weather\",\r\n              location = as.character(station_list_Yakima$chillR_code[c(4, 6, 8)]),\r\n              time_interval = c(1990, 2020)) %>%\r\n  handle_gsod()\r\n\r\n\r\nUse the patch_daily_temperatures function to fill gaps\r\n\r\n\r\npatched <- patch_daily_temperatures(weather = Yakima,\r\n                                    patch_weather = patch_weather)\r\n\r\n\r\n\r\n\r\n# Patch statistics for YRANGE OP 13 /AKIMA TRAINING CENTER\r\npatched$statistics[[1]]\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean_bias\r\n\r\n\r\nstdev_bias\r\n\r\n\r\nfilled\r\n\r\n\r\ngaps_remain\r\n\r\n\r\nTmin\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nTmax\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n\r\n\r\n# Patch statistics for HANFORD AIRPORT\r\npatched$statistics[[2]]\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean_bias\r\n\r\n\r\nstdev_bias\r\n\r\n\r\nfilled\r\n\r\n\r\ngaps_remain\r\n\r\n\r\nTmin\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nTmax\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n\r\n\r\n# Patch statistics for BOWERS FIELD AIRPORT\r\npatched$statistics[[3]]\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean_bias\r\n\r\n\r\nstdev_bias\r\n\r\n\r\nfilled\r\n\r\n\r\ngaps_remain\r\n\r\n\r\nTmin\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nTmax\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nInvestigate the results - have all gaps been filled?\r\n\r\n\r\nwrite.csv(patched$weather,\r\n          \"Yakima/Yakima_weather.csv\", row.names = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:35:54+01:00"
    },
    {
      "path": "generate_temp.html",
      "title": "Generating temperature scenarios",
      "author": [],
      "contents": "\r\nEffective orchard management requires understanding local climate conditions, especially chill availability, to make informed decisions on tree species and cultivars. While historical weather data has been useful, orchard managers now need more tailored, site-specific forecasts, especially regarding chill and heat levels, to optimize planting decisions.\r\nChill Scenarios\r\nThe key objective is to provide growers with reliable forecasts for chill availability, helping them select the right trees. Climate conditions like chill and heat accumulation must be predicted over the long term, beyond simple historical data. This enables better decision-making when considering tree species that will meet their specific climatic needs and cope with potential frost risks.\r\nRisk Assessment in Orchard Planning\r\nBecause trees have long lifespans, they experience different weather patterns throughout their productive years. For optimal yields, trees must consistently meet their climatic requirements each year. Orchard managers need to understand the full range of possible weather scenarios, ensuring that chosen trees will meet their chill requirements and avoid frost damage. This requires assessing the distribution of local climate data, as opposed to relying on a single year’s data.\r\nWeather Generators\r\nThe best way to assess the local climate is through long-term weather data. Weather generators model these patterns and simulate realistic weather conditions to help in orchard planning. chillR uses the R-compatible weather generator RMAWGEN for simulating temperature data, which is crucial for modeling chill availability.\r\nWeather Generation in chillR\r\nTo generate temperature data, chillR uses the RMAWGEN weather generator. The function temperature_generation is used to calibrate long-term temperature data, and it produces simulated temperature records that can be used to evaluate climate patterns over extended periods.\r\n\r\n\r\nTemp <- KA_weather %>%\r\n  temperature_generation(years = c(1998, 2009),\r\n                         sim_years = c(2001, 2100))\r\n\r\nTemperatures <- KA_weather %>% filter(Year %in% 1998:2009) %>%\r\n  cbind(Data_source = \"observed\") %>%\r\n  rbind(\r\n    Temp[[1]] %>% select(c(Year,\r\n                           Month,\r\n                           Day,\r\n                           Tmin,\r\n                           Tmax)) %>%\r\n      cbind(Data_source = \"simulated\")\r\n  ) %>%\r\n  mutate(Date = as.Date(ISOdate(2000,\r\n                                Month,\r\n                                Day)))\r\n\r\n\r\nTemperature Visualization\r\nTo compare observed and simulated temperature data, ggplot2 is used to visualize smoothed temperature trends, allowing for clearer analysis of temperature patterns.\r\n\r\n\r\nggplot(data = Temperatures,\r\n       aes(Date, Tmin)) +\r\n  geom_smooth(aes(colour = factor(Year))) +\r\n  facet_wrap(vars(Data_source)) +\r\n  theme_bw(base_size = 20) +\r\n  theme(legend.position = \"none\") +\r\n  scale_x_date(date_labels = \"%b\")\r\n\r\n\r\n\r\n\r\n\r\nggplot(data=Temperatures,\r\n       aes(Date, Tmax)) +\r\n  geom_smooth(aes(colour = factor(Year))) +\r\n  facet_wrap(vars(Data_source)) +\r\n  theme_bw(base_size = 20) +\r\n  theme(legend.position = \"none\") +\r\n  scale_x_date(date_labels = \"%b\")\r\n\r\n\r\n\r\nChill Comparison\r\nNext, chill accumulation is analyzed by comparing observed and simulated chill data using the stack_hourly_temps and chilling functions. This helps in assessing the chill accumulation over multiple years.\r\n\r\n\r\nchill_observed <- Temperatures %>%\r\n  filter(Data_source == \"observed\") %>%\r\n  stack_hourly_temps(latitude = 50.4) %>%\r\n  chilling(Start_JDay = 305,\r\n           End_JDay = 59)\r\n\r\nchill_simulated <- Temperatures %>%\r\n  filter(Data_source == \"simulated\") %>%\r\n  stack_hourly_temps(latitude = 50.4) %>%\r\n  chilling(Start_JDay = 305,\r\n           End_JDay = 59)\r\n\r\nchill_comparison <-\r\n  cbind(chill_observed,\r\n        Data_source = \"observed\") %>%\r\n  rbind(cbind(chill_simulated,\r\n              Data_source = \"simulated\"))\r\n\r\n\r\nChill Distribution Visualization\r\nThe distribution of chill accumulation is visualized using a histogram to compare observed and simulated chill portions.\r\n\r\n\r\nggplot(chill_comparison_full_seasons,\r\n       aes(x = Chill_portions)) + \r\n  geom_histogram(binwidth = 1,\r\n                 aes(fill = factor(Data_source))) +\r\n  theme_bw(base_size = 20) +\r\n  labs(fill = \"Data source\") +\r\n  xlab(\"Chill accumulation (Chill Portions)\") +\r\n  ylab(\"Frequency\")\r\n\r\n\r\n\r\nCumulative Distribution Function\r\nA cumulative distribution function (CDF) is plotted to visualize the likelihood of meeting the required chill accumulation thresholds.\r\n\r\n\r\nchill_simulations <-\r\n  chill_comparison_full_seasons %>%\r\n  filter(Data_source == \"simulated\")\r\n\r\nggplot(chill_simulations,\r\n       aes(x = Chill_portions)) +\r\n  stat_ecdf(geom = \"step\",\r\n            lwd = 1.5,\r\n            col = \"blue\") +\r\n  ylab(\"Cumulative probability\") +\r\n  xlab(\"Chill accumulation (in Chill Portions)\") +\r\n  theme_bw(base_size = 20)\r\n\r\n\r\n\r\nQuantiles for Safe Winter Chill\r\nSpecific quantiles of chill accumulation are calculated to assess “Safe Winter Chill” levels and the risk associated with not meeting chilling requirements.\r\n\r\n\r\n# 10% quantile (Safe Winter Chill)\r\nquantile(chill_simulations$Chill_portions, 0.1)\r\n\r\n     10% \r\n77.28649 \r\n\r\n# 50% confidence interval (25th to 75th percentile)\r\nquantile(chill_simulations$Chill_portions, c(0.25, 0.75))\r\n\r\n     25%      75% \r\n79.76791 84.23847 \r\n\r\nExercises on temperature generation\r\nFor the location you chose for your earlier analyses, use chillR’s weather generator to produce 100 years of synthetic temperature data.\r\n\r\n\r\n\r\n\r\n\r\n# Generate temperature data with temperature_generation function\r\nTemp <- Yakima %>% \r\n  temperature_generation(years = c(1998, 2009),\r\n                         sim_years = c(2001, 2100))\r\n\r\nTemperatures <- Yakima %>% \r\n  select(Year, Month, Day, Tmin, Tmax) %>%  \r\n  filter(Year %in% 1998:2009) %>%\r\n  cbind(Data_source = \"observed\") %>%\r\n  rbind(\r\n    Temp[[1]] %>% select(c(Year, Month, Day, Tmin, Tmax)) %>% \r\n      cbind(Data_source = \"simulated\")\r\n  ) %>%\r\n  mutate(Date = as.Date(ISOdate(2000, Month, Day)))\r\n\r\n\r\n\r\n\r\n# Plot observed vs. simulated minimum temperature data\r\nggplot(data = Temperatures,\r\n       aes(Date,\r\n           Tmin)) +\r\n  geom_smooth(aes(colour = factor(Year))) +\r\n  facet_wrap(vars(Data_source)) +\r\n  theme_bw(base_size = 20) +\r\n  theme(legend.position = \"none\") +\r\n  scale_x_date(date_labels = \"%b\")\r\n\r\n\r\n\r\n\r\n\r\n# Plot observed vs. simulated maximum temperature data\r\nggplot(data = Temperatures,\r\n       aes(Date,\r\n           Tmax)) +\r\n  geom_smooth(aes(colour = factor(Year))) +\r\n  facet_wrap(vars(Data_source)) +\r\n  theme_bw(base_size = 20) +\r\n  theme(legend.position = \"none\") +\r\n  scale_x_date(date_labels = \"%b\")\r\n\r\n\r\n\r\nCalculate winter chill (in Chill Portions) for your synthetic weather, and illustrate your results as histograms and cumulative distributions.\r\n\r\n\r\n# Analyzing chill accumulation by comparing observed and simulated chill data using the stack_hourly_temps function\r\nchill_observed <- Temperatures %>%\r\n  filter(Data_source == \"observed\") %>%\r\n  stack_hourly_temps(latitude = 46.6) %>%\r\n  chilling(Start_JDay = 305,\r\n           End_JDay = 59)\r\n  \r\nchill_simulated <- Temperatures %>%\r\n  filter(Data_source == \"simulated\") %>%\r\n  stack_hourly_temps(latitude = 46.6) %>%\r\n  chilling(Start_JDay = 305,\r\n           End_JDay = 59)\r\n  \r\nchill_comparison <-\r\n  cbind(chill_observed,\r\n        Data_source = \"observed\") %>%\r\n  rbind(cbind(chill_simulated,\r\n              Data_source = \"simulated\"))\r\n\r\nchill_comparison_full_seasons <- \r\n  chill_comparison %>%\r\n  filter(Perc_complete == 100)\r\n\r\n\r\n\r\n\r\n# Plot chill distribution as histogram\r\nggplot(chill_comparison_full_seasons,\r\n       aes(x = Chill_portions)) + \r\n  geom_histogram(binwidth = 1,\r\n                 aes(fill = factor(Data_source))) +\r\n  theme_bw(base_size = 20) +\r\n  labs(fill = \"Data source\") +\r\n  xlab(\"Chill accumulation (Chill Portions)\") +\r\n  ylab(\"Frequency\")\r\n\r\n\r\n\r\n\r\n\r\nchill_simulations <-\r\n  chill_comparison_full_seasons %>%\r\n  filter(Data_source == \"simulated\")\r\n  \r\n# Plot chill distribution as cumulative distribution\r\nggplot(chill_simulations,\r\n       aes(x = Chill_portions)) +\r\n  stat_ecdf(geom = \"step\",\r\n            lwd = 1.5,\r\n            col = \"blue\") +\r\n  ylab(\"Cumulative probability\") +\r\n  xlab(\"Chill accumulation (in Chill Portions)\") +\r\n  theme_bw(base_size = 20)\r\n\r\n\r\n\r\nProduce similar plots for the number of freezing hours (<0°C) in April (or October, if your site is in the Southern Hemisphere) for your location of interest\r\n\r\n\r\ndf <- data.frame(\r\n  lower =  c(-1000,    0),\r\n  upper =  c(    0, 1000),\r\n  weight = c(    1,    0))\r\n\r\nfreezing_hours <- function(x) step_model(x, df)\r\n\r\nchill_observed <- Temperatures %>%\r\n  filter(Data_source == \"observed\") %>%\r\n  stack_hourly_temps(latitude = 46.6) %>%\r\n  tempResponse(Start_JDay = 91,\r\n               End_JDay = 120,\r\n               models = list(Frost = freezing_hours,\r\n                             Chill_portions = Dynamic_Model,\r\n                             GDH = GDH))\r\n\r\nchill_simulated <- Temperatures %>%\r\n  filter(Data_source == \"simulated\") %>%\r\n  stack_hourly_temps(latitude = 46.6) %>%\r\n  tempResponse(Start_JDay = 91,\r\n               End_JDay = 120,\r\n               models=list(Frost = freezing_hours,\r\n                           Chill_portions = Dynamic_Model,\r\n                           GDH = GDH))\r\n\r\nchill_comparison <-\r\n  cbind(chill_observed,\r\n        Data_source = \"observed\") %>%\r\n  rbind(cbind(chill_simulated,\r\n              Data_source = \"simulated\"))\r\n\r\nchill_comparison_full_seasons <-\r\n  chill_comparison %>%\r\n  filter(Perc_complete == 100)\r\n\r\n\r\n\r\n\r\n# Plot chill distribution in April as histogram \r\nggplot(chill_comparison_full_seasons,\r\n       aes(x = Frost)) + \r\n  geom_histogram(binwidth = 25,\r\n                 aes(fill = factor(Data_source))) +\r\n  theme_bw(base_size = 10) +\r\n  labs(fill = \"Data source\") +\r\n  xlab(\"Frost incidence during April (hours)\") +\r\n  ylab(\"Frequency\")\r\n\r\n\r\n\r\n\r\n\r\nchill_simulations <-\r\n  chill_comparison_full_seasons %>%\r\n  filter(Data_source == \"simulated\")\r\n\r\n# Plot chill distribution in April as cumulative distribution \r\nggplot(chill_simulations,\r\n       aes(x = Frost)) +\r\n  stat_ecdf(geom = \"step\",\r\n            lwd = 1.5,\r\n            col = \"blue\") +\r\n  ylab(\"Cumulative probability\") +\r\n  xlab(\"Frost incidence during April (hours)\") +\r\n  theme_bw(base_size = 20)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:37:30+01:00"
    },
    {
      "path": "getting_started.html",
      "title": "Learning goals and tools used in the Module",
      "author": [],
      "contents": "\r\nLearning goals\r\nThe content begins with an introduction to phenology (with a special emphasis on dormancy) as well as an overview of climate change. It then focuses heavily on the practical application of the chillR package for R. This tool has been continuously developed since 2013 by Prof. Dr. Eike Lüdeling, head of the HortiBonn research group at the Institute of Crop Science and Resource Conservation (INRES) at the University of Bonn, to support this type of analysis.\r\nThis course will offer the following skills and experiences:\r\nKnowledge about phenology\r\nKnowledge about tree dormancy\r\nUnderstanding of climate change impact projection methods\r\nAppreciation for the importance of risks and uncertainty in climate change projection\r\nUnderstanding of how to use some staple tools of R code development\r\nAbility to use chillR functions for climate change impact projection\r\nAbility to use chillR functions for tree phenology analysis\r\nUnderstanding and ability to use the PhenoFlex dormancy analysis framework\r\nTools\r\nThis course is designed to provide knowledge about tree phenology, climate change, and related topics, along with hands-on exercises to demonstrate the functionalities of the chillR package. It is recommended to document everything learned in a learning logbook. To engage in these practical components effectively, various tools are required. Since chillR is an R package, using R, preferably through the RStudio interface, will be necessary.\r\nAlthough it is possible to run RStudio on a local computer and save files directly on the hard drive, this approach differs from the methods commonly used by professional programmers. To align with standard programming practices, familiarity with certain code development tools is essential. This course will therefore cover the basics of using Git and GitHub, which are valuable tools for organizing, securing, and sharing code. Additionally, proper documentation techniques in R will be introduced, focusing on creating well-structured, professional reports using RMarkdown. While these tools may seem complex at first, their usefulness is likely to become clearer as they are used throughout the module.\r\nDr. Cory Whitney, a researcher at HortiBonn, has volunteered to create tutorial videos to provide an introduction to these tools.\r\nR and RStudio\r\nThe first video Using R and RStudio demonstrates how to install and run R and RStudio:\r\n\r\n\r\n\r\n\r\nGit and Github\r\nThe next video Using Git and Github explores the programming version control environment Git and the interface GitHub, which is used to access these features:\r\n\r\n\r\n\r\n\r\nRmarkdown\r\nIn the last video Using R Mardown, R Markdown will be examined, a powerful tool that enables the creation of sophisticated reports, websites, and more from R code:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:37:38+01:00"
    },
    {
      "path": "historic.html",
      "title": "Historic temperature scenearios",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nGenerating Climate Scenarios with chillR\r\nA weather generator in the chillR package can create agroclimatic profiles for specific locations. By calibrating it with historical temperature data, the generated profile represents the climate of the calibration period. This generator also simulates future climate scenarios using the temperature_scenario parameter in the temperature_generation function.\r\nDefining a Temperature Scenario\r\nThe temperature_scenario parameter requires a data.frame with two columns (Tmin and Tmax), each containing 12 values representing monthly temperature adjustments. Without this parameter, no temperature changes are applied.\r\nA simple scenario increasing temperatures by 2°C in all months is created as follows:\r\n\r\n\r\nchange_scenario <- data.frame(Tmin = rep(2, 12), Tmax = rep(2, 12))\r\n\r\nTemp_2 <- temperature_generation(KA_weather,\r\n                                 years = c(1998, 2005),\r\n                                 sim_years = c(2001, 2100),\r\n                                 temperature_scenario = change_scenario)\r\n\r\n\r\nComparing Observed and Simulated Temperatures\r\nA dataset is created to compare observed and simulated temperatures:\r\n\r\n\r\nTemperature_scenarios <- KA_weather %>%\r\n  filter(Year %in% 1998:2005) %>%\r\n  cbind(Data_source = \"observed\") %>%\r\n  rbind(Temp[[1]] %>% \r\n          select(c(Year, Month, Day, Tmin, Tmax)) %>% \r\n          cbind(Data_source = \"simulated\")\r\n        ) %>%\r\n  rbind(Temp_2[[1]] %>%\r\n          select(c(Year, Month, Day, Tmin, Tmax)) %>% \r\n          cbind(Data_source = \"Warming_2C\")\r\n        ) %>%\r\n  mutate(Date = as.Date(ISOdate(2000,\r\n                                Month,\r\n                                Day)))\r\n\r\n\r\nThese scenarios can be visualized using ggplot2:\r\n\r\n\r\nggplot(data = Temperature_scenarios, \r\n       aes(Date, Tmin)) +\r\n  geom_smooth(aes(colour = factor(Year))) +\r\n  facet_wrap(vars(Data_source)) +\r\n  theme_bw(base_size = 20) +\r\n  theme(legend.position = \"none\") +\r\n  scale_x_date(date_labels = \"%b\")\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = Temperature_scenarios,\r\n       aes(Date,Tmax)) +\r\n  geom_smooth(aes(colour = factor(Year))) +\r\n  facet_wrap(vars(Data_source)) +\r\n  theme_bw(base_size = 20) +\r\n  theme(legend.position = \"none\") +\r\n  scale_x_date(date_labels = \"%b\")\r\n\r\n\r\n\r\nThis simplified approach applies uniform changes across all months, which does not reflect historical patterns but aligns with early climate modeling methods.\r\nCreating Historical Temperature Scenarios\r\nA long-term dataset is necessary to generate historical climate scenarios. Weather data for Cologne/Bonn Airport is downloaded and formatted for chillR:\r\n\r\n\r\nstation_list <- handle_gsod(action = \"list_stations\", location = c(7.1, 50.8))\r\nBonn_weather <- handle_gsod(action = \"download_weather\",\r\n                            location = station_list$chillR_code[1],\r\n                            time_interval = c(1973, 2019)) %>%\r\n  handle_gsod()\r\n\r\n\r\nMissing data is identified and interpolated:\r\n\r\n\r\nBonn_patched <- patch_daily_temperatures(weather = Bonn_weather$`KOLN BONN`,\r\n                                         patch_weather = list(KA_weather))\r\n\r\nBonn <- fix_weather(Bonn_patched)\r\n\r\nBonn_temps <- Bonn$weather\r\n\r\n\r\nGenerating Scenarios for Specific Years\r\nHistorical temperature scenarios for years like 1980, 1990, 2000, and 2010 are created:\r\n\r\n\r\nscenario_1980 <- temperature_scenario_from_records(weather = Bonn_temps, \r\n                                                   year = 1980)\r\n\r\n\r\nThe scenario is refined by setting a reference year (1996) and adjusting accordingly:\r\n\r\n\r\nscenario_1996 <- temperature_scenario_from_records(weather = Bonn_temps, \r\n                                                   year = 1996)\r\n\r\nrelative_scenario <- temperature_scenario_baseline_adjustment(\r\n  baseline = scenario_1996, \r\n  temperature_scenario = scenario_1980)\r\n\r\n\r\nThis adjusted scenario is used to generate temperature projections:\r\n\r\n\r\ntemps_1980 <- temperature_generation(weather = Bonn_temps,\r\n                                     years = c(1973, 2019),\r\n                                     sim_years = c(2001, 2100),\r\n                                     temperature_scenario = relative_scenario)\r\n\r\n\r\nThe process is repeated for multiple years:\r\n\r\n\r\nall_past_scenarios <- temperature_scenario_from_records(\r\n  weather = Bonn_temps, \r\n  year = c(1980, \r\n           1990, \r\n           2000, \r\n           2010))\r\n\r\nadjusted_scenarios <- temperature_scenario_baseline_adjustment(\r\n  baseline = scenario_1996, \r\n  temperature_scenario = all_past_scenarios)\r\n\r\nall_past_scenario_temps <- temperature_generation(\r\n  weather = Bonn_temps,\r\n  years = c(1973, 2019),\r\n  sim_years = c(2001, 2100),\r\n  temperature_scenario = adjusted_scenarios)\r\n\r\n\r\nThe generated data is saved for future use:\r\n\r\n\r\nsave_temperature_scenarios(all_past_scenario_temps, \"data\", \"Bonn_hist_scenarios\")\r\n\r\n\r\nEstimating Chill Accumulation\r\nUsing the tempResponse_daily_list function, chill accumulation can be estimated:\r\n\r\n\r\nchill_hist_scenario_list <- tempResponse_daily_list(all_past_scenario_temps,\r\n                                                    latitude = 50.9,\r\n                                                    Start_JDay = 305,\r\n                                                    End_JDay = 59,\r\n                                                    models = models)\r\n\r\n\r\nObserved chill data is computed and saved:\r\n\r\n\r\nscenarios <- names(chill_hist_scenario_list)[1:4]\r\n\r\nall_scenarios <- chill_hist_scenario_list[[scenarios[1]]] %>%\r\n  mutate(scenario = as.numeric(scenarios[1]))\r\n\r\nfor (sc in scenarios[2:4])\r\n all_scenarios <- all_scenarios %>%\r\n  rbind(chill_hist_scenario_list[[sc]] %>%\r\n          cbind(\r\n            scenario=as.numeric(sc))\r\n        ) %>%\r\n  filter(Perc_complete == 100)\r\n\r\nactual_chill <- tempResponse_daily_list(Bonn_temps,\r\n                                        latitude=50.9,\r\n                                        Start_JDay = 305,\r\n                                        End_JDay = 59,\r\n                                        models)[[1]] %>%\r\n  filter(Perc_complete == 100)\r\n\r\nwrite.csv(actual_chill, \"data/Bonn_observed_chill_305_59.csv\", row.names = FALSE)\r\n\r\n\r\nVisualizing Chill Accumulation Scenarios\r\n\r\n\r\nggplot(data = all_scenarios, aes(scenario, Chill_Portions, fill = factor(scenario))) +\r\n  geom_violin() +\r\n  ylab(\"Chill accumulation (Chill Portions)\") +\r\n  xlab(\"Scenario year\") +\r\n  theme_bw(base_size = 15) +\r\n  ylim(c(0,90)) +\r\n  geom_point(data = actual_chill, aes(End_year, Chill_Portions, fill = \"blue\"), col = \"blue\", show.legend = FALSE) +\r\n  scale_fill_discrete(name = \"Scenario\", breaks = unique(all_scenarios$scenario))\r\n\r\n\r\n\r\nComparing Running Mean and Linear Regression Approaches\r\nThe running mean and linear regression methods are compared to estimate long-term trends:\r\n\r\n\r\ntemperature_means <- \r\n  data.frame(Year = min(Bonn_temps$Year):max(Bonn_temps$Year),\r\n             Tmin = aggregate(Bonn_temps$Tmin,\r\n                              FUN = \"mean\",\r\n                              by = list(Bonn_temps$Year))[,2],\r\n             Tmax=aggregate(Bonn_temps$Tmax,\r\n                            FUN = \"mean\",\r\n                            by = list(Bonn_temps$Year))[,2]) %>%\r\n  mutate(runn_mean_Tmin = runn_mean(Tmin,15),\r\n         runn_mean_Tmax = runn_mean(Tmax,15))\r\n\r\nTmin_regression <- lm(Tmin~Year, temperature_means)\r\nTmax_regression <- lm(Tmax~Year, temperature_means)\r\n\r\ntemperature_means <- temperature_means %>%\r\n  mutate(regression_Tmin = Tmin_regression$coefficients[1]+\r\n           Tmin_regression$coefficients[2]*temperature_means$Year,\r\n           regression_Tmax = Tmax_regression$coefficients[1]+\r\n           Tmax_regression$coefficients[2]*temperature_means$Year\r\n         )\r\n\r\n# Plot mean monthly minimum temperature \r\nggplot(temperature_means,\r\n       aes(Year,\r\n           Tmin)) + \r\n  geom_point() + \r\n  geom_line(data = temperature_means,\r\n            aes(Year,\r\n                runn_mean_Tmin),\r\n            lwd = 2,\r\n            col = \"blue\") + \r\n  geom_line(data = temperature_means,\r\n            aes(Year,\r\n                regression_Tmin),\r\n            lwd = 2,\r\n            col = \"red\") +\r\n  theme_bw(base_size = 15) +\r\n  ylab(\"Mean monthly minimum temperature (°C)\")\r\n\r\n\r\n\r\n\r\n\r\n# Plot mean monthly maximum temperature\r\nggplot(temperature_means,\r\n       aes(Year,\r\n           Tmax)) + \r\n  geom_point() + \r\n  geom_line(data = temperature_means,\r\n            aes(Year,\r\n                runn_mean_Tmax),\r\n            lwd = 2,\r\n            col = \"blue\") + \r\n  geom_line(data = temperature_means,\r\n            aes(Year, \r\n                regression_Tmax),\r\n            lwd = 2,\r\n            col = \"red\") +\r\n  theme_bw(base_size = 15) +\r\n  ylab(\"Mean monthly maximum temperature (°C)\")\r\n\r\n\r\n\r\nThese methods yield different results, highlighting variations in trend estimation as climate change progresses. This comparison underscores the importance of selecting appropriate methods for temperature trend analysis.\r\nExercises on generating historic temperature scenarios\r\nFor the location you chose for previous exercises, produce historic temperature scenarios representing several years of the historic record (your choice).\r\n\r\n\r\n\r\n\r\n\r\n# Get a list of close-by weather stations\r\nstation_list <- handle_gsod(action = \"list_stations\",\r\n                            location = c(long = -120.5, lat = 46.6),\r\n                            time_interval = c(1973, 2023))\r\n\r\n\r\n# Download data\r\nYakima_weather <- handle_gsod(action = \"download_weather\",\r\n                            location = station_list$chillR_code[1],\r\n                            time_interval = c(1973, 2023)) %>%\r\n  handle_gsod()\r\n\r\n\r\n# Check record for missing data\r\nfix_weather(Yakima_weather$`YAKIMA AIR TERMINAL/MCALSR FIELD AP`)$QC\r\n\r\n\r\n# Filling gaps in temperature records\r\npatch_weather <-\r\n  handle_gsod(action = \"download_weather\",\r\n              location = as.character(station_list$chillR_code[c(4, 6)]),\r\n              time_interval = c(1973, 2023)) %>%\r\n  handle_gsod()\r\n\r\n\r\nYakima_patched <- patch_daily_temperatures(\r\n  weather = Yakima_weather$`YAKIMA AIR TERMINAL/MCALSR FIELD AP`,\r\n  patch_weather = patch_weather)\r\n\r\nfix_weather(Yakima_patched)$QC\r\n\r\nYakima_temps <- Yakima_patched$weather\r\n\r\n\r\n\r\n\r\n# Generating running mean and linear regression \r\ntemperature_means <- \r\n  data.frame(Year = min(Yakima_temps$Year):max(Yakima_temps$Year),\r\n             Tmin = aggregate(Yakima_temps$Tmin,\r\n                              FUN = \"mean\",\r\n                              by = list(Yakima_temps$Year))[,2],\r\n             Tmax=aggregate(Yakima_temps$Tmax,\r\n                            FUN = \"mean\",\r\n                            by = list(Yakima_temps$Year))[,2]) %>%\r\n  mutate(runn_mean_Tmin = runn_mean(Tmin,15),\r\n         runn_mean_Tmax = runn_mean(Tmax,15))\r\n\r\n\r\nTmin_regression <- lm(Tmin~Year,\r\n                      temperature_means)\r\n\r\nTmax_regression <- lm(Tmax~Year,\r\n                      temperature_means)\r\n\r\ntemperature_means <- temperature_means %>%\r\n  mutate(regression_Tmin = Tmin_regression$coefficients[1]+\r\n           Tmin_regression$coefficients[2]*temperature_means$Year,\r\n         regression_Tmax = Tmax_regression$coefficients[1]+\r\n           Tmax_regression$coefficients[2]*temperature_means$Year\r\n  )\r\n\r\n\r\n\r\n\r\n# Plot mean monthly minimum temperature of 1973 to 2023\r\nggplot(temperature_means,\r\n       aes(Year,\r\n           Tmin)) + \r\n  geom_point() + \r\n  geom_line(data = temperature_means,\r\n            aes(Year,\r\n                runn_mean_Tmin),\r\n            lwd = 2,\r\n            col = \"blue\") + \r\n  geom_line(data = temperature_means,\r\n            aes(Year,\r\n                regression_Tmin),\r\n            lwd = 2,\r\n            col = \"red\") +\r\n  theme_bw(base_size = 15) +\r\n  ylab(\"Mean monthly minimum temperature (°C)\")\r\n\r\n\r\n\r\n\r\n\r\n# Plot mean monthly maximum temperature of 1973 to 2023\r\nggplot(temperature_means,\r\n       aes(Year,\r\n           Tmax)) + \r\n  geom_point() + \r\n  geom_line(data = temperature_means,\r\n            aes(Year,\r\n                runn_mean_Tmax),\r\n            lwd = 2,\r\n            col = \"blue\") + \r\n  geom_line(data = temperature_means,\r\n            aes(Year, \r\n                regression_Tmax),\r\n            lwd = 2,\r\n            col = \"red\") +\r\n  theme_bw(base_size = 15) +\r\n  ylab(\"Mean monthly maximum temperature (°C)\")\r\n\r\n\r\n\r\nProduce chill distributions for these scenarios and plot them.\r\n\r\n\r\n# Generating scenarios for specific years \r\nscenario_1980 <- temperature_scenario_from_records(weather = Yakima_temps,\r\n                                                   year = 1980)\r\n\r\ntemps_1980 <- temperature_generation(weather = Yakima_temps,\r\n                                     years = c(1973, 2023),\r\n                                     sim_years = c(2001, 2100),\r\n                                     temperature_scenario = scenario_1980)\r\n\r\n# Setting a reference year (1998)\r\nscenario_1998 <- temperature_scenario_from_records(weather = Yakima_temps,\r\n                                                   year = 1998)\r\n\r\nrelative_scenario <- temperature_scenario_baseline_adjustment(\r\n  baseline = scenario_1998,\r\n  temperature_scenario = scenario_1980)\r\n\r\n# Adjusted scenario is used to generate temperature projections\r\ntemps_1980 <- temperature_generation(weather = Yakima_temps,\r\n                                   years = c(1973, 2023),\r\n                                   sim_years = c(2001,2100),\r\n                                   temperature_scenario = relative_scenario)\r\n\r\n# Process is repeated for multiple years \r\nall_past_scenarios <- temperature_scenario_from_records(\r\n  weather = Yakima_temps,\r\n  year = c(1980,\r\n           1990,\r\n           2000,\r\n           2010, \r\n           2020))\r\n\r\nadjusted_scenarios <- temperature_scenario_baseline_adjustment(\r\n  baseline = scenario_1998,\r\n  temperature_scenario = all_past_scenarios)\r\n\r\nall_past_scenario_temps <- temperature_generation(\r\n  weather = Yakima_temps,\r\n  years = c(1973, 2023),\r\n  sim_years = c(2001, 2100),\r\n  temperature_scenario = adjusted_scenarios)\r\n\r\n# Generated data is saved for future use \r\nsave_temperature_scenarios(all_past_scenario_temps, \"Yakima\", \"Yakima_hist_scenarios\")\r\n\r\n\r\n\r\n\r\n# Selecting models for evaluation \r\nfrost_model <- function(x)\r\n  step_model(x,\r\n             data.frame(\r\n               lower = c(-1000, 0),\r\n               upper = c(0, 1000),\r\n               weight = c(1, 0)))\r\n\r\nmodels <- list(Chill_Portions = Dynamic_Model, \r\n               GDH = GDH,\r\n               Frost_H = frost_model)\r\n\r\n# Using tempResponse_daily_list function to estimate chill accumulation\r\nchill_hist_scenario_list <- tempResponse_daily_list(all_past_scenario_temps,\r\n                                                    latitude = 46.6,\r\n                                                    Start_JDay = 305,\r\n                                                    End_JDay = 59,\r\n                                                    models = models)\r\n\r\nchill_hist_scenario_list <- lapply(chill_hist_scenario_list,\r\n                                   function(x) x %>%\r\n                                     filter(Perc_complete == 100))\r\n\r\n# Save generated chill data \r\nsave_temperature_scenarios(chill_hist_scenario_list, \"Yakima\",\"Yakima_hist_chill_305_59\")\r\n\r\n\r\n\r\n\r\n# Load generated chill data for Yakima\r\nchill_hist_scenario_list <- load_temperature_scenarios(\"Yakima\",\"Yakima_hist_chill_305_59\")\r\n\r\n# Compute the actual 'observed' chill for comparison\r\nscenarios <- names(chill_hist_scenario_list)[1:5]\r\n\r\nall_scenarios <- chill_hist_scenario_list[[scenarios[1]]] %>%\r\n  mutate(scenario = as.numeric(scenarios[1]))\r\n\r\nfor (sc in scenarios[2:5])\r\n  all_scenarios <- all_scenarios %>%\r\n  rbind(chill_hist_scenario_list[[sc]] %>%\r\n          cbind(\r\n            scenario=as.numeric(sc))\r\n  ) %>%\r\n  filter(Perc_complete == 100)\r\n\r\nactual_chill <- tempResponse_daily_list(Yakima_temps,\r\n                                        latitude=46.6,\r\n                                        Start_JDay = 305,\r\n                                        End_JDay = 59,\r\n                                        models)[[1]] %>%\r\n  filter(Perc_complete == 100)\r\n\r\n\r\n\r\n\r\n# Visualize chill accumulation \r\nggplot(data = all_scenarios,\r\n       aes(scenario,\r\n           Chill_Portions,\r\n           fill = factor(scenario))) +\r\n  geom_violin() +\r\n  ylab(\"Chill accumulation (Chill Portions)\") +\r\n  xlab(\"Scenario year\") +\r\n  theme_bw(base_size = 15) +\r\n  ylim(c(0,90)) +\r\n  geom_point(data = actual_chill,\r\n             aes(End_year,\r\n                 Chill_Portions,\r\n                 fill = \"blue\"),\r\n             col = \"blue\",\r\n             show.legend = FALSE) +\r\n  scale_fill_discrete(name = \"Scenario\",\r\n                      breaks = unique(all_scenarios$scenario)) \r\n\r\n\r\n\r\n\r\n\r\n# Save observed chill data for Yakima\r\nwrite.csv(actual_chill,\"Yakima/Yakima_observed_chill_305_59.csv\", row.names = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:38:24+01:00"
    },
    {
      "path": "improve.html",
      "title": "Can we improve the performance of PhenoFlex?",
      "author": [],
      "contents": "\r\nMotivation\r\nThe PhenoFlex model is relatively new and needs validation under different conditions. Additionally, some results from the original publication (Luedeling et al., 2021) suggest potentially implausible temperature responses during chill accumulation (e.g., the chill response for PhenoFlex_fitted in apple ‘Boskoop’).\r\nChill and heat response plots for two versions of the PhenoFlex modelThe figure above shows an unexpected chill response at temperatures above 25 °C. Since such high temperatures may even hinder chill accumulation, this suggests that the calibration conditions during endo-dormancy were not sufficiently variable. This highlights the need for more relevant temperature data to achieve a more accurate parameter set.\r\nUsing Experimental Phenology Data to Assess PhenoFlex\r\nIn the lab, the decision was made to use data from the tree-moving experiment, analyzed in the chapter on Experimentally Enhanced PLS, to calibrate the PhenoFlex model. This chapter provides an overview of the process and key results. For further details, the published analysis (Fernandez et al., 2022) can be referred to.\r\nThe weather and phenology data will be loaded from the data folder, followed by cleaning procedures to ensure compliance with the PhenoFlex format. If the files are not yet available, they need to be downloaded and saved in the data folder.\r\n\r\n\r\n# Load the data from the folder\r\ndata <- read_tab(\"data/final_weather_data_S1_S2_apple_hourly.csv\")\r\n\r\n# Generate a new column (Year_2) to simulate the year and comply with the format of PhenoFlex functions\r\ndata[\"Year_2\"] <- data$Treatment + data$Year \r\n\r\n# Since this experiment was conducted during two consecutive seasons, the next step will fix a small continuity issue\r\n# generated during the season 2\r\ndata[data$Treatment >= 34, \"Year_2\"] <-\r\n  data[data$Treatment >= 34, \"Year_2\"] - 1\r\n\r\n# For further compatibility, I will now select the columns needed and will drop \"Year\" (the original one)\r\ndata <- data[c(\"YEARMODA\", \"Year_2\", \"Month\",\r\n               \"Day\", \"Hour\", \"JDay\", \"Temp\")]\r\n\r\n# To replace the missing \"Year\" column, I will now change the name of the column\r\ncolnames(data)[which(colnames(data) == \"Year_2\")] <- \"Year\"\r\n\r\n\r\n# Import the phenology data from the repository\r\npheno <- read_tab(\"data/final_bio_data_S1_S2_apple.csv\")\r\n\r\n# Remove troubling treatments\r\npheno <- pheno[!(pheno$Treatment %in% c(36, 3, 23, 24, 17, 18, 61)), \r\n               c(\"Treatment\", \"pheno\")]\r\n\r\npheno[\"Treatment\"] <- pheno$Treatment + 2019\r\n\r\ncolnames(pheno) <- c(\"Year\", \"pheno\")\r\n\r\n\r\nThe format of both datasets can now be examined using the head() function to observe their structure.\r\nThe weather records contain the hourly temperature to which the trees were exposed during the experiment.\r\n\r\n\r\nhead(data)\r\n\r\n\r\n\r\n\r\nYEARMODA\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nHour\r\n\r\n\r\nJDay\r\n\r\n\r\nTemp\r\n\r\n\r\n20180606\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n0\r\n\r\n\r\n157\r\n\r\n\r\n13.60700\r\n\r\n\r\n20180606\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n1\r\n\r\n\r\n157\r\n\r\n\r\n12.49315\r\n\r\n\r\n20180606\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n2\r\n\r\n\r\n157\r\n\r\n\r\n11.76103\r\n\r\n\r\n20180606\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n3\r\n\r\n\r\n157\r\n\r\n\r\n11.11030\r\n\r\n\r\n20180606\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n4\r\n\r\n\r\n157\r\n\r\n\r\n10.45488\r\n\r\n\r\n20180606\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n5\r\n\r\n\r\n157\r\n\r\n\r\n11.95780\r\n\r\n\r\nThe phenology data includes the date of full bloom (in day of the year), recorded when 50% of the flowers were open and the first petals were falling, according to the BBCH scale.\r\n\r\n\r\nhead(pheno)\r\n\r\n\r\n\r\n\r\nYear\r\n\r\n\r\npheno\r\n\r\n\r\n2020\r\n\r\n\r\n115\r\n\r\n\r\n2021\r\n\r\n\r\n98\r\n\r\n\r\n2023\r\n\r\n\r\n64\r\n\r\n\r\n2024\r\n\r\n\r\n49\r\n\r\n\r\n2025\r\n\r\n\r\n64\r\n\r\n\r\n2026\r\n\r\n\r\n50\r\n\r\n\r\nBoth datasets contain a treatment column, representing a “fake” year or season corresponding to the actual treatment applied in the experiment.\r\nFor comparison, two versions of the analysis were developed to calibrate the PhenoFlex model. In version 1 (PhenoFlex_marginal), all available experimental seasons were included, even five seasons that might have been marginal in terms of temperature for overcoming the dormancy of apple trees. In version 2 (PhenoFlex_normal), these marginal seasons were excluded from the calibration dataset\r\nExperimental seasons used in the different versions of the calibration procedure. The histogram on the left shows the distribution of mean temperature among experimental seasonsAs shown in the figure above, the five marginal seasons were identified as a small cluster at the upper limit of the distribution after sorting the seasons based on the mean temperature experienced by the trees from the beginning of the experiment until the moment of full bloom.\r\nIn the following section, different datasets are created for the two versions of the analysis. The treatments corresponding to the years c(2032, 2061, 2065, 2077, 2081), identified as marginal seasons, are removed from the calibration dataset in PhenoFlex_normal.\r\n\r\n\r\npheno_marginal <- pheno\r\n\r\npheno_normal <- pheno[!(pheno$Year %in% \r\n                          c(2032, 2061, 2065, 2077, 2081)), ]\r\n\r\n\r\nThe same approach can now be applied to the weather data by defining two vectors containing the seasons selected for model calibration. Forty seasons will be randomly chosen for calibration in both versions of the analysis, while the remaining 14 experimental seasons will be reserved for validation.\r\n\r\n\r\n# Define a vector of calibration and validation seasons. Marginal includes\r\n# the marginal seasons\r\ncalibration_seasons <-\r\n  sort(sample(pheno_normal$Year, \r\n              40, \r\n              replace = FALSE))\r\n\r\ncalibration_seasons_marginal <-\r\n  sort(c(sample(calibration_seasons, \r\n                35,\r\n                replace = FALSE),\r\n         pheno_marginal$Year[which(!(pheno_marginal$Year %in%\r\n                                       pheno_normal$Year))]))\r\n\r\ncalibration_seasons_normal <- calibration_seasons\r\n\r\n# Common validation seasons\r\nvalidation_seasons <- \r\n  sort(pheno_normal[!(pheno_normal$Year %in% \r\n                        calibration_seasons), \"Year\"])\r\n\r\n# Define the list of seasons (weather data)\r\nseason_list_marginal <- \r\n  genSeasonList(data,\r\n                mrange = c(9, 7),\r\n                years = calibration_seasons_marginal)\r\n\r\nseason_list_normal <-\r\n  genSeasonList(data,\r\n                mrange = c(9, 7),\r\n                years = calibration_seasons_normal)\r\n\r\n\r\nThe techniques learned in the chapter on The PhenoFlex Model are now applied to fit the model parameters to the data. The fitting procedure begins with wide ranges, especially for yc and zc, to allow the model to identify the best estimates.\r\n\r\n\r\n# Set the initial parameters (wide ranges)\r\n#   yc,  zc,  s1, Tu,     E0,      E1,     A0,          A1,   Tf, Tc, Tb, slope\r\nlower <- \r\n  c(20, 100, 0.1,  0, 3000.0,  9000.0, 6000.0,       5.e13,    0,  0,  0,  0.05)\r\npar   <- \r\n  c(40, 190, 0.5, 25, 3372.8,  9900.3, 6319.5, 5.939917e13,    4, 36,  4,  1.60)\r\nupper <-\r\n  c(80, 500, 1.0, 30, 4000.0, 10000.0, 7000.0,       6.e13,   10, 40, 10, 50.00)\r\n\r\n# Run the fitter\r\npheno_fit_marginal <- \r\n  phenologyFitter(par.guess = par,\r\n                  modelfn = PhenoFlex_GDHwrapper,\r\n                  bloomJDays = pheno_marginal[pheno_marginal$Year %in%\r\n                                                calibration_seasons_marginal,\r\n                                              \"pheno\"],\r\n                  SeasonList = season_list_marginal,\r\n                  lower = lower,\r\n                  upper = upper,\r\n                  control = list(smooth = FALSE,\r\n                                 verbose = FALSE,\r\n                                 maxit = 100,\r\n                                 nb.stop.improvement = 10))\r\n\r\n# Same for version 2\r\npheno_fit_normal <- \r\n  phenologyFitter(par.guess = par,\r\n                  modelfn = PhenoFlex_GDHwrapper,\r\n                  bloomJDays = pheno_normal[pheno_normal$Year %in%\r\n                                              calibration_seasons_normal,\r\n                                            \"pheno\"],\r\n                  SeasonList = season_list_normal,\r\n                  lower = lower,\r\n                  upper = upper,\r\n                  control = list(smooth = FALSE,\r\n                                 verbose = FALSE,\r\n                                 maxit = 100,\r\n                                 nb.stop.improvement = 10))\r\n\r\n\r\nThe argument maxit in the control list is set to 100 to ensure the code runs quickly. For a more thorough assessment, higher values (e.g., 1,000) would likely be more appropriate.\r\nThe results of the fitting, including the model parameters and predicted bloom dates, are saved and read from the folder to save time in future analyses.\r\n\r\n\r\nwrite.csv(pheno_fit_marginal$par,\r\n          \"data/PhenoFlex_marginal_params.csv\",\r\n          row.names = FALSE)\r\n\r\nwrite.csv(pheno_fit_normal$par,\r\n          \"data/PhenoFlex_normal_params.csv\",\r\n          row.names = FALSE)\r\n\r\nwrite.csv(data.frame(pheno_marginal[pheno_marginal$Year %in%\r\n                                      calibration_seasons_marginal, ],\r\n                     \"Predicted\" = pheno_fit_marginal$pbloomJDays),\r\n          \"data/PhenoFlex_marginal_predicted_bloom.csv\",\r\n          row.names = FALSE)\r\n\r\nwrite.csv(data.frame(pheno_normal[pheno_normal$Year %in%\r\n                                    calibration_seasons_normal, ],\r\n                     \"Predicted\" = pheno_fit_normal$pbloomJDays),\r\n          \"data/PhenoFlex_normal_predicted_bloom.csv\",\r\n          row.names = FALSE)\r\n\r\n\r\nThe results from the fitting procedure can be examined by obtaining the predictions made by the model using the fitted parameters. The prediction error can then be estimated based on these predictions.\r\n\r\n\r\n# Read the parameters\r\nparams_marginal <- read.csv(\"data/PhenoFlex_marginal_params.csv\")[[1]]\r\nparams_normal <- read.csv(\"data/PhenoFlex_normal_params.csv\")[[1]]\r\n\r\n# Generate a data set to collect the outputs of the fitting for the calibration data \r\nout_df_marginal <- read.csv(\"data/PhenoFlex_marginal_predicted_bloom.csv\")\r\nout_df_normal <- read.csv(\"data/PhenoFlex_normal_predicted_bloom.csv\")\r\n\r\n# Compute the error (observed - predicted)\r\nout_df_marginal[[\"Error\"]] <-\r\n  out_df_marginal$pheno - out_df_marginal$Predicted\r\n\r\nout_df_normal[[\"Error\"]] <-\r\n  out_df_normal$pheno - out_df_normal$Predicted\r\n\r\n\r\nModel performance metrics can now be computed based on the previously estimated prediction error. While this may not be highly relevant during the calibration procedure, it provides a way to compare the performance of the two PhenoFlex versions.\r\n\r\n\r\n\r\n\r\n\r\ncalibration_metrics\r\n\r\n\r\n\r\n\r\nMetric\r\n\r\n\r\nPhenoFlex_marginal\r\n\r\n\r\nPhenoFlex_normal\r\n\r\n\r\nRMSEP\r\n\r\n\r\n10.61386\r\n\r\n\r\n3.837238\r\n\r\n\r\nRPIQ\r\n\r\n\r\n3.39179\r\n\r\n\r\n10.163561\r\n\r\n\r\nThere is room for improvement, particularly given that only 10 iterations were used. However, the results clearly show that calibrating the model with the marginal seasons reduced the performance of PhenoFlex.\r\nThe next step involves plotting some of the results.\r\n\r\n\r\nout_df_all <- bind_rows(\"PhenoFlex marginal\" = out_df_marginal,\r\n                        \"PhenoFlex normal\" = out_df_normal,\r\n                        .id = \"PhenoFlex version\")\r\n\r\n# Plot the observed versus predicted values\r\nggplot(out_df_all,\r\n       aes(pheno, \r\n           Predicted)) +\r\n  geom_point() +\r\n  geom_abline(intercept = 0, slope = 1) +\r\n  labs(x = \"Observed\") +\r\n  facet_grid(~ `PhenoFlex version`) +\r\n  theme_bw()\r\n\r\n\r\n\r\nThe plot reveals that the version including the marginal seasons displays greater dispersion compared to the version that excludes the marginal seasons in the calibration of the framework.\r\nValidation\r\nAt this point, it’s important to evaluate how well both versions can predict bloom dates for seasons not included in the calibration dataset. To do this, the model parameters need to be extracted, and the function PhenoFlex_GDHwrapper() should be used. It’s essential to remember that the same set of seasons is used for the validation of both the PhenoFlex_marginal and PhenoFlex_normal versions.\r\n\r\n\r\n# Generate a validation data set with phenology data\r\nvalid_df_marginal <- pheno_marginal[pheno_marginal$Year %in% \r\n                                      validation_seasons, ]\r\n\r\nvalid_df_normal <- pheno_normal[pheno_normal$Year %in%\r\n                                  validation_seasons, ]\r\n\r\n# Generate a list of seasons with weather data for the validation procedure\r\nvalid_season_list <- genSeasonList(data,\r\n                                   mrange = c(9, 7), \r\n                                   years = validation_seasons)\r\n\r\n# Estimate the bloom dates with PhenoFlexGDHwrapper\r\nfor (i in 1 : nrow(valid_df_marginal)) {\r\n  valid_df_marginal[i, \"Predicted\"] <-\r\n    PhenoFlex_GDHwrapper(valid_season_list[[i]],\r\n                         params_marginal)\r\n  }\r\n\r\n# The same for the second version\r\nfor (i in 1 : nrow(valid_df_normal)) {\r\n  valid_df_normal[i, \"Predicted\"] <-\r\n    PhenoFlex_GDHwrapper(valid_season_list[[i]],\r\n                         params_normal)\r\n  }\r\n\r\n# Compute the error (observed - predicted)\r\nvalid_df_marginal[[\"Error\"]] <-\r\n  valid_df_marginal$pheno - valid_df_marginal$Predicted\r\n\r\nvalid_df_normal[[\"Error\"]] <- \r\n  valid_df_normal$pheno - valid_df_normal$Predicted\r\n\r\n\r\nSince the difference between the observed values and the values predicted by the model (i.e., the prediction error) is already known for the validation dataset, model performance metrics such as RMSEP and RPIQ can be estimated. The functions RMSEP() and RPIQ() from the chillR package can be used to calculate these metrics.\r\n\r\n\r\n\r\n\r\n\r\nvalidation_metrics\r\n\r\n\r\n\r\n\r\nMetric\r\n\r\n\r\nPhenoFlex_marginal\r\n\r\n\r\nPhenoFlex_normal\r\n\r\n\r\nRMSEP\r\n\r\n\r\n11.865823\r\n\r\n\r\n4.458876\r\n\r\n\r\nRPIQ\r\n\r\n\r\n1.411617\r\n\r\n\r\n3.756552\r\n\r\n\r\nThe results presented in the table confirm the pattern observed during the calibration procedure, with the inclusion of the marginal seasons lowering the performance of PhenoFlex. However, visualizing the results graphically is always helpful. Therefore, the next step is to plot the results.\r\n\r\n\r\n# Create a unique data set\r\nvalid_df_all <- \r\n  bind_rows(\"PhenoFlex marginal\" = valid_df_marginal,\r\n            \"PhenoFlex normal\" = valid_df_normal,\r\n            .id = \"PhenoFlex version\")\r\n\r\n# Plot the calibrated and validated \r\nggplot(out_df_all,\r\n       aes(pheno,\r\n           Predicted,\r\n           color = \"Calibration\")) +\r\n  geom_point() +\r\n  geom_point(data = valid_df_all,\r\n             aes(pheno, \r\n                 Predicted,\r\n                 color = \"Validation\")) + \r\n  scale_color_manual(values = c(\"cadetblue\",\r\n                                \"firebrick\")) +\r\n  geom_abline(intercept = 0, \r\n              slope = 1) +\r\n  labs(x = \"Observed\",\r\n       color = \"Dataset\") +\r\n  facet_grid(~ `PhenoFlex version`) +\r\n  theme_bw()\r\n\r\n\r\n\r\nOnce again, the red dots (representing the validation seasons) in PhenoFlex_marginal appear more dispersed and farther from the line x = y compared to the red dots in the PhenoFlex_normal version.\r\nNext, the chill and heat response curves for both versions of the analysis will be examined. To do this, the functions apply_const_temp(), gen_bell(), and GDH_response(), which were introduced in the chapter The PhenoFlex model - a second look, will be used. These functions will be reloaded, with the process hidden using chunk options.\r\n\r\n\r\n\r\n\r\n\r\n# Create a data set with theoretical temperatures and heat and chill responses\r\ntemp_response_marginal <- data.frame(Temp = seq(-5, 60, 0.1),\r\n                                     Chill_res = gen_bell(params_marginal,\r\n                                                          temp_values = seq(-5, 60, 0.1)),\r\n                                     Heat_res = GDH_response(params_marginal,\r\n                                                             seq(-5, 60, 0.1)),\r\n                                     Version = \"PhenoFlex marginal\")\r\n\r\ntemp_response_normal <- data.frame(Temp = seq(-5, 60, 0.1),\r\n                                   Chill_res = gen_bell(params_normal,\r\n                                                        temp_values = seq(-5, 60, 0.1)),\r\n                                   Heat_res = GDH_response(params_normal,\r\n                                                           seq(-5, 60, 0.1)),\r\n                                   Version = \"PhenoFlex normal\")\r\n\r\n\r\n# Generate a single data set\r\ntemp_response <- bind_rows(temp_response_marginal, \r\n                           temp_response_normal)\r\n\r\n# Plotting\r\nggplot(temp_response, \r\n       aes(Temp)) +\r\n  geom_line(aes(y = Chill_res,\r\n                color = \"Chill\")) +\r\n  geom_line(aes(y = Heat_res * 25, \r\n                color = \"Heat\")) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.001, 0.01)),\r\n                     sec.axis = sec_axis(~ . / 25, \r\n                                         name = \"Arbitrary heat units\")) +\r\n  scale_x_continuous(expand = expansion(mult = 0)) +\r\n  scale_color_manual(values = c(\"blue4\", \r\n                                \"firebrick\")) +\r\n  labs(x = \"Temperature (°C)\",\r\n       y = \"Arbitrary chill units\",\r\n       color = NULL) +\r\n  facet_grid(Version ~ .) +\r\n  theme_bw() +\r\n  theme(legend.position = c(0.85, 0.85))\r\n\r\n\r\n\r\nSome differences in the chill response are observed between the versions, while the heat curves are similar. PhenoFlex_normal somewhat emulates the chill response seen in the original chill model (i.e., the Dynamic model). In PhenoFlex_marginal, the accumulation of chill begins only at temperatures above 5 °C. Regarding the heat response, PhenoFlex_normal shows heat accumulation at slightly higher temperatures compared to PhenoFlex_marginal. However, since very few model iterations were used, these results are challenging to interpret.\r\nConclusions\r\nThis was a condensed version of the analysis, but it provides some valuable insights into the potential limitations of the modeling framework. It appears that, under extreme conditions, the process of dormancy breaking may be influenced by mechanisms not accounted for in the PhenoFlex framework (or any other existing frameworks). To test this hypothesis, however, further systematic studies would be necessary.\r\nExercises on improving the performance of PhenoFlex\r\nWhat was the objective of this work?\r\nThe objective of this work was to calibrate the PhenoFlex model using data from the tree-moving experiment and analyze its performance. The aim was to assess the impact of including marginal seasons on the model’s calibration and performance, particularly in predicting bloom dates, and to explore potential limitations of the modeling framework.\r\nWhat was the main conclusion?\r\nThe main conclusion of the work was that calibrating the PhenoFlex model with the marginal seasons lowered its performance. The results suggested that extreme conditions might involve mechanisms not considered in the PhenoFlex framework, which could affect the process of dormancy breaking in apple trees. However, further studies are needed to explore this potential limitation in more detail.\r\nWhat experiments could we conduct to test the hypothesis that emerged at the end of the conclusion?\r\nTo test the hypothesis that dormancy breaking is influenced by mechanisms not accounted for in the PhenoFlex framework under extreme conditions, additional systematic studies could be conducted. These experiments might include:\r\nTesting with more iterations: Increasing the number of iterations in the model to better understand the impact of marginal seasons on performance\r\nManipulating environmental conditions: Conducting experiments that simulate extreme temperatures or other environmental stressors to observe how these conditions affect dormancy breaking\r\nIncluding additional physiological factors: Exploring the incorporation of other biological processes or mechanisms, such as hormonal changes in trees during dormancy, which might influence the bloom timing\r\nField experiments under diverse climates: Setting up controlled field trials in different climatic regions with varying temperature extremes to assess the model’s robustness under different environmental stresses\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:39:14+01:00"
    },
    {
      "path": "index.html",
      "title": "Tree phenology analysis with R",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Jacqueline Wingen\r\n          \r\n          \r\n          Home\r\n          Goals & Tools\r\n          \r\n          \r\n          Introduction\r\n           \r\n          ▾\r\n          \r\n          \r\n          Tree Dormancy\r\n          Climate change and impact projection\r\n          Winter chill projections\r\n          Manual chill analysis\r\n          Chill models\r\n          \r\n          \r\n          \r\n          \r\n          Temperature\r\n           \r\n          ▾\r\n          \r\n          \r\n          Making hourly temperatures\r\n          Useful tools in R\r\n          Getting temperature data\r\n          Filling gaps in temperature records\r\n          Generating temperature scenarios\r\n          Saving and loading data\r\n          Historic temperature scenarios\r\n          Future temperature scenarios\r\n          Making CMIP6 scenarios\r\n          Making CMIP5 scenarios with the ClimateWizard\r\n          Plotting future scenarios\r\n          Chill model comparison\r\n          \r\n          \r\n          \r\n          \r\n          PLS\r\n           \r\n          ▾\r\n          \r\n          \r\n          Simple phenology analysis\r\n          Delineating temperature response phases with PLS regression\r\n          Successes and limitations of PLS regression analysis\r\n          PLS regression with agroclimatic metrics\r\n          Examples of PLS regression with agroclimatic metrics\r\n          Why PLS doesn’t always work\r\n          Evaluating PLS outputs\r\n          The relative importance of chill and heat\r\n          Experimentally enhanced PLS\r\n          Making valid tree phenology models\r\n          \r\n          \r\n          \r\n          \r\n          PhenoFlex\r\n           \r\n          ▾\r\n          \r\n          \r\n          The PhenoFlex model\r\n          The PhenoFlex model - a second look\r\n          Can we improve the performance of PhenoFlex?\r\n          Frost risk analysis\r\n          A robust method to estimate future frost risk\r\n          Major concepts\r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Tree phenology analysis with R\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                         GitHub\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                         Email\r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n        \r\n        \r\n          \r\n            \r\n            Welcome!\r\n            Hi, my name is Jacqueline. I’m a master’s student in Crop\r\n            Sciences at the University of Bonn.\r\n            This is my learning logbook for the module “Tree phenology\r\n            analysis with R”. This module provides an overview of\r\n            methods to study the impact of climate and climate change on\r\n            tree phenology.\r\n            It is designed for those who may not yet be familiar with\r\n            phenology or how to analyze climate change effects, but it\r\n            also aims to offer new insights for those with existing\r\n            knowledge in these areas. Initially developed for M.Sc.\r\n            students in Crop Science and Agricultural Science and\r\n            Resource Management in the Tropics and Subtropics (ARTS) at\r\n            the University of Bonn, the material is accessible to anyone\r\n            interested.\r\n            For more information, feel free to visit the GitHub page\r\n            of the HortiBonn\r\n            research group at the University of Bonn, as well as the\r\n            website of the Tree\r\n            Phenology Analysis with R course, where all the\r\n            information for this learning logbook come from.\r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Tree phenology analysis with R\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                       GitHub\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                       Email\r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              \r\n              Welcome!\r\n              Hi, my name is Jacqueline. I’m a master’s student in\r\n              Crop Sciences at the University of Bonn.\r\n              This is my learning logbook for the module “Tree phenology\r\n              analysis with R”. This module provides an overview of\r\n              methods to study the impact of climate and climate change\r\n              on tree phenology.\r\n              It is designed for those who may not yet be familiar\r\n              with phenology or how to analyze climate change effects,\r\n              but it also aims to offer new insights for those with\r\n              existing knowledge in these areas. Initially developed for\r\n              M.Sc. students in Crop Science and Agricultural Science\r\n              and Resource Management in the Tropics and Subtropics\r\n              (ARTS) at the University of Bonn, the material is\r\n              accessible to anyone interested.\r\n              For more information, feel free to visit the GitHub\r\n              page of the HortiBonn research\r\n              group at the University of Bonn, as well as the website of\r\n              the Tree\r\n              Phenology Analysis with R course, where all the\r\n              information for this learning logbook come from.\r\n              \r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2025-03-15T09:39:26+01:00"
    },
    {
      "path": "major_concepts.html",
      "title": "Major concepts",
      "author": [],
      "contents": "\r\nTree Dormancy\r\nWoody plants in cold climates enter a dormant phase during winter. To\r\nresume growth in spring, they must first go through a period of cold\r\nexposure (endodormancy) and then a period of warmth (ecodormancy). The\r\nrelease from dormancy is influenced by factors such as intercellular\r\ncommunication, carbohydrate storage and transport, plant hormones, and\r\nthe regulation of specific genes. The exact mechanisms are not yet fully\r\nunderstood, and there are no complete process-based models.\r\nClimate Change\r\nGlobal warming leads to changes in temperature and precipitation\r\npatterns worldwide. While the exact developments remain uncertain,\r\nclimate scientists have a strong understanding to create models of\r\nfuture conditions. The extent of future warming depends on atmospheric\r\ngreenhouse gas concentrations, which are uncertain. Therefore, different\r\nscenarios are used to represent these uncertainties. Effective climate\r\nchange mitigation requires significant reductions in greenhouse gas\r\nemissions, particularly in the energy sector.\r\nPhenology Modeling\r\nModeling phenology, which is the timing of plant growth phases, is\r\nchallenging due to gaps in understanding. Various models exist for chill\r\nand heat accumulation, but estimates of their effects on phenology\r\ndiffer greatly. The Dynamic Model is the leading model for chill\r\naccumulation, while the Growing Degree Hour Model is favored for heat\r\naccumulation. Some comprehensive modeling frameworks attempt to predict\r\nfuture phenology based on temperature data, but they have limitations\r\nand fail to account for uncertainties.\r\nPhenology Responses to Global Warming\r\nMost plant species have advanced their phenology in response to rising\r\ntemperatures. However, this trend may not continue indefinitely as\r\nwarming progresses. In areas where temperatures are high enough to\r\ninterfere with chill accumulation during endodormancy, phenology shifts\r\nmay slow, stop, or even reverse. This hypothesis is supported by\r\nfundamental principles but requires further validation.\r\nThe PhenoFlex Modeling Framework\r\nPhenoFlex integrates effective chill and heat accumulation models into a\r\ncomprehensive framework to predict the timing of spring phenological\r\nphases. The model can be parameterized using long-term phenology data\r\nthrough an empirical fitting algorithm called Simulated Annealing. It\r\nallows the characterization of cultivar-specific temperature response\r\nfunctions. Initial results are promising, but the model has limitations,\r\nincluding challenges in generalizing across species and the risk of\r\nsuboptimal parameters from the fitting procedure.\r\nKey Concepts\r\nReproducibility & Transparency: Science should prioritize\r\nreproducibility and transparency. While experiments are often\r\nchallenging to fully replicate, modeling studies typically allow for\r\nhigher reproducibility. Methods should be clearly documented, and\r\nthe code and raw data should be shared for verification.\r\nTools: GitHub, R, RStudio, and various R add-ons were used in\r\nthis study to enhance workflows. Whether these tools will remain\r\nrelevant in the future is uncertain, but using effective tools\r\nremains important.\r\nAutomation: Repetitive tasks should be automated to free up time\r\nfor more creative and meaningful work. This also helps in generating\r\ncomparable results across different contexts efficiently.\r\nThe Power of R: R is not just a statistical program but also a\r\npowerful tool for advanced statistical analyses, spatial analyses,\r\nanimated visualizations, and interactive applications. R is free and\r\na valuable investment for any scientific career.\r\nCuriosity and Interdisciplinarity: Focusing too narrowly on one\r\nfield can lead to deep expertise but may stifle innovation. Exposure\r\nto other fields fosters new perspectives and can lead to\r\ngroundbreaking discoveries.\r\nUncertainty: Uncertainty is an inherent part of real-world\r\nproblems. Models are approximations of complex natural processes,\r\nand acknowledging and quantifying this uncertainty is essential.\r\nEnsemble Analysis: In cases like climate change, where\r\nuncertainty arises from not knowing which scenario will unfold,\r\nensemble analysis combines multiple models and scenarios to provide\r\na more comprehensive view of possible future developments.\r\nP-Hacking: P-hacking refers to manipulating data to find random\r\ncorrelations that lack true significance. This leads to findings\r\nthat do not provide meaningful insights into the system.\r\nDangers of Machine Learning: Machine learning can be problematic\r\nwhen applied without domain-specific knowledge, as many models\r\noperate as “black boxes,” making it difficult to understand how\r\nconclusions are reached. This increases the risk of\r\nmisinterpretation and flawed conclusions.\r\nRationalizing: A problematic practice in science involves\r\ndrawing conclusions from data and then crafting stories to justify\r\nthe results. These explanations can mislead and should be avoided.\r\nOverfitting: When models are too complex and capture random\r\nnoise in the data rather than the true underlying process,\r\noverfitting occurs. This leads to incorrect conclusions.\r\nProcess and Model Validation: Models should not only fit the\r\ndata but also accurately represent the underlying biological or\r\necological processes. Models need validation to ensure they provide\r\nreliable predictions in real-world scenarios.\r\nModel Validation and Purpose: Validation should reflect the\r\ncontext of the intended prediction. For example, climate change\r\nmodels should use data from warmer conditions, and prediction models\r\nshould be tested on years without prior data.\r\nOur Role in Research: Scientific research should be grounded in\r\ntheory, hypotheses, and predictions. There is a debate over whether\r\nprior knowledge and beliefs should influence research, but\r\nintegrating expertise can enhance scientific inquiry if assumptions\r\nare made explicit and continuously questioned.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:39:34+01:00"
    },
    {
      "path": "making_temp.html",
      "title": "Making hourly temperatures",
      "author": [],
      "contents": "\r\nThe calculation of Chilling Hours requires hourly temperature data. However, since only daily minimum and maximum temperatures are often available, methods for hourly interpolation must be developed. Previous approaches used linear interpolation and triangular temperature profiles (Baldocchi & Wong, 2008):\r\n\r\n\r\n\r\nHowever, triangular temperature profiles are not always realistic, as temperature increases and decreases are asymmetric.\r\nIdealized Daily Temperature Model\r\nDale E. Linvill (1990) developed a model that describes daytime warming with a sine curve and nighttime cooling with a logarithmic function. The chillR function daylength() uses astronomical calculations to determine the length of daylight:\r\n\r\n\r\nDays <- daylength(latitude = 50.4, JDay = 1:365)\r\nDays_df <-\r\n  data.frame(\r\n    JDay = 1:365,\r\n    Sunrise = Days$Sunrise,\r\n    Sunset = Days$Sunset,\r\n    Daylength = Days$Daylength\r\n  )\r\nDays_df <- pivot_longer(Days_df, cols = c(Sunrise:Daylength))\r\n\r\nggplot(Days_df, aes(JDay, value)) +\r\n  geom_line(lwd = 1.5) +\r\n  facet_grid(cols = vars(name)) +\r\n  ylab(\"Time of Day / Daylength (Hours)\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nThe stack_hourly_temps() function calculates hourly temperatures based on daily Tmin/Tmax values and latitude:\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nTmax\r\n\r\n\r\nTmin\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n8.2\r\n\r\n\r\n5.1\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n9.1\r\n\r\n\r\n5.0\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n10.4\r\n\r\n\r\n3.3\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n8.4\r\n\r\n\r\n4.5\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n7\r\n\r\n\r\n12.0\r\n\r\n\r\n6.9\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n8\r\n\r\n\r\n11.2\r\n\r\n\r\n8.6\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n9\r\n\r\n\r\n13.9\r\n\r\n\r\n8.5\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n10\r\n\r\n\r\n14.5\r\n\r\n\r\n3.6\r\n\r\n\r\nAnd the following process describes how hourly temperatures can be calculated based on the idealized daily temperature curve:\r\n\r\n\r\nstack_hourly_temps(KA_weather, latitude = 50.4)\r\n\r\n\r\n\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nTmax\r\n\r\n\r\nTmin\r\n\r\n\r\nJDay\r\n\r\n\r\nHour\r\n\r\n\r\nTemp\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n4.844164\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n4\r\n\r\n\r\n4.746566\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n4.656244\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n4.572187\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n4.493583\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n8\r\n\r\n\r\n4.569464\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n9\r\n\r\n\r\n5.384001\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n10\r\n\r\n\r\n6.139939\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n11\r\n\r\n\r\n6.787169\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n12\r\n\r\n\r\n7.282787\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n13\r\n\r\n\r\n7.593939\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n14\r\n\r\n\r\n7.700000\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n15\r\n\r\n\r\n7.593939\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n16\r\n\r\n\r\n7.282787\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n17\r\n\r\n\r\n6.591821\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n18\r\n\r\n\r\n6.168074\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n19\r\n\r\n\r\n5.870570\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n20\r\n\r\n\r\n5.641106\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n21\r\n\r\n\r\n5.454280\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n22\r\n\r\n\r\n5.296704\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n7.7\r\n\r\n\r\n4.5\r\n\r\n\r\n5\r\n\r\n\r\n23\r\n\r\n\r\n5.160445\r\n\r\n\r\n\r\nBased on this the following plot shows the calculated data:\r\n\r\n\r\n\r\nEmpirical Daily Temperature Profiles\r\nIn complex topographies (e.g., Oman), idealized models can be inaccurate. Here, the Empirical_daily_temperature_curve() function helps to empirically determine typical hourly temperature patterns:\r\n\r\n\r\nempi_curve <- Empirical_daily_temperature_curve(Winters_hours_gaps)\r\n\r\n\r\n\r\n\r\n\r\nMonth\r\n\r\n\r\nHour\r\n\r\n\r\nPrediction_coefficient\r\n\r\n\r\n3\r\n\r\n\r\n0\r\n\r\n\r\n0.1774859\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n0.1550693\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n0.1285651\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n0.1145597\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n0.0696064\r\n\r\n\r\n3\r\n\r\n\r\n5\r\n\r\n\r\n0.0339583\r\n\r\n\r\n3\r\n\r\n\r\n6\r\n\r\n\r\n0.0000000\r\n\r\n\r\n3\r\n\r\n\r\n7\r\n\r\n\r\n0.0313115\r\n\r\n\r\n3\r\n\r\n\r\n8\r\n\r\n\r\n0.3121959\r\n\r\n\r\n3\r\n\r\n\r\n9\r\n\r\n\r\n0.4953232\r\n\r\n\r\n3\r\n\r\n\r\n10\r\n\r\n\r\n0.6819674\r\n\r\n\r\n3\r\n\r\n\r\n11\r\n\r\n\r\n0.8227423\r\n\r\n\r\n3\r\n\r\n\r\n12\r\n\r\n\r\n0.9506491\r\n\r\n\r\n3\r\n\r\n\r\n13\r\n\r\n\r\n0.9662604\r\n\r\n\r\n3\r\n\r\n\r\n14\r\n\r\n\r\n0.9915996\r\n\r\n\r\n3\r\n\r\n\r\n15\r\n\r\n\r\n1.0000000\r\n\r\n\r\n3\r\n\r\n\r\n16\r\n\r\n\r\n0.9490319\r\n\r\n\r\n3\r\n\r\n\r\n17\r\n\r\n\r\n0.8483098\r\n\r\n\r\n3\r\n\r\n\r\n18\r\n\r\n\r\n0.6864529\r\n\r\n\r\n3\r\n\r\n\r\n19\r\n\r\n\r\n0.4945415\r\n\r\n\r\n3\r\n\r\n\r\n20\r\n\r\n\r\n0.3636642\r\n\r\n\r\n3\r\n\r\n\r\n21\r\n\r\n\r\n0.2972377\r\n\r\n\r\n3\r\n\r\n\r\n22\r\n\r\n\r\n0.2360349\r\n\r\n\r\n3\r\n\r\n\r\n23\r\n\r\n\r\n0.1794802\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n0.1960789\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n0.1407018\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n0.1283250\r\n\r\n\r\n4\r\n\r\n\r\n3\r\n\r\n\r\n0.0819307\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\n0.0541415\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n0.0188241\r\n\r\n\r\n4\r\n\r\n\r\n6\r\n\r\n\r\n0.0000000\r\n\r\n\r\n4\r\n\r\n\r\n7\r\n\r\n\r\n0.1697052\r\n\r\n\r\n4\r\n\r\n\r\n8\r\n\r\n\r\n0.4442722\r\n\r\n\r\n4\r\n\r\n\r\n9\r\n\r\n\r\n0.5939797\r\n\r\n\r\n4\r\n\r\n\r\n10\r\n\r\n\r\n0.7363923\r\n\r\n\r\n4\r\n\r\n\r\n11\r\n\r\n\r\n0.8399804\r\n\r\n\r\n4\r\n\r\n\r\n12\r\n\r\n\r\n0.9245702\r\n\r\n\r\n4\r\n\r\n\r\n13\r\n\r\n\r\n0.9770693\r\n\r\n\r\n4\r\n\r\n\r\n14\r\n\r\n\r\n0.9963131\r\n\r\n\r\n4\r\n\r\n\r\n15\r\n\r\n\r\n1.0000000\r\n\r\n\r\n4\r\n\r\n\r\n16\r\n\r\n\r\n0.9568107\r\n\r\n\r\n4\r\n\r\n\r\n17\r\n\r\n\r\n0.8698369\r\n\r\n\r\n4\r\n\r\n\r\n18\r\n\r\n\r\n0.7343896\r\n\r\n\r\n4\r\n\r\n\r\n19\r\n\r\n\r\n0.5330597\r\n\r\n\r\n4\r\n\r\n\r\n20\r\n\r\n\r\n0.3941038\r\n\r\n\r\n4\r\n\r\n\r\n21\r\n\r\n\r\n0.3186075\r\n\r\n\r\n4\r\n\r\n\r\n22\r\n\r\n\r\n0.2594569\r\n\r\n\r\n4\r\n\r\n\r\n23\r\n\r\n\r\n0.2114486\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = empi_curve[1:96, ], aes(Hour, Prediction_coefficient)) +\r\n  geom_line(lwd = 1.3, \r\n            col = \"red\") + \r\n  facet_grid(rows = vars(Month)) + \r\n  xlab(\"Hour of the day\") +\r\n  ylab(\"Prediction coefficient\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nThe function Empirical_hourly_temperatures() uses these coefficients to generate hourly temperatures. To fill gaps in daily or hourly temperature records, the make_all_day_table() function is used.\r\n\r\n\r\ncoeffs <- Empirical_daily_temperature_curve(Winters_hours_gaps)\r\nWinters_daily <-\r\n  make_all_day_table(Winters_hours_gaps, input_timestep = \"hour\")\r\nWinters_hours <- Empirical_hourly_temperatures(Winters_daily, coeffs)\r\n\r\n\r\nThe next step is to plot the results to visualize the hourly temperature data. This enables a comparison between the empirical method, the triangular function, and the idealized temperature curve. Furthermore, actual observed temperatures will be used for validation. To streamline this process, the data will first be simplified for easier handling:\r\n\r\n\r\nWinters_hours <- Winters_hours[, c(\"Year\", \"Month\", \"Day\", \"Hour\", \"Temp\")]\r\ncolnames(Winters_hours)[ncol(Winters_hours)] <- \"Temp_empirical\"\r\nWinters_ideal <-\r\n  stack_hourly_temps(Winters_daily, latitude = 38.5)$hourtemps\r\nWinters_ideal <- Winters_ideal[, c(\"Year\", \"Month\", \"Day\", \"Hour\", \"Temp\")]\r\ncolnames(Winters_ideal)[ncol(Winters_ideal)] <- \"Temp_ideal\"\r\n\r\n\r\nThe next step is to generate the triangular dataset, requiring a clear understanding of its construction.\r\n\r\n\r\nWinters_triangle <- Winters_daily\r\nWinters_triangle[, \"Hour\"] <- 0\r\nWinters_triangle$Hour[nrow(Winters_triangle)] <- 23\r\nWinters_triangle[, \"Temp\"] <- 0\r\nWinters_triangle <-\r\n  make_all_day_table(Winters_triangle, timestep = \"hour\")\r\ncolnames(Winters_triangle)[ncol(Winters_triangle)] <-\r\n  \"Temp_triangular\"\r\n\r\n# with the following loop, we fill in the daily Tmin and Tmax values for every\r\n# hour of the dataset\r\n\r\nfor (i in 2:nrow(Winters_triangle))\r\n{\r\n  if (is.na(Winters_triangle$Tmin[i]))\r\n    Winters_triangle$Tmin[i] <- Winters_triangle$Tmin[i - 1]\r\n  if (is.na(Winters_triangle$Tmax[i]))\r\n    Winters_triangle$Tmax[i] <- Winters_triangle$Tmax[i - 1]\r\n}\r\nWinters_triangle$Temp_triangular <- NA\r\n\r\n# now we assign the daily Tmin value to the 6th hour of every day\r\n\r\nWinters_triangle$Temp_triangular[which(Winters_triangle$Hour == 6)] <-\r\n  Winters_triangle$Tmin[which(Winters_triangle$Hour == 6)]\r\n\r\n# we also assign the daily Tmax value to the 18th hour of every day\r\n\r\nWinters_triangle$Temp_triangular[which(Winters_triangle$Hour == 18)] <-\r\n  Winters_triangle$Tmax[which(Winters_triangle$Hour == 18)]\r\n\r\n# in the following step, we use the chillR function \"interpolate_gaps\"\r\n# to fill in all the gaps in the hourly record with straight lines\r\n\r\nWinters_triangle$Temp_triangular <-\r\n  interpolate_gaps(Winters_triangle$Temp_triangular)$interp\r\nWinters_triangle <-\r\n  Winters_triangle[, c(\"Year\", \"Month\", \"Day\", \"Hour\", \"Temp_triangular\")]\r\n\r\n\r\nComparison of Temperature Models\r\nThree methods were compared: the triangular model, the idealized model, and the empirical model. The data were merged and visualized:\r\n\r\n\r\nWinters_temps <-\r\n  merge(Winters_hours_gaps,\r\n        Winters_hours,\r\n        by = c(\"Year\", \"Month\", \"Day\", \"Hour\"))\r\nWinters_temps <-\r\n  merge(Winters_temps,\r\n        Winters_triangle,\r\n        by = c(\"Year\", \"Month\", \"Day\", \"Hour\"))\r\nWinters_temps <-\r\n  merge(Winters_temps,\r\n        Winters_ideal,\r\n        by = c(\"Year\", \"Month\", \"Day\", \"Hour\"))\r\n\r\n\r\nAccuracy of the three models was then compared using the Root Mean Square Error (RMSE) metric:\r\n\r\n\r\nRMSEP(Winters_temps$Temp_triangular, Winters_temps$Temp)\r\n\r\n[1] 4.695289\r\n\r\nRMSEP(Winters_temps$Temp_ideal, Winters_temps$Temp)\r\n\r\n[1] 1.630714\r\n\r\nRMSEP(Winters_temps$Temp_empirical, Winters_temps$Temp)\r\n\r\n[1] 1.410625\r\n\r\nResults:\r\nTriangular method: RMSE = 4.7\r\nIdealized model: RMSE = 1.63\r\nEmpirical model: RMSE = 1.41\r\nThe empirical model achieves the highest accuracy. This approach is especially crucial for calculating the Chilling Hours.\r\nExercises on hourly temperatures\r\nChoose a location of interest, find out its latitude and produce plots of daily sunrise, sunset and daylength.\r\nThe Yakima Valley in Washington State, USA, is located at about 46.6° N latitude. This region has a continental climate with cold winters and hot, dry summers, creating ideal conditions for growing fruit trees. The valley is well known for producing a variety of fruits, including apples, cherries, pears, and grapes, which benefit from its distinct seasonal changes. Using the daylength() function, you could create plots showing daily sunrise, sunset, and day length times.\r\n\r\n\r\nYakima <- daylength(latitude = 46.6, JDay = 1:365)\r\n\r\nYakima_df <-\r\n  data.frame(\r\n    JDay = 1:365,\r\n    Sunrise = Yakima$Sunrise,\r\n    Sunset = Yakima$Sunset,\r\n    Daylength = Yakima$Daylength\r\n  )\r\n\r\nYakima_df_longer <- pivot_longer(Yakima_df, cols = c(Sunrise:Daylength))\r\n\r\nggplot(Yakima_df_longer, aes(JDay, value)) +\r\n  geom_line(lwd = 1.5) +\r\n  facet_grid(cols = vars(name)) +\r\n  ylab(\"Time of Day / Daylength (Hours)\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nProduce an hourly dataset, based on idealized daily curves, for the KA_weather dataset (included in chillR)\r\n\r\n\r\nKA_hourly <- stack_hourly_temps(KA_weather, latitude = 50.4)\r\n\r\n\r\nBased on idealized daily curves, the hourly dataset for Julian Day 6 (January 6th) is shown below:\r\n\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nTmax\r\n\r\n\r\nTmin\r\n\r\n\r\nJDay\r\n\r\n\r\nHour\r\n\r\n\r\nTemp\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n0\r\n\r\n\r\n4.990741\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n1\r\n\r\n\r\n4.881232\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n2\r\n\r\n\r\n4.782253\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n3\r\n\r\n\r\n4.691956\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n4\r\n\r\n\r\n4.608939\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n5\r\n\r\n\r\n4.532117\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n4.460628\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n7\r\n\r\n\r\n4.393780\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n8\r\n\r\n\r\n4.491337\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n9\r\n\r\n\r\n5.430950\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n10\r\n\r\n\r\n6.302486\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n11\r\n\r\n\r\n7.048391\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n12\r\n\r\n\r\n7.619410\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n13\r\n\r\n\r\n7.977836\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n14\r\n\r\n\r\n8.100000\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n15\r\n\r\n\r\n7.977836\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n16\r\n\r\n\r\n7.619410\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n17\r\n\r\n\r\n7.419674\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n18\r\n\r\n\r\n7.318918\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n19\r\n\r\n\r\n7.248287\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n20\r\n\r\n\r\n7.193854\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n21\r\n\r\n\r\n7.149557\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n22\r\n\r\n\r\n7.112208\r\n\r\n\r\n1998\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n8.1\r\n\r\n\r\n4.4\r\n\r\n\r\n6\r\n\r\n\r\n23\r\n\r\n\r\n7.079920\r\n\r\n\r\n\r\nProduce empirical temperature curve parameters for the Winters_hours_gaps dataset, and use them to predict hourly values from daily temperatures (this is very similar to the example above, but please make sure you understand what’s going on).\r\n\r\n\r\n# Generating empirical daily temperature curve from observed hourly data\r\nempi_curve <- Empirical_daily_temperature_curve(Winters_hours_gaps)\r\n\r\n# Filling gaps in daily or hourly temperature data\r\nWinters_daily <- make_all_day_table(Winters_hours_gaps, input_timestep = \"hour\")\r\n\r\n# Using empirical coefficients to predict hourly temperatures based on daily temperatures\r\nWinters_hours <- Empirical_hourly_temperatures(Winters_daily, empi_curve)\r\n\r\n# Make an empirical dataset \r\nWinters_hours <- Winters_hours[, c(\"Year\", \"Month\", \"Day\", \"Hour\", \"Temp\")]\r\ncolnames(Winters_hours)[ncol(Winters_hours)] <- \"Temp_empirical\"\r\n\r\n# Merge data frames\r\nWinters_temps <-\r\n  merge(Winters_hours_gaps,\r\n        Winters_hours,\r\n        by = c(\"Year\", \"Month\", \"Day\", \"Hour\"))\r\n\r\n\r\n\r\n\r\n# Covert Year, Month, Day and Hour columns into R's date formate and reorganizing the data frame\r\nWinters_temps[, \"DATE\"] <-\r\n  ISOdate(Winters_temps$Year,\r\n          Winters_temps$Month,\r\n          Winters_temps$Day,\r\n          Winters_temps$Hour)\r\n\r\nWinters_temps_to_plot <-\r\n  Winters_temps[, c(\"DATE\",\r\n                    \"Temp\",\r\n                    \"Temp_empirical\")]\r\nWinters_temps_to_plot <- Winters_temps_to_plot[100:200, ]\r\nWinters_temps_to_plot <- pivot_longer(Winters_temps_to_plot, cols=Temp:Temp_empirical)\r\ncolnames(Winters_temps_to_plot) <- c(\"DATE\", \"Method\", \"Temperature\")\r\n\r\nggplot(data = Winters_temps_to_plot, aes(DATE, Temperature, colour = Method)) +\r\n  geom_line(lwd = 1.3) + ylab(\"Temperature (°C)\") + xlab(\"Date\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:40:05+01:00"
    },
    {
      "path": "manual_chill.html",
      "title": "Manual chill",
      "author": [],
      "contents": "\r\nThis chapter explains how to calculate Chilling Hours using R and the chillR package. Chilling Hours measure the number of hours where temperatures are between 0°C and 7.2°C, which is important for certain plants to meet their cold requirements during dormancy and grow properly.\r\nData Requirements\r\nThe calculation requires hourly temperature data, which is not always available. The chillR package provides tools to approximate data from daily records. In this example, the dataset Winters_hours_gaps is used, containing hourly temperature data recorded in 2008 from a walnut orchard in Winters, California.\r\nData Preparation\r\nThe chillR package is loaded using library(chillR). The relevant columns (year, month, day, hour, temperature) are extracted and stored in a new dataset named hourtemps, ensuring the correct format for calculating Chilling Hours.\r\n\r\n\r\nhourtemps <- Winters_hours_gaps[,c(\"Year\",\r\n                                   \"Month\",\r\n                                   \"Day\",\r\n                                   \"Hour\",\r\n                                   \"Temp\")]\r\n\r\n\r\nManual Calculation of Chilling Hours\r\nChilling Hours are defined as any hour where the temperature falls between 0°C and 7.2°C. In R, this is implemented using a logical condition:\r\n\r\n\r\nhourtemps[, \"Chilling_Hour\"] <- hourtemps$Temp >= 0 & hourtemps$Temp <= 7.2\r\n\r\n\r\nA new column Chilling_Hour (TRUE/FALSE) is created to indicate whether a given hour qualifies. The total number of Chilling Hours can then be calculated using sum(hourtemps$Chilling_Hour).\r\nAutomation with Functions\r\nTo simplify the process, a function CH() was created to automatically add the Chilling_Hour column:\r\n\r\n\r\nCH <- function(hourtemps) {\r\n  hourtemps[, \"Chilling_Hour\"] <- hourtemps$Temp >= 0 & hourtemps$Temp <= 7.2\r\n  return(hourtemps)\r\n}\r\n\r\n\r\nAdditionally, a function sum_CH() was developed to calculate the total number of Chilling Hours between two specific dates:\r\n\r\n\r\nsum_CH <- function(hourtemps, Start_Year, Start_Month, Start_Day, Start_Hour, \r\n                              End_Year, End_Month, End_Day, End_Hour) {\r\n  hourtemps[,\"Chilling_Hour\"] <- hourtemps$Temp >= 0 & hourtemps$Temp <= 7.2\r\n\r\n  Start_Index <- which(hourtemps$Year == Start_Year & hourtemps$Month == Start_Month &\r\n                       hourtemps$Day == Start_Day & hourtemps$Hour == Start_Hour)\r\n  End_Index <- which(hourtemps$Year == End_Year & hourtemps$Month == End_Month &\r\n                     hourtemps$Day == End_Day & hourtemps$Hour == End_Hour)\r\n\r\n  CHs <- sum(hourtemps$Chilling_Hour[Start_Index:End_Index])\r\n  return(CHs)\r\n}\r\n\r\n\r\nThis function uses the which() function to identify the relevant rows in the dataset based on the selected time range.\r\nOptimization of Function Parameters\r\nInstead of passing year, month, day, and hour separately, compact strings in the format YEARMODAHO (e.g., 2008040100 for April 1, 2008, at 00:00) can be used. The function extracts values using substr() and converts them into numeric values.\r\n\r\n\r\nsum_CH <- function(hourtemps, startYEARMODAHO, endYEARMODAHO) {\r\n  hourtemps[, \"Chilling_Hour\"] <- hourtemps$Temp >= 0 & hourtemps$Temp <= 7.2\r\n\r\n  startYear <- as.numeric(substr(startYEARMODAHO, 1, 4))\r\n  startMonth <- as.numeric(substr(startYEARMODAHO, 5, 6))\r\n  startDay <- as.numeric(substr(startYEARMODAHO, 7, 8))\r\n  startHour <- as.numeric(substr(startYEARMODAHO, 9, 10))\r\n\r\n  endYear <- as.numeric(substr(endYEARMODAHO, 1, 4))\r\n  endMonth <- as.numeric(substr(endYEARMODAHO, 5, 6))\r\n  endDay <- as.numeric(substr(endYEARMODAHO, 7, 8))\r\n  endHour <- as.numeric(substr(endYEARMODAHO, 9, 10))\r\n\r\n  Start_Index <- which(hourtemps$Year == startYear & hourtemps$Month == startMonth &\r\n                       hourtemps$Day == startDay & hourtemps$Hour == startHour)\r\n  End_Index <- which(hourtemps$Year == endYear & hourtemps$Month == endMonth &\r\n                     hourtemps$Day == endDay & hourtemps$Hour == endHour)\r\n\r\n  CHs <- sum(hourtemps$Chilling_Hour[Start_Index:End_Index])\r\n  return(CHs)\r\n}\r\n\r\n\r\nApplication Example\r\nUsing the function sum_CH(), it was calculated that between April 1st and October 11th, 2008, the walnut orchard experienced 77 Chilling Hours:\r\n\r\n\r\nsum_CH(hourtemps, startYEARMODAHO = 2008040100, endYEARMODAHO = 2008101100)\r\n\r\n[1] 77\r\n\r\nExercises on basic chill modeling\r\nWrite a basic function that calculates warm hours (>25°C).\r\n\r\n\r\nWH <- function(data)\r\n  {data[, \"Warm_Hour\"] <- data$Temp > 25\r\n  return(data)\r\n}\r\n\r\n\r\nApply this function to the Winters_hours_gaps dataset.\r\n\r\n\r\nWH(Winters_hours_gaps)\r\n\r\n\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nHour\r\n\r\n\r\nTemp_gaps\r\n\r\n\r\nTemp\r\n\r\n\r\nWarm_Hour\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n10\r\n\r\n\r\n15.127\r\n\r\n\r\n15.127\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n11\r\n\r\n\r\n17.153\r\n\r\n\r\n17.153\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n12\r\n\r\n\r\n18.699\r\n\r\n\r\n18.699\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n13\r\n\r\n\r\n18.699\r\n\r\n\r\n18.699\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n14\r\n\r\n\r\n18.842\r\n\r\n\r\n18.842\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n15\r\n\r\n\r\n19.508\r\n\r\n\r\n19.508\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n16\r\n\r\n\r\n19.318\r\n\r\n\r\n19.318\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n17\r\n\r\n\r\n17.701\r\n\r\n\r\n17.701\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n18\r\n\r\n\r\n15.414\r\n\r\n\r\n15.414\r\n\r\n\r\nFALSE\r\n\r\n\r\n2008\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n19\r\n\r\n\r\n12.727\r\n\r\n\r\n12.727\r\n\r\n\r\nFALSE\r\n\r\n\r\nExtend this function, so that it can take start and end dates as inputs and sums up warm hours between these dates.\r\n\r\n\r\nsum_WH <- function(data, \r\n                   startYEARMODAHO,\r\n                   endYEARMODAHO)\r\n  \r\n{data[,\"Warm_Hour\"] <- data$Temp > 25\r\n\r\nstartYear <- as.numeric(substr(startYEARMODAHO, 1, 4))\r\nstartMonth <- as.numeric(substr(startYEARMODAHO, 5, 6))\r\nstartDay <- as.numeric(substr(startYEARMODAHO, 7, 8))\r\nstartHour <- as.numeric(substr(startYEARMODAHO, 9, 10))\r\n\r\nendYear <- as.numeric(substr(endYEARMODAHO, 1, 4))\r\nendMonth <- as.numeric(substr(endYEARMODAHO, 5, 6))\r\nendDay <- as.numeric(substr(endYEARMODAHO, 7, 8))\r\nendHour <- as.numeric(substr(endYEARMODAHO, 9, 10))\r\n\r\n\r\nStart_Date <- which(data$Year == startYear &\r\n                    data$Month == startMonth &\r\n                    data$Day == startDay &\r\n                    data$Hour == startHour)\r\n\r\nEnd_Date <- which(data$Year == endYear &\r\n                  data$Month == endMonth &\r\n                  data$Day == endDay &\r\n                  data$Hour == endHour)\r\n\r\nWHs <- sum(data$Warm_Hour[Start_Date:End_Date])\r\nreturn(WHs)\r\n}\r\n\r\n\r\nApplication Example:\r\n\r\n\r\nsum_WH(Winters_hours_gaps, startYEARMODAHO = 2008080100, \r\n                           endYEARMODAHO = 2008083100)\r\n\r\n[1] 283\r\n\r\nDuring the month of August 2008, from the 1st to the 31st, the walnut orchard experienced a total of 283 warm hours (defined as hours when the temperature exceeded 25°C).\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:40:21+01:00"
    },
    {
      "path": "outputs.html",
      "title": "Evaluating PLS outputs",
      "author": [],
      "contents": "\r\nChilling and forcing requirements\r\nIdentifying dormancy phases enables a more precise analysis of phenological responses. Using the pear variety ‘Alexander Lucas’, PLS analysis is applied to examine the effects of temperature during dormancy.\r\n\r\n\r\nAlex_first <- read_tab(\"data/Alexander_Lucas_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, \r\n         JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\ntemps <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\ntemps_hourly <- temps %>%\r\n  stack_hourly_temps(latitude = 50.6)\r\n\r\ndaychill <- daily_chill(hourtemps = temps_hourly,\r\n                        running_mean = 1,\r\n                        models = list(Chilling_Hours = Chilling_Hours,\r\n                                      Utah_Chill_Units = Utah_Model,\r\n                                      Chill_Portions = Dynamic_Model,\r\n                                      GDH = GDH)\r\n                        )\r\n\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = daychill,\r\n                         bio_data_frame = Alex_first,\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\",\r\n                         runn_means = 11)\r\n\r\n\r\nThe results are visualized using plot_PLS_chill_force to illustrate temperature effects during dormancy.\r\n\r\n\r\n\r\n\r\n\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Chill_Portions\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CP\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(-48, 62),\r\n                     heat_phase = c(3, 105.5))\r\n\r\n\r\n\r\nThe identified phases for chilling and forcing are highlighted in color. To determine climatic requirements, accumulated values are calculated.\r\n\r\n\r\nchill_phase <- c(317, 62)\r\nheat_phase <- c(3, 105.5)\r\n\r\nchill <- tempResponse(hourtemps = temps_hourly,\r\n                      Start_JDay = chill_phase[1],\r\n                      End_JDay = chill_phase[2],\r\n                      models = list(Chill_Portions = Dynamic_Model),\r\n                      misstolerance = 10)\r\n\r\nheat <- tempResponse(hourtemps = temps_hourly,\r\n                     Start_JDay = heat_phase[1],\r\n                     End_JDay = heat_phase[2],\r\n                     models = list(GDH = GDH))\r\n\r\n\r\nHistograms show the distribution of chill and heat accumulation:\r\n\r\n\r\nggplot(data = chill,\r\n       aes(x = Chill_Portions)) +\r\n  geom_histogram() +\r\n  ggtitle(\"Chill accumulation during endodormancy (Chill Portions)\") +\r\n  xlab(\"Chill accumulation (Chill Portions)\") +\r\n  ylab(\"Frequency between 1958 and 2019\") +\r\n  theme_bw(base_size = 12)\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = heat,\r\n       aes(x = GDH)) +\r\n  geom_histogram() +\r\n  ggtitle(\"Heat accumulation during ecodormancy (GDH)\") +\r\n  xlab(\"Heat accumulation (Growing Degree Hours)\") +\r\n  ylab(\"Frequency between 1958 and 2019\") +\r\n  theme_bw(base_size = 12)\r\n\r\n\r\n\r\nThe average requirements are calculated:\r\n\r\n\r\nchill_requirement <- mean(chill$Chill_Portions)\r\nchill_req_error <- sd(chill$Chill_Portions)\r\n\r\nheat_requirement <- mean(heat$GDH)\r\nheat_req_error <- sd(heat$GDH)\r\n\r\n\r\nThe chilling requirement (chilling_requirement) was estimated at approximately 72.3 Chill Portions, with an error margin (chill_req_error) of 7.7 CP, while the heat requirement (heat_requirement) was around 3415 Growing Degree Hours, with an estimated error (heat_req_error) of 1402 GDH. Compared to other fruit trees, this represents a relatively high chilling requirement but a low heat requirement.\r\nThis method provides an initial estimate of climatic requirements based on long-term bloom data. While uncertainties remain regarding phase duration, this approach has proven useful for preliminary analyses.\r\nAnalysis of Tree Responses to Temperature\r\nIn addition to estimating chill and heat requirements, this analysis focuses on how trees respond to temperature during specific phases. While agroclimatic models provide reasonable approximations, raw temperature data will be reconsidered. The goal is to determine whether the mean temperatures during these periods explain the variation in bloom dates.\r\nSince both chilling and forcing conditions influence bloom timing, their combined impact is visualized in a three-dimensional plot. The chillR package offers a function for this, but the process will also be manually replicated.\r\n\r\n\r\nchill_phase <- c(317, 62)\r\nheat_phase <- c(360, 106)\r\n\r\nmpt <- make_pheno_trend_plot(weather_data_frame = temps,\r\n                             pheno = Alex_first,\r\n                             Start_JDay_chill = chill_phase[1], \r\n                             End_JDay_chill = chill_phase[2],\r\n                             Start_JDay_heat = heat_phase[1],\r\n                             End_JDay_heat = heat_phase[2],\r\n                             outpath = \"data/\",\r\n                             file_name = \"pheno_trend_plot\",\r\n                             plot_title = \"Impacts of chilling and forcing temperatures on pear phenology\",\r\n                             image_type = \"png\", \r\n                             colorscheme = \"normal\")\r\n\r\n\r\nStandard chillR output of phenology trend plot for pears ‘Alexander Lucas’ in Klein-AltendorfThe resulting plot links bloom timing to temperatures during chilling (x-axis) and forcing (y-axis) phases. Observations since 1958 reveal a strong correlation between these phases, forming a diagonal distribution. Warmer forcing temperatures lead to earlier blooms (blue), while cooler temperatures delay them (red). However, due to data gaps, assessing chilling effects is more complex.\r\nRecreating the Plot with ggplot2\r\nThe first step involves calculating the mean temperatures for the chilling and forcing phases:\r\n\r\n\r\nmean_temp_period <- function(\r\n    temps,\r\n    start_JDay,\r\n    end_JDay,\r\n    end_season = end_JDay)\r\n{ temps_JDay <- make_JDay(temps) %>%\r\n  mutate(Season =Year)\r\n  \r\n  if(start_JDay > end_season)\r\n    temps_JDay$Season[which(temps_JDay$JDay >= start_JDay)]<-\r\n        temps_JDay$Year[which(temps_JDay$JDay >= start_JDay)]+1\r\n  \r\n  if(start_JDay > end_season)\r\n    sub_temps <- subset(temps_JDay,\r\n                        JDay <= end_JDay | JDay >= start_JDay)\r\n  \r\n  if(start_JDay <= end_JDay)\r\n    sub_temps <- subset(temps_JDay,\r\n                        JDay <= end_JDay & JDay >= start_JDay)\r\n  \r\n  mean_temps <- aggregate(sub_temps[, c(\"Tmin\", \"Tmax\")],\r\n                          by = list(sub_temps$Season),\r\n                          FUN = function(x) mean(x,\r\n                                                 na.rm=TRUE))\r\n  \r\n  mean_temps[, \"n_days\"] <- aggregate(sub_temps[, \"Tmin\"],\r\n                                      by = list(sub_temps$Season),\r\n                                      FUN = length)[,2]\r\n  \r\n  mean_temps[, \"Tmean\"] <- (mean_temps$Tmin + mean_temps$Tmax) / 2\r\n  mean_temps <- mean_temps[, c(1, 4, 2, 3, 5)]\r\n  colnames(mean_temps)[1] <- \"End_year\"\r\n  \r\n  return(mean_temps)\r\n}\r\n\r\nmean_temp_chill <- mean_temp_period(temps = temps,\r\n                                    start_JDay = chill_phase[1],\r\n                                    end_JDay = chill_phase[2],\r\n                                    end_season = 60)\r\n\r\nmean_temp_heat <- mean_temp_period(temps = temps,\r\n                                   start_JDay = heat_phase[1],\r\n                                   end_JDay = heat_phase[2],\r\n                                   end_season = 60)\r\n\r\n\r\nThe end_season parameter ensures proper seasonal assignment, preventing errors when the chilling phase spans two years. Next, the datasets are merged:\r\n\r\n\r\nmean_temp_chill <- \r\n  mean_temp_chill[which(mean_temp_chill$n_days >=\r\n                          max(mean_temp_chill$n_days)-1),]\r\nmean_temp_heat <-\r\n  mean_temp_heat[which(mean_temp_heat$n_days >=\r\n                         max(mean_temp_heat$n_days)-1),]\r\n\r\nmean_chill <- mean_temp_chill[, c(\"End_year\",\r\n                                  \"Tmean\")]\r\ncolnames(mean_chill)[2] <- \"Tmean_chill\"\r\n\r\nmean_heat <- mean_temp_heat[,c(\"End_year\",\r\n                               \"Tmean\")]\r\ncolnames(mean_heat)[2] <- \"Tmean_heat\"\r\n\r\nphase_Tmeans <- merge(mean_chill,\r\n                      mean_heat, \r\n                      by = \"End_year\")\r\n\r\n\r\npheno <- Alex_first\r\ncolnames(pheno)[1] <- \"End_year\"\r\n\r\nTmeans_pheno <- merge(phase_Tmeans,\r\n                      pheno,\r\n                      by = \"End_year\")\r\n\r\n\r\nTo interpolate the data, Kriging from the fields package is used:\r\n\r\n\r\nlibrary(fields)\r\nk <- Krig(x = as.matrix(\r\n                Tmeans_pheno[,\r\n                             c(\"Tmean_chill\",\r\n                               \"Tmean_heat\")]),\r\n          Y = Tmeans_pheno$pheno)\r\n\r\npred <- predictSurface(k)\r\ncolnames(pred$z) <- pred$y\r\nrownames(pred$z) <- pred$x\r\n\r\nlibrary(reshape2)\r\nmelted <- melt(pred$z)\r\n  \r\nlibrary(metR)\r\nlibrary(colorRamps)\r\n  \r\ncolnames(melted) <- c(\"Tmean_chill\",\r\n                      \"Tmean_heat\",\r\n                      \"value\")\r\n\r\n\r\nggplot(melted,\r\n       aes(x = Tmean_chill,\r\n           y = Tmean_heat,\r\n           z = value)) +\r\n  geom_contour_fill(bins = 100) +\r\n  scale_fill_gradientn(colours = alpha(matlab.like(15)),\r\n                       name = \"Bloom date \\n(day of the year)\") +\r\n  geom_contour(col = \"black\")  +\r\n  geom_point(data = Tmeans_pheno,\r\n             aes(x = Tmean_chill,\r\n                 y = Tmean_heat,\r\n                 z = NULL),\r\n             size = 0.7) +\r\n  geom_text_contour(stroke = 0.2) +\r\n  ylab(expression(paste(\"Forcing phase \", \r\n                        T[mean],\r\n                        \" (\",\r\n                        degree,\r\n                        \"C)\"))) +\r\n  xlab(expression(paste(\"Chilling phase \",\r\n                        T[mean],\r\n                        \" (\",\r\n                        degree,\r\n                        \"C)\")))  +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nAutomating with a Function\r\nTo streamline future analyses, the entire process can be encapsulated in a function:\r\n\r\n\r\npheno_trend_ggplot <- function(temps,\r\n                               pheno,\r\n                               chill_phase,\r\n                               heat_phase,\r\n                               phenology_stage = \"Bloom\")\r\n{\r\n  library(fields)\r\n  library(reshape2)\r\n  library(metR)\r\n  library(ggplot2)\r\n  library(colorRamps)\r\n  \r\n  # first, a sub-function (function defined within a function) to\r\n  # compute the temperature means\r\n  \r\n  mean_temp_period <- function(temps,\r\n                               start_JDay,\r\n                               end_JDay, \r\n                               end_season = end_JDay)\r\n    { temps_JDay <- make_JDay(temps) %>%\r\n      mutate(Season = Year)\r\n\r\n    if(start_JDay > end_season)\r\n      temps_JDay$Season[which(temps_JDay$JDay >= start_JDay)] <-\r\n        temps_JDay$Year[which(temps_JDay$JDay >= start_JDay)]+1\r\n    \r\n    if(start_JDay > end_season)\r\n      sub_temps <- subset(temps_JDay,\r\n                          JDay <= end_JDay | JDay >= start_JDay)\r\n    \r\n    if(start_JDay <= end_JDay)\r\n      sub_temps <- subset(temps_JDay,\r\n                          JDay <= end_JDay & JDay >= start_JDay)\r\n    \r\n    mean_temps <- aggregate(sub_temps[,\r\n                                      c(\"Tmin\",\r\n                                        \"Tmax\")],\r\n                            by = list(sub_temps$Season),\r\n                            FUN = function(x) mean(x,\r\n                                                   na.rm = TRUE))\r\n    mean_temps[, \"n_days\"] <- aggregate(sub_temps[,\r\n                                                  \"Tmin\"],\r\n                                        by = list(sub_temps$Season),\r\n                                        FUN = length)[,2]\r\n    \r\n    mean_temps[,\"Tmean\"] <- (mean_temps$Tmin + mean_temps$Tmax) / 2\r\n    mean_temps <- mean_temps[, c(1, 4, 2, 3, 5)]\r\n    colnames(mean_temps)[1] <- \"End_year\"\r\n    return(mean_temps)\r\n    }\r\n  \r\n  mean_temp_chill <- mean_temp_period(temps = temps,\r\n                                      start_JDay = chill_phase[1],\r\n                                      end_JDay = chill_phase[2],\r\n                                      end_season = heat_phase[2])\r\n  \r\n  mean_temp_heat <- mean_temp_period(temps = temps,\r\n                                     start_JDay = heat_phase[1],\r\n                                     end_JDay = heat_phase[2],\r\n                                     end_season = heat_phase[2])\r\n  \r\n  mean_temp_chill <-\r\n    mean_temp_chill[which(mean_temp_chill$n_days >= \r\n                            max(mean_temp_chill$n_days)-1),]\r\n  mean_temp_heat <-\r\n    mean_temp_heat[which(mean_temp_heat$n_days >= \r\n                           max(mean_temp_heat$n_days)-1),]\r\n  mean_chill <- mean_temp_chill[, c(\"End_year\",\r\n                                    \"Tmean\")]\r\n  colnames(mean_chill)[2] <- \"Tmean_chill\"\r\n  mean_heat<-mean_temp_heat[,c(\"End_year\",\r\n                               \"Tmean\")]\r\n  colnames(mean_heat)[2] <- \"Tmean_heat\"\r\n  phase_Tmeans <- merge(mean_chill,\r\n                        mean_heat,\r\n                        by = \"End_year\")\r\n  \r\n  colnames(pheno) <- c(\"End_year\",\r\n                       \"pheno\")\r\n  Tmeans_pheno <- merge(phase_Tmeans,\r\n                        pheno, \r\n                        by=\"End_year\")\r\n  \r\n  # Kriging interpolation\r\n  k <- Krig(x = as.matrix(Tmeans_pheno[,c(\"Tmean_chill\",\r\n                                          \"Tmean_heat\")]),\r\n            Y = Tmeans_pheno$pheno)\r\n  pred <- predictSurface(k)\r\n  colnames(pred$z) <- pred$y\r\n  rownames(pred$z) <- pred$x\r\n  melted <- melt(pred$z)\r\n  colnames(melted) <- c(\"Tmean_chill\",\r\n                        \"Tmean_heat\",\r\n                        \"value\")\r\n  \r\n  ggplot(melted,\r\n         aes(x = Tmean_chill,\r\n             y = Tmean_heat,\r\n             z = value)) +\r\n    geom_contour_fill(bins = 60) +\r\n    scale_fill_gradientn(colours = alpha(matlab.like(15)),\r\n                         name = paste(phenology_stage,\r\n                                      \"date \\n(day of the year)\")) +\r\n    geom_contour(col = \"black\") +\r\n    geom_text_contour(stroke = 0.2) +\r\n    geom_point(data = Tmeans_pheno,\r\n               aes(x = Tmean_chill,\r\n                   y = Tmean_heat,\r\n                   z = NULL),\r\n               size = 0.7)  +\r\n    ylab(expression(paste(\"Forcing phase \",\r\n                          T[mean],\r\n                          \" (\",\r\n                          degree,\r\n                          \"C)\"))) +\r\n    xlab(expression(paste(\"Chilling phase \",\r\n                          T[mean],\r\n                          \" (\",\r\n                          degree,\r\n                          \"C)\"))) +\r\n    theme_bw(base_size = 15)\r\n}\r\n\r\n\r\nchill_phase <- c(317, 62)\r\nheat_phase <- c(360, 105.5)\r\n\r\npheno_trend_ggplot(temps = temps,\r\n                   pheno = Alex_first,\r\n                   chill_phase = chill_phase,\r\n                   heat_phase = heat_phase,\r\n                   phenology_stage = \"Bloom\")\r\n\r\n\r\n\r\nBy automating the process, generating high-quality plots becomes efficient and consistent across datasets.\r\nApplying our functions to California walnuts\r\nThe developed functions can now be used to analyze walnuts in California, using a dataset with leaf emergence observations for the Payne walnut cultivar in Davis.\r\n\r\n\r\nCali_temps <- read_tab(\"data/Davis_weather.csv\")\r\nWalnut_pheno <- read_tab(\"data/Davis_Payne_leaf_out.csv\") %>%\r\n  mutate(Year = as.numeric(substr(Leaf.date,7,8)),\r\n         Year = Year+(19+(Year<25))*100,\r\n         Month = as.numeric(substr(Leaf.date,4,5)),\r\n         Day = as.numeric(substr(Leaf.date,1,2))) %>%\r\n  make_JDay() %>%\r\n  select(Year, JDay)\r\n\r\ncolnames(Walnut_pheno) <- c(\"Year\",\r\n                            \"pheno\")\r\n\r\nCali_temps_hourly <- stack_hourly_temps(Cali_temps,\r\n                                        latitude = 38.5)\r\n\r\nCali_daychill <- daily_chill(hourtemps = Cali_temps_hourly,\r\n                             running_mean = 1,\r\n                             models = list(Chilling_Hours = Chilling_Hours,\r\n                                           Utah_Chill_Units = Utah_Model,\r\n                                           Chill_Portions = Dynamic_Model,\r\n                                           GDH = GDH)\r\n    )\r\n\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = Cali_daychill,\r\n                         bio_data_frame = Walnut_pheno,\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\",\r\n                         runn_means = 11)\r\n\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Chill_Portions\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CP\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(-56, 5),\r\n                     heat_phase = c(19, 77))\r\n\r\n\r\n\r\n\r\n\r\npheno_trend_ggplot(temps = Cali_temps,\r\n                   pheno = Walnut_pheno,\r\n                   chill_phase = c(309, 5),\r\n                   heat_phase = c(19, 77),\r\n                   phenology_stage = \"Leaf emergence\")\r\n\r\n\r\n\r\nThe color pattern in this analysis reveals clearer insights. Early leaf emergence occurred with a cool chilling phase and a warm forcing phase, indicated by the blue in the top left corner. Conversely, the bottom right corner, marked in red, shows that a warm chilling phase with a cool forcing phase led to later leaf emergence.\r\nExercises on evaluating PLS regression results\r\nReproduce the analysis for the ‘Roter Boskoop’ dataset.\r\n\r\n\r\n# Load weather data for Klein-Altendorf and Roter Boskoop dataset\r\nKA_temps <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\nApple_pheno <- read_tab(\"data/Roter_Boskoop_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, \r\n         JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\ncolnames(Apple_pheno) <- c(\"Year\",\r\n                            \"pheno\")\r\n\r\nKA_temps_hourly <- stack_hourly_temps(KA_temps,\r\n                                        latitude = 50.6)\r\n\r\nKA_daychill <- daily_chill(hourtemps = KA_temps_hourly,\r\n                             running_mean = 1,\r\n                             models = list(Chilling_Hours = Chilling_Hours,\r\n                                           Utah_Chill_Units = Utah_Model,\r\n                                           Chill_Portions = Dynamic_Model,\r\n                                           GDH = GDH)\r\n    )\r\n\r\nplscf <- PLS_chill_force(daily_chill_obj = KA_daychill,\r\n                         bio_data_frame = Apple_pheno,\r\n                         split_month = 6,\r\n                         chill_models = \"Chill_Portions\",\r\n                         heat_models = \"GDH\",\r\n                         runn_means = 11)\r\n\r\n# Plot PLS analysis of 'Roter Boskoop' apples in Klein-Altendorf\r\nplot_PLS_chill_force(plscf,\r\n                     chill_metric = \"Chill_Portions\",\r\n                     heat_metric = \"GDH\",\r\n                     chill_label = \"CP\",\r\n                     heat_label = \"GDH\",\r\n                     chill_phase = c(-48, 62),\r\n                     heat_phase = c(3, 105.5))\r\n\r\n\r\n\r\n\r\n\r\n# Plot phenology trend plot for apples 'Roter Boskoop' in Klein-Altendorf\r\npheno_trend_ggplot(temps = KA_temps,\r\n                   pheno = Apple_pheno,\r\n                   chill_phase = c(317, 62),\r\n                   heat_phase = c(360, 106),\r\n                   phenology_stage = \"Bloom\")\r\n\r\nWarning: \r\nGrid searches over lambda (nugget and sill variances) with  minima at the endpoints: \r\n  (REML) Restricted maximum likelihood \r\n   minimum at  right endpoint  lambda  =  39821.23 (eff. df= 3.001015 \r\n)\r\n\r\n\r\nWe’ve looked at data from a number of locations so far. How would you expect this surface plot to look like in Beijing? And how should it look in Tunisia?\r\nThe surface plot for Beijing would likely show a broader range of chilling and forcing conditions due to its continental climate, characterized by cold winters and hot summers. The plot might have a more pronounced contrast, with early leaf emergence occurring under cold chilling conditions followed by warm forcing temperatures, while late emergence would be associated with mild winters and cooler spring temperatures. Given Beijing’s significant seasonal variation, the dataset may cover a wider range of temperature combinations.\r\nIn Tunisia, which has a Mediterranean climate with mild winters and warm springs, the chilling phase would generally be warmer, potentially limiting strong chilling accumulation. The surface plot might show a more compressed distribution, with fewer extreme chilling conditions. Early emergence would likely still correspond to relatively cooler chilling conditions and warm forcing temperatures, but the overall variation in the dataset might be narrower compared to Beijing. Additionally, insufficient chilling accumulation in some years could lead to irregular or delayed leaf emergence, which might result in a less smooth pattern in the plot.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:43:24+01:00"
    },
    {
      "path": "phenoflex1.html",
      "title": "The PhenoFlex Model ",
      "author": [],
      "contents": "\r\nImprovements in Phenology Models\r\nExisting phenology models are mostly based on simple concepts of chill and heat accumulation. The Dynamic Model was developed for peaches in Israel, while the Growing Degree Hours Model originated in Utah, often without adaptation to other tree species. One crucial aspect is missing: the ability of trees to compensate for insufficient chill with additional heat.\r\nThe PhenoFlex Framework\r\nPhenoFlex (Luedeling et al., 2021) was developed through interdisciplinary collaboration, inspired by the Dynamic Model’s origins. It integrates the Dynamic Model for chill accumulation with the Growing Degree Hours concept for heat. A sigmoidal function governs the dormancy transition, ensuring heat accumulation begins only after a critical chill threshold. Originally in R, its core code is now in C++ for better performance.\r\nRunning PhenoFlex\r\nThe chillR package now includes the PhenoFlex function, which can be run with default parameters. Before making predictions, suitable parameters should be identified. Below is a demonstration using an hourly temperature dataset from Klein-Altendorf.\r\n\r\n\r\nlibrary(chillR)\r\nlibrary(tidyverse)\r\nCKA_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\nhourtemps <- stack_hourly_temps(CKA_weather, \r\n                                latitude = 50.6)\r\n\r\n\r\nA critical chilling requirement (yc) and a heat requirement (zc) are set, while all other parameters remain at their default values. The dataset is filtered for the 2009 season.\r\n\r\n\r\nyc <- 40\r\nzc <- 190\r\niSeason <- genSeason(hourtemps,\r\n                     mrange = c(8, 6),\r\n                     years = c(2009))\r\n\r\nseason_data <- hourtemps$hourtemps[iSeason[[1]],]\r\n\r\nres <- PhenoFlex(temp = season_data$Temp,\r\n                 times = c(1: length(season_data$Temp)),\r\n                 zc = zc,\r\n                 stopatzc = TRUE,\r\n                 yc = yc,\r\n                 basic_output = FALSE)\r\n\r\n\r\nThe res object contains key variables describing dormancy:\r\nx: precursor to the dormancy-breaking factor\r\ny: dormancy-breaking factor (Chill Portion)\r\nz: heat accumulation (Growing Degree Hours model)\r\nxs: ratio of formation to destruction rate of x\r\nThe predicted bloom date is stored in res$bloomindex and can be converted to an actual date. The following plots show the development of chill accumulation (y) and heat accumulation (z) over time. Vertical lines indicate when the critical chilling requirement (blue) and heat requirement (red) are met.\r\n\r\n\r\nDBreakDay <- res$bloomindex\r\nseasontemps <- hourtemps$hourtemps[iSeason[[1]],]\r\nseasontemps[,\"x\"] <- res$x\r\nseasontemps[,\"y\"] <- res$y\r\nseasontemps[,\"z\"] <- res$z\r\nseasontemps <- add_date(seasontemps)\r\n\r\nCR_full <- seasontemps$Date[which(seasontemps$y >= yc)[1]]\r\nBloom <- seasontemps$Date[which(seasontemps$z >= zc)[1]]\r\n\r\nchillplot <- ggplot(data = seasontemps[1:DBreakDay,],\r\n                    aes(x = Date,\r\n                        y = y)) +\r\n  geom_line(col = \"blue\",\r\n            lwd = 1.5) +\r\n  theme_bw(base_size = 20) +\r\n  geom_hline(yintercept = yc,\r\n             lty = 2,\r\n             col = \"blue\",\r\n             lwd = 1.2) +\r\n  geom_vline(xintercept = CR_full,\r\n             lty = 3,\r\n             col = \"blue\",\r\n             lwd = 1.2) +\r\n  ylab(\"Chill accumulation (y)\") +\r\n  labs(title = \"Chilling\") +\r\n  annotate(\"text\",\r\n           label = \"Chill req. (yc)\", \r\n           x = ISOdate(2008,10,01),\r\n           y = yc*1.1,\r\n           col = \"blue\",\r\n           size = 5)\r\n\r\nheatplot <- ggplot(data = seasontemps[1:DBreakDay,],\r\n                   aes(x = Date,\r\n                       y = z)) +\r\n  geom_line(col = \"red\",\r\n            lwd = 1.5) +\r\n  theme_bw(base_size = 20) +\r\n  scale_y_continuous(position = \"right\") +\r\n  geom_hline(yintercept = zc,\r\n             lty = 2,\r\n             col = \"red\",\r\n             lwd = 1.2) +\r\n  geom_vline(xintercept = CR_full,\r\n             lty = 3,\r\n             col = \"blue\",\r\n             lwd = 1.2) +\r\n  geom_vline(xintercept = Bloom,\r\n             lty = 3,\r\n             col = \"red\",\r\n             lwd = 1.2) +\r\n  ylab(\"Heat accumulation (z)\") +\r\n  labs(title = \"Forcing\") +\r\n  annotate(\"text\",\r\n           label = \"Heat req. (zc)\", \r\n           x = ISOdate(2008,10,01),\r\n           y = zc*0.95,\r\n           col = \"red\",\r\n           size = 5)\r\n\r\n\r\nlibrary(patchwork)\r\nchillplot + heatplot\r\n\r\n\r\n\r\nRunning PhenoFlex for Multiple Years\r\nA loop can be used to predict bloom dates for multiple years (1959–2019), outputting only the bloom date.\r\n\r\n\r\nyc <- 40\r\nzc <- 190\r\nseasons <- 1959:2019\r\n\r\niSeason <- genSeason(hourtemps,\r\n                     mrange = c(8, 6),\r\n                     years = seasons)\r\nfor (sea in 1:length(seasons))\r\n{season_data <- hourtemps$hourtemps[iSeason[[sea]], ]\r\n res <- PhenoFlex(temp = season_data$Temp,\r\n                  times = c(1: length(season_data$Temp)),\r\n                  zc = zc,\r\n                  stopatzc = TRUE,\r\n                  yc = yc,\r\n                  basic_output = FALSE)\r\n if(sea == 1)\r\n    results <- season_data$DATE[res$bloomindex] else\r\n      results <- c(results,\r\n                   season_data$DATE[res$bloomindex])}\r\n\r\npredictions <- data.frame(Season = seasons,\r\n                          Prediction = results)\r\npredictions$Prediction <-\r\n  ISOdate(2001,\r\n          substr(predictions$Prediction, 4, 5),\r\n          substr(predictions$Prediction, 1, 2))\r\n\r\nggplot(data = predictions,\r\n       aes(x = Season,\r\n           y = Prediction)) +\r\n  geom_smooth() +\r\n  geom_point() +\r\n  ylab(\"Predicted bloom date\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nPhenoFlex can now be used to compute bloom dates but still requires parameter adjustments for specific tree cultivars. The model structure allows calibration with observed phenology data, improving prediction accuracy.\r\nParameterizing PhenoFlex\r\nThe PhenoFlex model is a phenological model designed to predict bloom dates based on temperature data. It consists of 12 parameters that govern chilling accumulation, heat accumulation, and dormancy-breaking processes. Since these parameters are not directly measurable from real-world biological processes, they must be estimated using computational solvers.\r\nPhenoFlex Model Parameters\r\nThe model’s parameters define various aspects of dormancy and temperature response. Below is an overview of these parameters and their default values:\r\nyc (Chilling requirement) – Defines the critical value at which chill accumulation ends (Default: 40)\r\nzc (Heat requirement) – Defines the critical value at which heat accumulation ends (Default: 190)\r\ns1 (Transition slope) – Governs the transition from chill to heat accumulation (Default: 0.5)\r\nTu (Optimal temperature for GDH model) – The temperature at which heat accumulation is most effective (Default: 25°C)\r\nE0 (Activation energy for precursor formation) – A key parameter from the Dynamic Model (Default: 3372.8)\r\nE1 (Activation energy for precursor destruction) – Represents energy needed to break down dormancy (Default: 9900.3)\r\nA0 (Amplitude of precursor formation process) – Describes dormancy precursor production (Default: 6319.5)\r\nA1 (Amplitude of precursor destruction process) – Determines the intensity of dormancy precursor degradation (Default: 5.94 × 10¹³)\r\nTf (Transition temperature for sigmoidal function) – Converts the dormancy precursor to Chill Portions (Default: 4°C)\r\nTc (Upper threshold for GDH model) – The maximum temperature where heat accumulation is effective (Default: 36°C)\r\nTb (Base temperature for GDH model) – The minimum temperature for heat accumulation (Default: 4°C)\r\nSlope (Sigmoidal slope parameter) – Defines the rate of precursor conversion to Chill Portions (Default: 1.6)\r\nThese parameters are essential for capturing the biological response of plants to temperature variations.\r\nChallenges in Parameter Estimation\r\nUnlike simple regression models, PhenoFlex lacks a direct analytical solution for parameter estimation. Instead, empirical optimization techniques are needed to identify a parameter set that minimizes prediction error.\r\nThis process begins with initial parameter guesses, and plausible upper and lower bounds for each parameter are defined. The solver iteratively adjusts these values to improve predictions. However, multiple solutions can exist due to local minima in the error function, requiring multiple runs with different starting conditions.\r\nDataset Preparation\r\nTo fit the PhenoFlex model, historical bloom data and hourly temperature records for ‘Alexander Lucas’ pears from 1958 to 2019 were used. The data is preprocessed as follows:\r\n\r\n\r\nAlex_first <-\r\n  read_tab(\"data/Alexander_Lucas_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\nhourtemps <- \r\n  read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  stack_hourly_temps(latitude = 50.6)\r\n\r\n\r\nDefining Initial Parameter Sets\r\nBefore fitting the model, initial parameter values, along with their upper and lower limits, must be set:\r\n\r\n\r\n# here's the order of the parameters (from the helpfile of the\r\n# PhenoFlex_GDHwrapper function)\r\n#          yc,  zc,  s1, Tu,    E0,      E1,     A0,         A1,   Tf, Tc, Tb,  slope\r\npar <-   c(40, 190, 0.5, 25, 3372.8,  9900.3, 6319.5,\r\n           5.939917e13,  4, 36,  4,  1.60)\r\nupper <- c(41, 200, 1.0, 30, 4000.0, 10000.0, 7000.0,  \r\n           6.e13, 10, 40, 10, 50.00)\r\nlower <- c(38, 180, 0.1, 0 , 3000.0,  9000.0, 6000.0,   \r\n           5.e13,  0,  0,  0,  0.05)\r\n\r\n\r\nSince some parameters describe hypothetical biological processes, their estimation remains difficult. The activation energies (E0, E1) and amplitudes (A0, A1) in particular do not have well-defined values in literature.\r\nModel Fitting Using Simulated Annealing\r\nThe Simulated Annealing algorithm is used to optimize the parameters. The process is as follows:\r\nGenerate a season list based on hourly temperature data.\r\nUse the phenologyFitter function to adjust the parameters iteratively.\r\nRun the solver for 1000 iterations to ensure convergence.\r\n\r\n\r\nSeasonList <- genSeasonList(hourtemps$hourtemps,\r\n                            mrange = c(8, 6),\r\n                            years = c(1959:2019))\r\n\r\nFit_res <- \r\n  phenologyFitter(par.guess = par, \r\n                  modelfn = PhenoFlex_GDHwrapper,\r\n                  bloomJDays = Alex_first$pheno[which(Alex_first$Year > 1958)],\r\n                  SeasonList = SeasonList,\r\n                  lower = lower,\r\n                           upper = upper,\r\n                           control = list(smooth = FALSE,\r\n                                          verbose = FALSE, \r\n                                          maxit = 1000,\r\n                                          nb.stop.improvement = 5))\r\n\r\nAlex_par <- Fit_res$par\r\n\r\nwrite.csv(Alex_par,\r\n          \"data/PhenoFlex_parameters_Alexander_Lucas.csv\")\r\n\r\n\r\nModel Performance and Error Analysis\r\nAfter fitting, the model’s predictions are compared with actual bloom dates:\r\n\r\n\r\nAlex_par <- \r\n  read_tab(\"data/PhenoFlex_parameters_Alexander_Lucas.csv\")[,2]\r\n\r\nSeasonList <- genSeasonList(hourtemps$hourtemps, \r\n                            mrange = c(8, 6),\r\n                            years = c(1959:2019))\r\n\r\nAlex_PhenoFlex_predictions <- Alex_first[which(Alex_first$Year > 1958),]\r\n\r\nfor(y in 1:length(Alex_PhenoFlex_predictions$Year))\r\n   Alex_PhenoFlex_predictions$predicted[y] <-\r\n    PhenoFlex_GDHwrapper(SeasonList[[y]],\r\n                         Alex_par)\r\n\r\nAlex_PhenoFlex_predictions$Error <- \r\n  Alex_PhenoFlex_predictions$predicted - \r\n  Alex_PhenoFlex_predictions$pheno\r\n\r\nRMSEP(Alex_PhenoFlex_predictions$predicted,\r\n      Alex_PhenoFlex_predictions$pheno)\r\n\r\n[1] 6.146689\r\n\r\n\r\n\r\nmean(Alex_PhenoFlex_predictions$Error)\r\n\r\n[1] 1.030738\r\n\r\n\r\n\r\nmean(abs(Alex_PhenoFlex_predictions$Error))\r\n\r\n[1] 4.622268\r\n\r\nVisualization\r\nThe model’s performance is visualized using scatter plots and histograms:\r\n\r\n\r\nggplot(Alex_PhenoFlex_predictions,\r\n       aes(x = pheno,\r\n           y = predicted)) +\r\n  geom_point() +\r\n  geom_abline(intercept = 0,\r\n              slope = 1) +\r\n  theme_bw(base_size = 15) +\r\n  xlab(\"Observed bloom date (Day of the year)\") +\r\n  ylab(\"Predicted bloom date (Day of the year)\") +\r\n  ggtitle(\"Predicted vs. observed bloom dates\")\r\n\r\n\r\n\r\n\r\n\r\nggplot(Alex_PhenoFlex_predictions,\r\n       aes(Error)) +\r\n  geom_histogram() +\r\n  ggtitle(\"Distribution of prediction errors\")\r\n\r\n\r\n\r\nConclusions\r\nThe PhenoFlex framework successfully predicted bloom dates for ‘Alexander Lucas’ pears in Klein-Altendorf, with errors normally distributed and evenly spread across all bloom dates. This suggests the model is reliable for both early and late blooming.\r\nThe model’s accuracy makes it a promising tool for predicting phenology under future climate conditions, outperforming previous models that lacked integration of current temperature response knowledge during dormancy.\r\nExercises on the PhenoFlex model\r\nParameterize the PhenoFlex model for `Roter Boskoop’ apples.\r\n\r\n\r\n# Use Roter Boskoop dataset to fit the PhenoFlex model\r\nRB_first <-\r\n  read_tab(\"data/Roter_Boskoop_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\nhourtemps <- \r\n  read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  stack_hourly_temps(latitude = 50.6)\r\n\r\n# Parameter selection\r\npar <- c(40, 190, 0.5, 25, 3372.8, 9900.3, 6319.5,\r\n         5.939917e13, 4, 36, 4, 1.60)\r\n\r\nupper <- c(41, 200, 1.0, 30, 4000.0, 10000.0, 7000.0,  \r\n           6.e13, 10, 40, 10, 50.00)\r\n\r\nlower <- c(38, 180, 0.1, 0, 3000.0, 9000.0, 6000.0,   \r\n           5.e13, 0, 0, 0, 0.05)\r\n\r\n# Generate season list for model fitting\r\nSeasonList <- genSeasonList(hourtemps$hourtemps,\r\n                            mrange = c(8, 6),\r\n                            years = c(1959:2019))\r\n\r\n# Fit model using phenologyFitter\r\nFit_res <-\r\n  phenologyFitter(par.guess = par,\r\n                  modelfn = PhenoFlex_GDHwrapper,\r\n                  bloomJDays = RB_first$pheno[which(RB_first$Year >= 1958)],\r\n                  SeasonList = SeasonList,\r\n                  lower = lower,\r\n                  upper = upper,\r\n                  control = list(smooth = FALSE,\r\n                                 verbose = FALSE,\r\n                                 maxit = 1000,\r\n                                 nb.stop.improvement = 5))\r\n# Save fitted parameters\r\nRB_par <- Fit_res$par\r\n\r\nwrite.csv(RB_par,\r\n          \"data/PhenoFlex_parameters_Roter_Boskoop.csv\")\r\n\r\n\r\nProduce plots of predicted vs. observed bloom dates and distribution of prediction errors.\r\n\r\n\r\nRB_PhenoFlex_predictions <- RB_first[which(RB_first$Year > 1958),]\r\nfor(y in 1:length(RB_PhenoFlex_predictions$Year))\r\n   RB_PhenoFlex_predictions$predicted[y] <-\r\n    PhenoFlex_GDHwrapper(SeasonList[[y]],\r\n                         RB_par)\r\n\r\nRB_PhenoFlex_predictions$Error <- \r\n  RB_PhenoFlex_predictions$predicted - \r\n  RB_PhenoFlex_predictions$pheno\r\n  \r\n# Predicted vs. observed bloom dates \r\nggplot(RB_PhenoFlex_predictions,\r\n       aes(x = pheno,\r\n           y = predicted)) +\r\n  geom_point() +\r\n  geom_abline(intercept = 0,\r\n              slope = 1) +\r\n  theme_bw(base_size = 15) +\r\n  xlab(\"Observed bloom date (Day of the year)\") +\r\n  ylab(\"Predicted bloom date (Day of the year)\") +\r\n  ggtitle(\"Predicted vs. observed bloom dates\")\r\n\r\n\r\n\r\n\r\n\r\n# Distribution of prediction errors\r\nggplot(RB_PhenoFlex_predictions,\r\n       aes(Error)) +\r\n  geom_histogram() +\r\n  ggtitle(\"Distribution of prediction errors\")\r\n\r\n\r\n\r\nCompute the model performance metrics RMSEP, mean error and mean absolute error.\r\n\r\n\r\n# RMSEP\r\nRMSEP(RB_PhenoFlex_predictions$predicted,\r\n      RB_PhenoFlex_predictions$pheno)\r\n\r\n[1] 4.52457\r\n\r\n\r\n\r\n# Mean Error\r\nmean(RB_PhenoFlex_predictions$Error)\r\n\r\n[1] -1.23125\r\n\r\n\r\n\r\n# Absolute Error\r\nmean(abs(RB_PhenoFlex_predictions$Error))\r\n\r\n[1] 3.347917\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:44:08+01:00"
    },
    {
      "path": "phenoflex2.html",
      "title": "The PhenoFlex model - a second look",
      "author": [],
      "contents": "\r\nBasic Diagnostics for PhenoFlex Fit\r\nWhen developing a new model, it’s easy to become overly enthusiastic and overlook potential weaknesses. This is a common issue for modelers, particularly those without proper statistical training who may not be as aware of the potential pitfalls.\r\nTo examine the PhenoFlex framework and its parameters, helper functions can be used to explore the temperature response during chilling and forcing periods.\r\n\r\n\r\napply_const_temp <-\r\n  function(temp, A0, A1, E0, E1, Tf, slope, portions = 1200, deg_celsius = TRUE)\r\n    {\r\n  temp_vector <- rep(temp,\r\n                     times = portions)\r\n  res <- chillR::DynModel_driver(temp = temp_vector,\r\n                                 A0 = A0,\r\n                                 A1 = A1,\r\n                                 E0 = E0,\r\n                                 E1 = E1,\r\n                                 Tf = Tf,\r\n                                 slope = slope,\r\n                                 deg_celsius = deg_celsius)\r\n  return(res$y[length(res$y)])\r\n}\r\n\r\ngen_bell <- function(par,\r\n                     temp_values = seq(-5, 20, 0.1)) {\r\n  E0 <- par[5]\r\n  E1 <- par[6]\r\n  A0 <- par[7]\r\n  A1 <- par[8]\r\n  Tf <- par[9]\r\n  slope <- par[12]\r\n\r\n  y <- c()\r\n  for(i in seq_along(temp_values)) {\r\n    y[i] <- apply_const_temp(temp = temp_values[i],\r\n                             A0 = A0,\r\n                             A1 = A1,\r\n                             E0 = E0,\r\n                             E1 = E1,\r\n                             Tf = Tf,\r\n                             slope = slope)\r\n  }\r\n  return(invisible(y))\r\n}\r\n\r\nGDH_response <- function(T, par)\r\n  {Tb <- par[11]\r\n   Tu <- par[4]\r\n   Tc <- par[10]\r\n   GDH_weight <- rep(0, length(T))\r\n   GDH_weight[which(T >= Tb & T <= Tu)] <-\r\n     1/2 * (1 + cos(pi + pi * (T[which(T >= Tb & T <= Tu)] - Tb)/(Tu - Tb)))\r\n   GDH_weight[which(T > Tu & T <= Tc)] <-\r\n     (1 + cos(pi/2 + pi/2 * (T[which(T >  Tu & T <= Tc)] -Tu)/(Tc - Tu)))\r\n  return(GDH_weight)\r\n}\r\n\r\n\r\nThe gen_bell function generates the chilling response curve, and GDH_response illustrates the heat effectiveness curve.\r\nThese functions can be applied to the parameter set for ‘Alexander Lucas’ pears:\r\n\r\n\r\nAlex_par <- read_tab(\"data/PhenoFlex_parameters_Alexander_Lucas.csv\")[,2]\r\n\r\ntemp_values = seq(-5, 30, 0.1)\r\n\r\ntemp_response <- data.frame(Temperature = temp_values,\r\n                            Chill_response = gen_bell(Alex_par,\r\n                                                      temp_values),\r\n                            Heat_response = GDH_response(temp_values,\r\n                                                         Alex_par))\r\n\r\npivoted_response <- pivot_longer(temp_response, \r\n                                 c(Chill_response,\r\n                                   Heat_response))\r\n\r\nggplot(pivoted_response,\r\n       aes(x = Temperature,\r\n           y = value)) +\r\n  geom_line(linewidth = 2,\r\n            aes(col = name)) +\r\n  ylab(\"Temperature response (arbitrary units)\") +\r\n  xlab(\"Temperature (°C)\") +\r\n  facet_wrap(vars(name),\r\n             scales = \"free\",\r\n             labeller = \r\n               labeller(name = c(Chill_response = c(\"Chill response\"),\r\n                                     Heat_response = c(\"Heat response\")))) +\r\n  scale_color_manual(values = c(\"Chill_response\" = \"blue\",\r\n                                \"Heat_response\" = \"red\")) +\r\n  theme_bw(base_size = 15) +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nThe response curves show reasonable patterns. The chilling curve peaks around 2-3°C and declines sharply after 6°C, assuming constant temperatures, which rarely occur in orchards. The heat response also appears plausible, though the final drop-off at high temperatures (30°C) may not be accurate, as such temperatures are rarely observed during dormancy.\r\nPhenoFlex Temperature Response\r\nIn this case, temperature responses are evaluated based on daily minimum and maximum temperatures, as opposed to assuming constant temperatures. Below is the simulation process:\r\n\r\n\r\nlatitude <- 50.6\r\n\r\nmonth_range <- c(10, 11, 12, 1, 2, 3)\r\n\r\nTmins = c(-20:20)\r\nTmaxs = c(-15:30)\r\n\r\nmins <- NA\r\nmaxs <- NA\r\nchill_eff <- NA\r\nheat_eff <- NA\r\nmonth <- NA\r\n\r\nsimulation_par <- Alex_par\r\n\r\nfor(mon in month_range)\r\n    {days_month <- as.numeric(difftime(ISOdate(2002, mon+1, 1),\r\n                                       ISOdate(2002, mon, 1)))\r\n     if(mon == 12) days_month <- 31\r\n     weather <- \r\n       make_all_day_table(data.frame(Year = c(2002, 2002),                                   \r\n                                     Month = c(mon, mon),\r\n                                     Day = c(1, days_month),\r\n                                     Tmin = c(0, 0),\r\n                                     Tmax = c(0, 0)))\r\n     \r\n     for(tmin in Tmins)\r\n      for(tmax in Tmaxs)\r\n        if(tmax >= tmin)\r\n          {\r\n           hourtemps <- weather %>%\r\n             mutate(Tmin = tmin,\r\n                    Tmax = tmax) %>%\r\n             stack_hourly_temps(latitude = latitude) %>%\r\n             pluck(\"hourtemps\", \"Temp\")\r\n           \r\n           chill_eff <- \r\n             c(chill_eff,\r\n               PhenoFlex(temp = hourtemps,\r\n                         times = c(1: length(hourtemps)),\r\n                         A0 = simulation_par[7],\r\n                         A1 = simulation_par[8],\r\n                         E0 = simulation_par[5],\r\n                         E1 = simulation_par[6],\r\n                         Tf = simulation_par[9],\r\n                         slope = simulation_par[12],\r\n                         deg_celsius = TRUE,\r\n                         basic_output = FALSE)$y[length(hourtemps)] /\r\n                                            (length(hourtemps) / 24))\r\n           \r\n          heat_eff <- \r\n            c(heat_eff,\r\n              cumsum(GDH_response(hourtemps,\r\n                                  simulation_par))[length(hourtemps)] /\r\n                                                 (length(hourtemps) / 24))\r\n          mins <- c(mins, tmin)\r\n          maxs <- c(maxs, tmax)\r\n          month <- c(month, mon)\r\n        }\r\n}\r\n\r\nresults <- data.frame(Month = month,\r\n                      Tmin = mins,\r\n                      Tmax = maxs,\r\n                      Chill_eff = chill_eff,\r\n                      Heat_eff = heat_eff) %>%\r\n  filter(!is.na(Month))\r\n\r\nwrite.csv(results,\r\n          \"data/model_sensitivity_PhenoFlex.csv\")\r\n\r\n\r\nNext, the results can be visualized to examine chill and heat efficiency sensitivities:\r\n\r\n\r\nModel_sensitivities_PhenoFlex <-\r\n  read.csv(\"data/model_sensitivity_PhenoFlex.csv\")\r\n\r\nCKA_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_PhenoFlex,\r\n                        CKA_weather,\r\n                        temp_model = \"Chill_eff\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        Tmins = c(-20:20),\r\n                        Tmaxs = c(-15:30),\r\n                        legend_label = \"Chill per day \\n(arbitrary)\") +\r\n  ggtitle(\"PhenoFlex chill efficiency ('Alexander Lucas')\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_PhenoFlex,\r\n                        CKA_weather,\r\n                        temp_model = \"Heat_eff\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        Tmins = c(-20:20),\r\n                        Tmaxs = c(-15:30),\r\n                        legend_label = \"Heat per day \\n(arbitrary)\") +\r\n  ggtitle(\"PhenoFlex heat efficiency ('Alexander Lucas')\")\r\n\r\n\r\n\r\nOverall Impression\r\nEvaluating the accuracy of the PhenoFlex model remains challenging. Although the fit is good, the model should be assessed beyond predictive performance. It aligns well with existing knowledge about tree phenology during dormancy and is consistent with the parameterization derived from the data.\r\nCaveats and Assumptions\r\nThe parameter ranges used for the model fit were quite restricted, which could have limited the model’s flexibility. Several assumptions were made, including the start date of chill accumulation and the general nature of chill and heat accumulation dynamics. Additionally, the model’s output is location- and cultivar-specific, which complicates comparisons across regions.\r\nOutlook\r\nThe PhenoFlex model requires further testing, better guidance for parameter selection, and standardization of chill and heat units. It should also be applied across various climates to explore its potential for future phenology projections.\r\nExercises on basic PhenoFlex diagnostics\r\nMake chill and heat response plots for the ‘Roter Boskoop’ PhenoFlex model for the location you did the earlier analyses for.\r\n\r\n\r\n# Evaluate temperature responses based on daily minimum and maximum temperatures\r\nlatitude <- 50.6\r\n\r\nmonth_range <- c(10, 11, 12, 1, 2, 3)\r\n\r\nTmins = c(-20:20)\r\nTmaxs = c(-15:30)\r\n\r\nmins <- NA\r\nmaxs <- NA\r\nchill_eff <- NA\r\nheat_eff <- NA\r\nmonth <- NA\r\n\r\nsimulation_par <- RB_par\r\n\r\nfor(mon in month_range)\r\n    {days_month <- as.numeric(difftime(ISOdate(2002, mon+1, 1),\r\n                                       ISOdate(2002, mon, 1)))\r\n     if(mon == 12) days_month <- 31\r\n     weather <- \r\n       make_all_day_table(data.frame(Year = c(2002, 2002),                                   \r\n                                     Month = c(mon, mon),\r\n                                     Day = c(1, days_month),\r\n                                     Tmin = c(0, 0),\r\n                                     Tmax = c(0, 0)))\r\n     \r\n     for(tmin in Tmins)\r\n      for(tmax in Tmaxs)\r\n        if(tmax >= tmin)\r\n          {\r\n           hourtemps <- weather %>%\r\n             mutate(Tmin = tmin,\r\n                    Tmax = tmax) %>%\r\n             stack_hourly_temps(latitude = latitude) %>%\r\n             pluck(\"hourtemps\", \"Temp\")\r\n           \r\n           chill_eff <- \r\n             c(chill_eff,\r\n               PhenoFlex(temp = hourtemps,\r\n                         times = c(1: length(hourtemps)),\r\n                         A0 = simulation_par[7],\r\n                         A1 = simulation_par[8],\r\n                         E0 = simulation_par[5],\r\n                         E1 = simulation_par[6],\r\n                         Tf = simulation_par[9],\r\n                         slope = simulation_par[12],\r\n                         deg_celsius = TRUE,\r\n                         basic_output = FALSE)$y[length(hourtemps)] /\r\n                                            (length(hourtemps) / 24))\r\n           \r\n          heat_eff <- \r\n            c(heat_eff,\r\n              cumsum(GDH_response(hourtemps,\r\n                                  simulation_par))[length(hourtemps)] /\r\n                                                 (length(hourtemps) / 24))\r\n          mins <- c(mins, tmin)\r\n          maxs <- c(maxs, tmax)\r\n          month <- c(month, mon)\r\n        }\r\n}\r\n\r\nresults <- data.frame(Month = month,\r\n                      Tmin = mins,\r\n                      Tmax = maxs,\r\n                      Chill_eff = chill_eff,\r\n                      Heat_eff = heat_eff) %>%\r\n  filter(!is.na(Month))\r\n\r\nwrite.csv(results,\r\n          \"data/model_sensitivity_PhenoFlex_RB.csv\")\r\n\r\n\r\n\r\n\r\n# Load necessary data \r\nModel_sensitivities_PhenoFlex <-\r\n  read.csv(\"data/model_sensitivity_PhenoFlex_RB.csv\")\r\n\r\nCKA_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\n\r\n\r\n\r\n# Plot chill response fo 'Roter Boskoop'\r\nChill_sensitivity_temps(Model_sensitivities_PhenoFlex,\r\n                        CKA_weather,\r\n                        temp_model = \"Chill_eff\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        Tmins = c(-20:20),\r\n                        Tmaxs = c(-15:30),\r\n                        legend_label = \"Chill per day \\n(arbitrary)\") +\r\n  ggtitle(\"PhenoFlex chill efficiency ('Roter Boskoop')\")\r\n\r\n\r\n\r\n\r\n\r\n# Plot heat response of 'Roter Boskoop'\r\nChill_sensitivity_temps(Model_sensitivities_PhenoFlex,\r\n                        CKA_weather,\r\n                        temp_model = \"Heat_eff\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        Tmins = c(-20:20),\r\n                        Tmaxs = c(-15:30),\r\n                        legend_label = \"Heat per day \\n(arbitrary)\") +\r\n  ggtitle(\"PhenoFlex heat efficiency ('Roter Boskoop')\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:48:53+01:00"
    },
    {
      "path": "plot.html",
      "title": "Plotting future scenarios",
      "author": [],
      "contents": "\r\nThe default function in chillR generates climate impact projections but offers limited customization options due to its design prior to the widespread use of ggplot2. In this chapter, a similar plot will be created using ggplot to allow for greater flexibility. A function developed by Eduardo Fernandez is already included in chillR, but the process will be built step by step to demonstrate how complex plots can be constructed with ggplot. His code has been slightly modified for clarity.\r\nFirst, all necessary packages, including ggpmisc and patchwork, will be installed and loaded. Additionally, outputs from previous chapters, such as “Historic Temperature Scenarios” and “Making CMIP6 Scenarios,” will be used.\r\n\r\n\r\nlibrary(kableExtra)\r\nlibrary(chillR)\r\nlibrary(tidyverse)\r\nlibrary(ggpmisc)\r\nlibrary(patchwork)\r\n\r\nchill_hist_scenario_list <- load_temperature_scenarios(\"data\",\r\n                                                       \"Bonn_hist_chill_305_59\")\r\nactual_chill <- read_tab(\"data/Bonn_observed_chill_305_59.csv\")\r\n\r\nchill_future_scenario_list <- load_temperature_scenarios(\"data/future_climate\",\"Bonn_futurechill_305_59\")\r\n\r\nchills <- make_climate_scenario(\r\n  chill_hist_scenario_list,\r\n  caption = \"Historic\",\r\n  historic_data = actual_chill,\r\n  time_series = TRUE)\r\n\r\nSSPs <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\nlist_ssp <- \r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n    {\r\n    \r\n    # find all scenarios for the ssp and time\r\n    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]\r\n    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    if(SSP == \"ssp126\") SSPcaption <- \"SSP1\"\r\n    if(SSP == \"ssp245\") SSPcaption <- \"SSP2\"\r\n    if(SSP == \"ssp370\") SSPcaption <- \"SSP3\"\r\n    if(SSP == \"ssp585\") SSPcaption <- \"SSP5\"    \r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- chill %>% \r\n      make_climate_scenario(\r\n        caption = c(SSPcaption,\r\n                    Time_caption),\r\n        add_to = chills)\r\n}\r\n\r\n\r\nTo work effectively with ggplot, data needs to be arranged in a single data.frame rather than the list format currently used for storing projection data. Each row of the data.frame should include details such as the GCM, SSP, and Year. The most straightforward way to achieve this is by iterating through the elements of the chill projection list, extracting the necessary information, and combining it into a new, long-format data.frame.\r\n\r\n\r\n# We'll first process the past scenarios (element 1 of the chills list).\r\n# Within the data element, we have a list of multiple data.frames for\r\n# the various past scenarios.\r\n# Using a 'for' loop, we cycle through all these data.frames.\r\n\r\nfor(nam in names(chills[[1]]$data))\r\n  {\r\n   # Extract the data frame.\r\n   ch <- chills[[1]]$data[[nam]]\r\n   # Add columns for the new information we have to add and fill them.\r\n   ch[,\"GCM\"] <- \"none\"\r\n   ch[,\"SSP\"] <- \"none\"\r\n   ch[,\"Year\"] <- as.numeric(nam)\r\n   \r\n   # Now check if this is the first time we've gone through this loop.\r\n   # If this is the first time, the ch data.frame becomes the output\r\n   # object (past_simulated).\r\n   # If it is not the first time ('else'), we add the current data.frame\r\n   # to the 'past_simulated' object\r\n  if(nam == names(chills[[1]]$data)[1])\r\n    past_simulated <- ch else\r\n      past_simulated <- rbind(past_simulated,\r\n                              ch)\r\n  }\r\n\r\n# We add another column called 'Scenario' and label all rows as 'Historical' \r\npast_simulated[\"Scenario\"] <- \"Historical\"\r\n\r\n\r\n\r\n\r\n# We'll want to add the historic observation too, so let's simplify the\r\n# pointer to this information for easier use later\r\n\r\npast_observed <- chills[[1]][[\"historic_data\"]]\r\n\r\n\r\nThe same process is now applied to the future data, transforming it into a long-format data.frame for further analysis and visualization.\r\n\r\n\r\n# Extract future data\r\nfor(i in 2:length(chills))\r\n  for(nam in names(chills[[i]]$data))\r\n    {ch <- chills[[i]]$data[[nam]]\r\n     ch[,\"GCM\"] <- nam\r\n     ch[,\"SSP\"] <- chills[[i]]$caption[1]\r\n     ch[,\"Year\"] <- chills[[i]]$caption[2]\r\n     if(i == 2 & nam == names(chills[[i]]$data)[1])\r\n       future_data <- ch else\r\n         future_data <- rbind(future_data,ch)\r\n  }\r\n\r\n\r\nThe data is now formatted for use with ggplot and includes three metrics: Chill_Portions, GDH, and Frost_H. While a function could be written to handle this process, the current approach focuses on making the code flexible enough to plot all three metrics without significant modifications. This is achieved by defining variables for the metric (metric) and its axis label (axis_label) to allow easy adjustments.\r\nThe complex plot to be created cannot be produced as a single ggplot plot. However, this is manageable since multiple plots can be combined into a compound figure using the plot_layout function from the patchwork package.\r\nA challenge when combining plots with shared axes, such as a common y-axis, is that the axes may vary due to differing data ranges across plots. To address this, the range function is used to determine axis extents that are consistent and reasonable for all plots.\r\n\r\n\r\nmetric <- \"GDH\"\r\naxis_label <- \"Heat (in GDH)\"\r\n\r\n# get extreme values for the axis scale\r\n\r\nrng <- range(past_observed[[metric]],\r\n             past_simulated[[metric]],\r\n             future_data[[metric]])  \r\nrng\r\n\r\n[1]   739.4306 20977.6554\r\n\r\nThe first plot can now be created, focusing on the past scenarios. This serves as the starting point for visualizing the data:\r\n\r\n\r\npast_plot <- ggplot() +\r\n  geom_boxplot(data = past_simulated,\r\n               aes_string(\"as.numeric(Year)\",\r\n                          metric,group=\"Year\"),\r\n               fill = \"skyblue\")\r\n\r\n\r\n\r\n\r\npast_plot\r\n\r\n\r\n\r\nThe general layout of this ggplot command should now be familiar. However, a specific detail requires attention: in many ggplot commands, variable names can be written directly (e.g., group = Year) without quotation marks, which is uncommon in R. Normally, strings must be quoted; otherwise, R interprets them as variable names and raises an error if the variable is undefined.\r\nThe developers of ggplot2, led by Hadley Wickham, have removed this requirement, simplifying code in many cases. However, this convenience can cause issues when actual variable names are needed in ggplot2 calls. To address this, the aes_string function can be used instead of aes, as demonstrated in the example.\r\nAdditionally, the y-axis must accommodate not only the data from this plot but also the data from all future scenarios. This is achieved using the range (rng) determined earlier. The axis labels will also be customized, with the y-axis label set using the previously defined axis_label.\r\n\r\n\r\npast_plot <- past_plot +\r\n  scale_y_continuous(\r\n    limits = c(0, \r\n               round(rng[2] + rng[2]/10))) +\r\n  labs(x = \"Year\", \r\n       y = axis_label)\r\n\r\npast_plot\r\n\r\n\r\n\r\nThe plot for the past scenarios is now complete, but adjustments are needed to ensure consistent formatting with the future data. While the past scenarios involve only one plot, the future scenarios will include multiple plots, created using the facet_wrap function. This function automatically adds design elements to the plots.\r\nTo maintain a uniform layout across the entire figure, the single plot for the past scenarios will also be converted into a facet layout. Additionally, the black-and-white theme will be applied for consistency.\r\n\r\n\r\npast_plot <- past_plot +\r\n  facet_grid(~ Scenario) +\r\n  theme_bw(base_size = 15) \r\n  \r\npast_plot\r\n\r\n\r\n\r\nThe final adjustments involve modifying the facet title and x-axis text. The facet title’s background will be removed, and its text will be set to bold for clarity. The x-axis text will be angled to ensure proper display of all year labels, even if the text size is adjusted.\r\n\r\n\r\npast_plot <- past_plot +  \r\n  theme(strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\"),\r\n        axis.text.x = element_text(angle=45,\r\n                                   hjust=1)) \r\n\r\npast_plot\r\n\r\n\r\n\r\nThe plot of past scenarios is now complete. Next, actual observations will be added to the plot. To distinguish them from the outliers in the box plots, the observations will be colored blue. This step is straightforward and will enhance the clarity of the visualization.\r\n\r\n\r\n# add historic data\r\npast_plot <- past_plot +\r\n  geom_point(data = past_observed,\r\n             aes_string(\"End_year\",\r\n                        metric),\r\n             col = \"blue\")\r\n\r\npast_plot\r\n\r\n\r\n\r\nWith the past scenarios complete, attention shifts to the future scenarios. These will be organized into two plots: one for the two scenarios in 2050 and another for 2085. These plots will later be displayed as two groups of two plots each.\r\nTo maintain an organized structure and allow for potential expansion with additional scenarios, the two plots will be stored in a list. While not strictly necessary for this case, this approach provides flexibility for future modifications.\r\nBefore constructing the plots within the list, one will be assembled first as an example, starting with the year 2050.\r\n\r\n\r\ny <- 2050\r\n\r\nfuture_2050 <- ggplot(data= future_data[which(future_data$Year==y),]) +\r\n  geom_boxplot(aes_string(\"GCM\", \r\n                          metric, \r\n                          fill = \"GCM\"))\r\n\r\nfuture_2050\r\n\r\n\r\n\r\nAt this point, all SSPs are mixed together, and the y-axis is not easily readable. While the information is conveyed through the color scheme, the plot appears cluttered. To improve its appearance, padding will be added to the sides of the plots using the expand parameter, ensuring the plot doesn’t look too crowded and enhancing its overall readability.\r\n\r\n\r\nfuture_2050 <- future_2050 +\r\n  facet_wrap(vars(SSP), nrow = 1) +\r\n   scale_x_discrete(labels = NULL,\r\n                    expand = expansion(add = 1)) \r\n\r\n\r\nIn this case, the axis limits also need to be adjusted to ensure consistency between the future and past plots. Additionally, the scenario year will be added to the plots for clarity. This is accomplished using the geom_text_npc function from the ggpmisc package. This will help in placing the year label appropriately on the plot.\r\n\r\n\r\nfuture_2050 <- future_2050 +\r\n  scale_y_continuous(limits = c(0, \r\n                                round(round(1.1*rng[2])))) +\r\n    geom_text_npc(aes(npcx = \"center\", \r\n                      npcy = \"top\",\r\n                      label = Year),\r\n                  size = 5)\r\n\r\nfuture_2050\r\n\r\n\r\n\r\nFor the final adjustments, the black-and-white theme will be applied once again. To reduce clutter, all axis text and titles will be removed, as they don’t add significant value. The y-axis ticks will also be removed. The legend will be placed at the bottom, and the facet title will be formatted in the same way as in the past plot to maintain visual consistency.\r\n\r\n\r\nfuture_2050 <- future_2050 +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.ticks.y = element_blank(),\r\n        axis.text = element_blank(),\r\n        axis.title = element_blank(),\r\n        legend.position = \"bottom\",\r\n        legend.margin = margin(0,\r\n                               0,\r\n                               0,\r\n                               0,\r\n                               \"cm\"),\r\n        legend.background = element_rect(),\r\n        strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\"),\r\n        legend.box.spacing = unit(0, \"cm\"),\r\n        plot.subtitle = element_text(hjust = 0.5,\r\n                                     vjust = -1,\r\n                                     size = 15 * 1.05,\r\n                                     face = \"bold\")) \r\n\r\nfuture_2050\r\n\r\n\r\n\r\nThe legend is currently too large, but it can be adjusted later.\r\nNext, the procedure will be implemented in a loop to create a list of two plots: one for 2050 and one for 2085. This approach will allow for consistent plotting while maintaining flexibility for future adjustments.\r\n\r\n\r\nfuture_plot_list <- list()\r\n\r\ntime_points <- c(2050, 2085)\r\n\r\nfor(y in time_points)\r\n{\r\n  future_plot_list[[which(y == time_points)]] <-\r\n    ggplot(data = future_data[which(future_data$Year==y),]) +\r\n    geom_boxplot(aes_string(\"GCM\",\r\n                            metric,\r\n                            fill=\"GCM\")) +\r\n    facet_wrap(vars(SSP), nrow = 1) +\r\n    scale_x_discrete(labels = NULL,\r\n                     expand = expansion(add = 1)) +\r\n    scale_y_continuous(limits = c(0, \r\n                                  round(round(1.1*rng[2])))) +\r\n    geom_text_npc(aes(npcx = \"center\",\r\n                      npcy = \"top\", \r\n                      label = Year),\r\n                  size = 5) +\r\n    theme_bw(base_size = 15) +\r\n    theme(axis.ticks.y = element_blank(),\r\n          axis.text = element_blank(),\r\n          axis.title = element_blank(),\r\n          legend.position = \"bottom\",\r\n          legend.margin = margin(0, \r\n                                 0, \r\n                                 0, \r\n                                 0, \r\n                                 \"cm\"),\r\n          legend.background = element_rect(),\r\n          strip.background = element_blank(),\r\n          strip.text = element_text(face = \"bold\"),\r\n          legend.box.spacing = unit(0, \"cm\"),\r\n          plot.subtitle = element_text(\r\n            hjust = 0.5,\r\n            vjust = -1,\r\n            size = 15 * 1.05,\r\n            face = \"bold\")) \r\n}\r\n\r\nfuture_plot_list\r\n\r\n[[1]]\r\n\r\n\r\n[[2]]\r\n\r\n\r\nCombining the plots is straightforward. The plots can be joined using the + sign, which allows for easy layering and combination into a single cohesive figure.\r\n\r\n\r\nboth_plots <- past_plot + future_plot_list\r\n\r\nboth_plots\r\n\r\n\r\n\r\nThe basic structure of the plots is in place, but a few adjustments are needed, particularly for the legend. The plot_layout function from the patchwork package is used for this purpose. It allows for the creation of patchwork figures by collecting all legends and removing duplicates, ensuring that only one version of the legend is displayed. Additionally, the width of the individual plots can be adjusted. To achieve this, a vector c(1, 1.8, 1.8) is used, specifying that each set of future plots should be 1.8 times the width of the past scenario plot. This provides a more balanced layout.\r\n\r\n\r\nplot <- both_plots +\r\n           plot_layout(guides = \"collect\",\r\n                       widths = c(1,rep(2,length(future_plot_list))))\r\n\r\n\r\nWith the previous adjustments, the plot is no longer clearly visible, so the legend will be moved to the bottom for better visibility. Due to how the patchwork package works, the corresponding theme call must be added after an & symbol to ensure the legend is positioned correctly at the bottom. This adjustment ensures the plot remains clear and the layout is consistent.\r\n\r\n\r\nplot <- plot & theme(legend.position = \"bottom\",\r\n                     legend.text = element_text(size=8),\r\n                     legend.title = element_text(size=10),\r\n                     axis.title.x = element_blank())\r\n\r\n\r\nThe results\r\nThe adjustments have resulted in clear and consistent ggplot figures displaying a heat analysis for Bonn. The figures are well-organized, with properly placed legends and axis labels, making them ready for presentation.\r\n\r\n\r\nplot\r\n\r\n\r\n\r\nThe process is now complete. The same procedure can be applied to the other metrics. By simply changing the metric and axis_label variables and running the code again, the same steps can be followed for the other metrics. The full code will not be shown here, but the approach should work seamlessly for these adjustments.\r\n\r\n\r\n\r\n\r\n\r\nmetric <- \"Frost_H\"\r\naxis_label <- \"Frost duration (in hours)\"\r\n\r\n# get extreme values for the axis scale\r\n\r\nrng <- range(past_observed[[metric]],\r\n             past_simulated[[metric]],\r\n             future_data[[metric]])  \r\npast_plot <- ggplot() +\r\n  geom_boxplot(data = past_simulated,\r\n               aes_string(\"as.numeric(Year)\",\r\n                          metric,group=\"Year\"),\r\n               fill=\"skyblue\") +\r\n  scale_y_continuous(limits = c(0, \r\n                                round(round(1.1*rng[2])))) +\r\n  labs(x = \"Year\", y = axis_label) +\r\n  facet_grid(~ Scenario) +\r\n  theme_bw(base_size = 15) +  \r\n  theme(strip.background = element_blank(),\r\n           strip.text = element_text(face = \"bold\"),\r\n           axis.text.x = element_text(angle=45, hjust=1)) +\r\n  geom_point(data = past_observed,\r\n             aes_string(\"End_year\",\r\n                        metric),\r\n             col=\"blue\")\r\n\r\nfuture_plot_list <- list()\r\n\r\nfor(y in time_points)\r\n{\r\n  future_plot_list[[which(y == time_points)]] <-\r\n    ggplot(data = future_data[which(future_data$Year==y),]) +\r\n    geom_boxplot(aes_string(\"GCM\", \r\n                            metric, \r\n                            fill=\"GCM\")) +\r\n  facet_wrap(vars(SSP), nrow = 1) +\r\n   scale_x_discrete(labels = NULL,\r\n                    expand = expansion(add = 1)) +\r\n  scale_y_continuous(limits = c(0,\r\n                                round(round(1.1*rng[2])))) +\r\n    geom_text_npc(aes(npcx = \"center\", \r\n                      npcy = \"top\", \r\n                      label = Year),\r\n                  size = 5) +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.ticks.y = element_blank(),\r\n        axis.text = element_blank(),\r\n        axis.title = element_blank(),\r\n        legend.position = \"bottom\",\r\n        legend.margin = margin(0,\r\n                               0, \r\n                               0,\r\n                               0, \r\n                               \"cm\"),\r\n        legend.background = element_rect(),\r\n        strip.background = element_blank(),\r\n        strip.text = element_text(face = \"bold\"),\r\n        legend.box.spacing = unit(0, \"cm\"),\r\n        plot.subtitle = element_text(hjust = 0.5, \r\n                                     vjust = -1,\r\n                                     size = 15 * 1.05,\r\n                                     face = \"bold\")) \r\n}\r\n\r\nplot <- (past_plot +\r\n           future_plot_list +\r\n           plot_layout(guides = \"collect\",\r\n                       widths = c(1,rep(2,length(future_plot_list))))\r\n         ) & theme(legend.position = \"bottom\",\r\n                   legend.text = element_text(size = 8),\r\n                   legend.title = element_text(size = 10),\r\n                   axis.title.x=element_blank())\r\nplot\r\n\r\n\r\n\r\nTo create a generally applicable function for the entire processing workflow, some additional effort is required. This involves ensuring that the function produces appropriate warnings or errors when given incorrect inputs, and making it flexible enough to handle different types of data. However, if the function is only intended for use with data structured in the same way as the current dataset, it can be simplified. In this case, all the steps performed so far can be wrapped into a function call, streamlining the process.\r\n\r\n\r\nplot_scenarios_gg <- function(past_observed,\r\n                              past_simulated,\r\n                              future_data,\r\n                              metric,\r\n                              axis_label,\r\n                              time_points)\r\n{\r\n  rng <- range(past_observed[[metric]],\r\n               past_simulated[[metric]],\r\n               future_data[[metric]])  \r\n  past_plot <- ggplot() +\r\n    geom_boxplot(data = past_simulated,\r\n                 aes_string(\"as.numeric(Year)\",\r\n                            metric,\r\n                            group=\"Year\"),\r\n                 fill=\"skyblue\") +\r\n    scale_y_continuous(limits = c(0, \r\n                                  round(round(1.1*rng[2])))) +\r\n    labs(x = \"Year\", y = axis_label) +\r\n    facet_grid(~ Scenario) +\r\n    theme_bw(base_size = 15) +  \r\n    theme(strip.background = element_blank(),\r\n          strip.text = element_text(face = \"bold\"),\r\n          axis.text.x = element_text(angle=45, \r\n                                     hjust=1)) +\r\n    geom_point(data = past_observed,\r\n               aes_string(\"End_year\",\r\n                          metric),\r\n               col=\"blue\")\r\n  \r\n  future_plot_list <- list()\r\n  \r\n  for(y in time_points)\r\n  {\r\n    future_plot_list[[which(y == time_points)]] <-\r\n      ggplot(data = future_data[which(future_data$Year==y),]) +\r\n      geom_boxplot(aes_string(\"GCM\", \r\n                              metric, \r\n                              fill=\"GCM\")) +\r\n      facet_wrap(vars(SSP), nrow = 1) +\r\n      scale_x_discrete(labels = NULL,\r\n                       expand = expansion(add = 1)) +\r\n      scale_y_continuous(limits = c(0, \r\n                                    round(round(1.1*rng[2])))) +\r\n      geom_text_npc(aes(npcx = \"center\",\r\n                        npcy = \"top\",\r\n                        label = Year),\r\n                    size = 5) +\r\n      theme_bw(base_size = 15) +\r\n      theme(axis.ticks.y = element_blank(),\r\n            axis.text = element_blank(),\r\n            axis.title = element_blank(),\r\n            legend.position = \"bottom\",\r\n            legend.margin = margin(0,\r\n                                   0, \r\n                                   0, \r\n                                   0, \r\n                                   \"cm\"),\r\n            legend.background = element_rect(),\r\n            strip.background = element_blank(),\r\n            strip.text = element_text(face = \"bold\"),\r\n            legend.box.spacing = unit(0, \"cm\"),\r\n            plot.subtitle = element_text(hjust = 0.5,\r\n                                         vjust = -1,\r\n                                         size = 15 * 1.05,\r\n                                         face = \"bold\")) \r\n  }\r\n  \r\n  plot <- (past_plot +\r\n             future_plot_list +\r\n             plot_layout(guides = \"collect\",\r\n                         widths = c(1,rep(2,length(future_plot_list))))\r\n           ) & theme(legend.position = \"bottom\",\r\n                     legend.text = element_text(size = 8),\r\n                     legend.title = element_text(size = 10),\r\n                     axis.title.x=element_blank())\r\n  plot\r\n  \r\n}\r\n\r\n\r\nBy wrapping all the steps into a function, the same outputs can now be generated more quickly. This approach simplifies the process and allows for efficient reuse of the code without repeating all the individual commands.\r\n\r\n\r\nplot_scenarios_gg(past_observed = past_observed,\r\n                  past_simulated = past_simulated,\r\n                  future_data = future_data,\r\n                  metric = \"GDH\",\r\n                  axis_label = \"Heat (in Growing Degree Hours)\",\r\n                  time_points = c(2050, 2085))\r\n\r\n\r\n\r\n\r\n\r\nplot_scenarios_gg(past_observed = past_observed,\r\n                  past_simulated = past_simulated,\r\n                  future_data = future_data,\r\n                  metric = \"Chill_Portions\",\r\n                  axis_label = \"Chill (in Chill Portions)\",\r\n                  time_points = c(2050, 2085))\r\n\r\n\r\n\r\n\r\n\r\nplot_scenarios_gg(past_observed = past_observed,\r\n                  past_simulated = past_simulated,\r\n                  future_data = future_data,\r\n                  metric = \"Frost_H\",\r\n                  axis_label = \"Frost duration (in hours)\",\r\n                  time_points = c(2050, 2085))\r\n\r\n\r\n\r\nExercises on plotting future projections\r\nProduce similar plots for the weather station you selected for earlier exercises.\r\n\r\n\r\nchill_hist_scenario_list <- load_temperature_scenarios(\"Yakima\",\r\n                                                       \"Yakima_hist_chill_305_59\")\r\nactual_chill <- read_tab(\"Yakima/Yakima_observed_chill_305_59.csv\")\r\n\r\nchill_future_scenario_list <- load_temperature_scenarios(\"Yakima/future_climate\",\"Yakima_futurechill_305_59\")\r\n\r\nchills <- make_climate_scenario(\r\n  chill_hist_scenario_list,\r\n  caption = \"Historic\",\r\n  historic_data = actual_chill,\r\n  time_series = TRUE)\r\n\r\nSSPs <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\nlist_ssp <- \r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(chill_future_scenario_list), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n    {\r\n    \r\n    # find all scenarios for the ssp and time\r\n    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]\r\n    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    if(SSP == \"ssp126\") SSPcaption <- \"SSP1\"\r\n    if(SSP == \"ssp245\") SSPcaption <- \"SSP2\"\r\n    if(SSP == \"ssp370\") SSPcaption <- \"SSP3\"\r\n    if(SSP == \"ssp585\") SSPcaption <- \"SSP5\"    \r\n    if(Time == \"2050\") Time_caption <- \"2050\"\r\n    if(Time == \"2085\") Time_caption <- \"2085\"\r\n    chills <- chill %>% \r\n      make_climate_scenario(\r\n        caption = c(SSPcaption,\r\n                    Time_caption),\r\n        add_to = chills)\r\n}\r\n\r\n\r\n\r\n\r\nfor(nam in names(chills[[1]]$data))\r\n  {\r\n   # Extract the data frame.\r\n   ch <- chills[[1]]$data[[nam]]\r\n   # Add columns for the new information we have to add and fill them.\r\n   ch[,\"GCM\"] <- \"none\"\r\n   ch[,\"SSP\"] <- \"none\"\r\n   ch[,\"Year\"] <- as.numeric(nam)\r\n   \r\n  if(nam == names(chills[[1]]$data)[1])\r\n    past_simulated <- ch else\r\n      past_simulated <- rbind(past_simulated,\r\n                              ch)\r\n  }\r\n\r\n# We add another column called 'Scenario' and label all rows as 'Historical' \r\npast_simulated[\"Scenario\"] <- \"Historical\"\r\n\r\n\r\n\r\n\r\nfor(i in 2:length(chills))\r\n  for(nam in names(chills[[i]]$data))\r\n    {ch <- chills[[i]]$data[[nam]]\r\n     ch[,\"GCM\"] <- nam\r\n     ch[,\"SSP\"] <- chills[[i]]$caption[1]\r\n     ch[,\"Year\"] <- chills[[i]]$caption[2]\r\n     if(i == 2 & nam == names(chills[[i]]$data)[1])\r\n       future_data <- ch else\r\n         future_data <- rbind(future_data,ch)\r\n  }\r\n\r\n\r\n\r\n\r\nplot_scenarios_gg(past_observed = past_observed,\r\n                  past_simulated = past_simulated,\r\n                  future_data = future_data,\r\n                  metric = \"Frost_H\",\r\n                  axis_label = \"Frost duration (in hours)\",\r\n                  time_points = c(2050, 2085))\r\n\r\n\r\n\r\n\r\n\r\nplot_scenarios_gg(past_observed = past_observed,\r\n                  past_simulated = past_simulated,\r\n                  future_data = future_data,\r\n                  metric = \"Chill_Portions\",\r\n                  axis_label = \"Chill (in Chill Portions)\",\r\n                  time_points = c(2050, 2085))\r\n\r\n\r\n\r\n\r\n\r\nplot_scenarios_gg(past_observed = past_observed,\r\n                  past_simulated = past_simulated,\r\n                  future_data = future_data,\r\n                  metric = \"GDH\",\r\n                  axis_label = \"Heat (in Growing Degree Hours)\",\r\n                  time_points = c(2050, 2085))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:52:03+01:00"
    },
    {
      "path": "PLS_not_work.html",
      "title": "Why PLS doesn’t always work",
      "author": [],
      "contents": "\r\nDisappointing PLS Performance\r\nPLS regression struggles to differentiate temperature response phases, particularly during chilling. Using agroclimatic metrics didn’t resolve the issue. Expected patterns appeared at the start and end of the endodormancy phase but showed large gaps in between. This highlights the limitations of the analysis method used.\r\nWhat PLS Regression Can Find\r\nPLS regression identifies relationships between variables with monotonic responses. It requires sufficient variation in input variables to detect meaningful patterns. In regions like Klein-Altendorf or Beijing, where winter chill accumulation shows little variation, PLS performance is limited.\r\nExploring the Model’s Response to Temperature\r\nTo better understand this, the model’s reaction to daily temperature patterns is visualized, considering extended periods for Chill Portion (CP) accumulation.\r\n\r\n\r\nmon <- 1 # Month\r\nndays <- 31 # Number of days per month\r\ntmin <- 1\r\ntmax <- 8\r\nlatitude <- 50\r\n\r\nweather <- make_all_day_table(\r\n  data.frame(Year = c(2001, 2001),\r\n             Month = c(mon, mon),\r\n             Day = c(1, ndays),\r\n             Tmin = c(0, 0),\r\n             Tmax = c(0, 0))) %>%\r\n  mutate(Tmin = tmin,\r\n         Tmax = tmax)\r\n\r\nhourly_temps <- stack_hourly_temps(weather, latitude = latitude)\r\n\r\nCPs <- Dynamic_Model(hourly_temps$hourtemps$Temp)\r\n\r\ndaily_CPs <- CPs[length(CPs)] / nrow(weather)\r\n\r\ndaily_CPs\r\n\r\n[1] 0.712235\r\n\r\nImproving Flexibility with Code Adjustments\r\nTo allow dynamic model usage, the do.call function is employed, enabling flexibility between models like Dynamic_Model and GDH.\r\nThe analysis is extended to include a wider range of Tmin/Tmax values and all months. The following code computes model sensitivity across these variables, generating data for temperature response patterns.\r\n\r\n\r\nlatitude <- 50.6\r\nmonth_range <- c(10, 11, 12, 1, 2, 3)\r\n\r\nTmins <- c(-20:20)\r\nTmaxs <- c(-15:30)\r\n\r\nmins <- NA\r\nmaxs <- NA\r\nCP <- NA\r\nmonth <- NA\r\ntemp_model <- Dynamic_Model\r\n\r\nfor(mon in month_range)\r\n    {days_month <- as.numeric(\r\n      difftime( ISOdate(2002,\r\n                        mon + 1,\r\n                        1),\r\n                ISOdate(2002,\r\n                        mon,\r\n                        1)))\r\n    \r\n    if(mon == 12) days_month <- 31\r\n    \r\n    weather <- make_all_day_table(\r\n                data.frame(Year = c(2001, 2001),\r\n                           Month = c(mon, mon),\r\n                           Day = c(1, days_month),\r\n                           Tmin = c(0, 0),\r\n                           Tmax = c(0, 0)))\r\n    \r\n    for(tmin in Tmins)\r\n      for(tmax in Tmaxs)\r\n        if(tmax >= tmin)\r\n          {\r\n          hourtemps <- weather %>%\r\n            mutate(Tmin = tmin,\r\n                   Tmax = tmax) %>%\r\n            stack_hourly_temps(latitude = latitude) %>% \r\n            pluck(\"hourtemps\", \"Temp\")\r\n\r\n          CP <- c(CP,\r\n                  tail(do.call(temp_model,\r\n                               list(hourtemps)), 1) /\r\n                    days_month)\r\n          mins <- c(mins, tmin)\r\n          maxs <- c(maxs, tmax)\r\n          month <- c(month, mon)\r\n        }\r\n}\r\n\r\nresults <- data.frame(Month = month,\r\n                      Tmin = mins,\r\n                      Tmax = maxs,\r\n                      CP)\r\n\r\nresults <- results[!is.na(results$Month), ]\r\n\r\n\r\nwrite.csv(results,\r\n          \"data/model_sensitivity_development.csv\",\r\n          row.names = FALSE)\r\n\r\n\r\nPlotting Results\r\nThe generated data is visualized through a plot, showing how the model reacts to different temperature combinations. The plot highlights chill effectiveness variations across temperatures, with a peak in the middle range and no accumulation at extreme temperatures.\r\n\r\n\r\n\r\n\r\n\r\nresults$Month_names <- factor(results$Month,\r\n                              levels = month_range,\r\n                              labels = month.name[month_range])  \r\n\r\nDM_sensitivity <- ggplot(results,\r\n                         aes(x = Tmin,\r\n                             y = Tmax,\r\n                             fill = CP)) +\r\n  geom_tile() +\r\n  scale_fill_gradientn(colours = alpha(matlab.like(15),\r\n                                       alpha = .5),\r\n                       name = \"Chill/day (CP)\") +\r\n  ylim(min(results$Tmax),\r\n       max(results$Tmax)) +\r\n  ylim(min(results$Tmin),\r\n       max(results$Tmin))\r\n\r\nDM_sensitivity\r\n\r\n\r\n\r\n\r\n\r\nDM_sensitivity <- DM_sensitivity +\r\n  facet_wrap(vars(Month_names)) +\r\n  ylim(min(results$Tmax),\r\n       max(results$Tmax)) +\r\n  ylim(min(results$Tmin),\r\n       max(results$Tmin))\r\n\r\nDM_sensitivity\r\n\r\n\r\n\r\nComparing Locations\r\nTo assess the model’s suitability for different climates, temperature data is overlaid on the model sensitivity plot. This enables comparison of temperature response patterns across multiple locations like Klein-Altendorf, Davis, Beijing, and Sfax.\r\n\r\n\r\ntemperatures <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  filter(Month %in% month_range) %>%\r\n  mutate(Month_names =\r\n           factor(Month,\r\n                  levels = c(10, 11, 12, 1, 2, 3),\r\n                  labels = c(\"October\", \"November\", \"December\",\r\n                             \"January\", \"February\", \"March\")))\r\n\r\ntemperatures[which(temperatures$Tmax < temperatures$Tmin),\r\n             c(\"Tmax\", \"Tmin\")] <- NA\r\n\r\nDM_sensitivity +\r\n  geom_point(data = temperatures,\r\n             aes(x = Tmin,\r\n                 y = Tmax,\r\n                 fill = NULL,\r\n                 color = \"Temperature\"),\r\n             size = 0.2) +\r\n  facet_wrap(vars(Month_names)) +\r\n  scale_color_manual(values = \"black\",\r\n                     labels = \"Daily temperature \\nextremes (°C)\",\r\n                     name = \"Observed at site\" ) +\r\n  guides(fill = guide_colorbar(order = 1),\r\n         color = guide_legend(order = 2)) +\r\n  ylab(\"Tmax (°C)\") +\r\n  xlab(\"Tmin (°C)\") + \r\n  theme_bw(base_size = 15) \r\n\r\n\r\n\r\nAutomating the Process\r\nFor efficiency, functions are created to automate model sensitivity calculations and visualizations for various locations. The results are saved to files for easy reuse, making it easier to compare responses across different locations without reprocessing data.\r\n\r\n\r\nChill_model_sensitivity <-\r\n  function(latitude,\r\n           temp_models = list(Dynamic_Model = Dynamic_Model,\r\n                              GDH = GDH),\r\n           month_range = c(10, 11, 12, 1, 2, 3),\r\n           Tmins = c(-10:20),\r\n           Tmaxs = c(-5:30))\r\n  {\r\n  mins <- NA\r\n  maxs <- NA\r\n  metrics <- as.list(rep(NA,\r\n                         length(temp_models)))\r\n  names(metrics) <- names(temp_models)\r\n  month <- NA\r\n \r\n  for(mon in month_range)\r\n    {\r\n    days_month <-\r\n      as.numeric(difftime(ISOdate(2002,\r\n                                  mon + 1,\r\n                                  1),\r\n                          ISOdate(2002,\r\n                                  mon,\r\n                                  1) ))\r\n    if(mon == 12) days_month <- 31\r\n    weather <- \r\n      make_all_day_table(data.frame(Year = c(2001, 2001),\r\n                                    Month = c(mon, mon),\r\n                                    Day = c(1, days_month),\r\n                                    Tmin = c(0, 0),\r\n                                    Tmax = c(0, 0)))\r\n\r\n    \r\n    for(tmin in Tmins)\r\n      for(tmax in Tmaxs)\r\n        if(tmax >= tmin)\r\n          {\r\n          hourtemps <- weather %>%\r\n            mutate(Tmin = tmin,\r\n                   Tmax = tmax) %>%\r\n            stack_hourly_temps(\r\n              latitude = latitude) %>%\r\n            pluck(\"hourtemps\",\r\n                  \"Temp\")\r\n          \r\n          for(tm in 1:length(temp_models))\r\n            metrics[[tm]] <- \r\n              c(metrics[[tm]],\r\n                tail(do.call(temp_models[[tm]],\r\n                        list(hourtemps)),1)/\r\n                              days_month)\r\n          \r\n          mins <- c(mins, tmin)\r\n          maxs <- c(maxs, tmax)\r\n          month <- c(month, mon)\r\n        }\r\n    }\r\n  results <- cbind(data.frame(Month = month,\r\n                              Tmin = mins,\r\n                              Tmax = maxs),\r\n                   as.data.frame(metrics))\r\n  \r\n  results <- results[!is.na(results$Month),]\r\n}\r\n\r\n\r\nChill_sensitivity_temps <-\r\n  function(chill_model_sensitivity_table,\r\n           temperatures,\r\n           temp_model,\r\n           month_range = c(10, 11, 12, 1, 2, 3),\r\n           Tmins = c(-10:20),\r\n           Tmaxs = c(-5:30),\r\n           legend_label = \"Chill/day (CP)\")\r\n{\r\n\r\n  cmst <- chill_model_sensitivity_table\r\n  cmst <- cmst[which(cmst$Month %in% month_range),]\r\n  cmst$Month_names <- factor(cmst$Month,\r\n                             levels = month_range,\r\n                             labels = month.name[month_range])  \r\n  \r\n  DM_sensitivity<-\r\n    ggplot(cmst,\r\n           aes_string(x = \"Tmin\",\r\n                      y = \"Tmax\",\r\n                      fill = temp_model)) +\r\n    geom_tile() +\r\n    scale_fill_gradientn(colours = alpha(matlab.like(15),\r\n                                         alpha = .5),\r\n                         name = legend_label) +\r\n    xlim(Tmins[1],\r\n         Tmins[length(Tmins)]) +\r\n    ylim(Tmaxs[1],\r\n         Tmaxs[length(Tmaxs)])\r\n  \r\n  temperatures<-\r\n    temperatures[which(temperatures$Month %in% month_range),]\r\n  \r\n  temperatures[which(temperatures$Tmax < temperatures$Tmin),\r\n               c(\"Tmax\", \r\n                 \"Tmin\")] <- NA\r\n  \r\n  temperatures$Month_names <-\r\n    factor(temperatures$Month,\r\n           levels = month_range,\r\n           labels = month.name[month_range])  \r\n  \r\n  DM_sensitivity +\r\n    geom_point(data = temperatures,\r\n               aes(x = Tmin,\r\n                   y = Tmax,\r\n                   fill = NULL,\r\n                   color = \"Temperature\"),\r\n               size = 0.2) +\r\n    facet_wrap(vars(Month_names)) +\r\n    scale_color_manual(values = \"black\",\r\n                       labels = \"Daily temperature \\nextremes (°C)\",\r\n                       name = \"Observed at site\" ) +\r\n    guides(fill = guide_colorbar(order = 1),\r\n           color = guide_legend(order = 2)) +\r\n    ylab(\"Tmax (°C)\") +\r\n    xlab(\"Tmin (°C)\") + \r\n    theme_bw(base_size = 15)\r\n\r\n}\r\n\r\n\r\n\r\n\r\nModel_sensitivities_CKA <-\r\n  Chill_model_sensitivity(latitude = 50,\r\n                          temp_models = list(Dynamic_Model = Dynamic_Model,\r\n                                             GDH = GDH),\r\n                          month_range = c(10:12, 1:5))\r\n\r\nwrite.csv(Model_sensitivities_CKA,\r\n          \"data/Model_sensitivities_CKA.csv\",\r\n          row.names = FALSE)\r\n\r\nModel_sensitivities_Davis <-\r\n  Chill_model_sensitivity(latitude = 38.5,\r\n                          temp_models = list(Dynamic_Model = Dynamic_Model,\r\n                                             GDH = GDH),\r\n                          month_range=c(10:12, 1:5))\r\n\r\nwrite.csv(Model_sensitivities_Davis,\r\n          \"data/Model_sensitivities_Davis.csv\",\r\n          row.names = FALSE)\r\n\r\nModel_sensitivities_Beijing <-\r\n  Chill_model_sensitivity(latitude = 39.9,\r\n                          temp_models = list(Dynamic_Model = Dynamic_Model, \r\n                                             GDH = GDH),\r\n                          month_range = c(10:12, 1:5))\r\n\r\nwrite.csv(Model_sensitivities_Beijing,\r\n          \"data/Model_sensitivities_Beijing.csv\",\r\n          row.names = FALSE)\r\n\r\nModel_sensitivities_Sfax <-\r\n  Chill_model_sensitivity(latitude = 35,\r\n                          temp_models = list(Dynamic_Model = Dynamic_Model,\r\n                                             GDH = GDH),\r\n                          month_range = c(10:12, 1:5))\r\n\r\nwrite.csv(Model_sensitivities_Sfax,\r\n          \"data/Model_sensitivities_Sfax.csv\",\r\n          row.names = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nNow, temperature patterns for specific months can be compared to the effective ranges for chill and heat accumulation at the four locations. By analyzing long-term weather data, it becomes possible to assess how well each site’s climate aligns with the conditions necessary for dormancy processes.\r\n\r\n\r\n\r\nThe analysis will begin with Beijing, the coldest location, and then progress through Klein-Altendorf, Davis, and Sfax, representing increasingly warmer growing regions. This step-by-step approach allows for a clear comparison of chill accumulation across different climates.\r\nSince Chill_sensitivity_temps outputs a ggplot object, it can be modified using the + notation in ggplot2. Titles will be added to each plot using the ggtitle function to improve clarity. Below are the generated plots for chill accumulation, quantified using the Dynamic Model, illustrating how effective temperature ranges vary across these locations.\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Beijing,\r\n                        Beijing_weather,\r\n                        temp_model = \"Dynamic_Model\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        legend_label = \"Chill per day \\n(Chill Portions)\") +\r\n  ggtitle(\"Chill model sensitivity at Beijing, China\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_CKA,\r\n                        CKA_temperatures,\r\n                        temp_model = \"Dynamic_Model\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        legend_label = \"Chill per day \\n(Chill Portions)\") +\r\n  ggtitle(\"Chill model sensitivity at Klein-Altendorf, Germany\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Davis,\r\n                        Davis_weather,\r\n                        temp_model = \"Dynamic_Model\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        legend_label = \"Chill per day \\n(Chill Portions)\") +\r\n  ggtitle(\"Chill model sensitivity at Davis, California\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Sfax,\r\n                        Sfax_weather,\r\n                        temp_model = \"Dynamic_Model\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        legend_label = \"Chill per day \\n(Chill Portions)\") +\r\n  ggtitle(\"Chill model sensitivity near Sfax, Tunisia\")\r\n\r\n\r\n\r\nBelow are the plots for heat accumulation:\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Beijing,\r\n                        Beijing_weather,\r\n                        temp_model = \"GDH\",\r\n                        month_range = c(12, 1:5),\r\n                        legend_label = \"Heat per day \\n(GDH)\") +\r\n  ggtitle(\"Heat model sensitivity at Beijing, China\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_CKA,\r\n                        CKA_temperatures,\r\n                        temp_model = \"GDH\",\r\n                        month_range = c(12, 1:5),\r\n                        legend_label = \"Heat per day \\n(GDH)\") +\r\n  ggtitle(\"Heat model sensitivity at Klein-Altendorf, Germany\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Davis,\r\n                        Davis_weather,\r\n                        temp_model = \"GDH\",\r\n                        month_range = c(12, 1:5),\r\n                        legend_label = \"Heat per day \\n(GDH)\") +\r\n  ggtitle(\"Heat model sensitivity at Davis, California\")\r\n\r\n\r\n\r\n\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Sfax,\r\n                        Sfax_weather,\r\n                        temp_model = \"GDH\",\r\n                        month_range = c(12, 1:5),\r\n                        legend_label = \"Heat per day \\n(GDH)\") +\r\n  ggtitle(\"Heat model sensitivity near Sfax, Tunisia\")\r\n\r\n\r\n\r\nNow, the response patterns will be analyzed in relation to the sensitivity of the chill model. This will help identify how different locations align with the effective temperature ranges for chill accumulation and how variations in climate influence dormancy processes.\r\nChill model sensitivity vs. observed temperature\r\nAlthough chill model sensitivity is influenced by hourly temperatures, which depend on sunrise and sunset times, the overall sensitivity patterns appear nearly identical across all locations. While minor differences exist, they are not significant enough to impact the broader interpretation.\r\nWhat does vary, however, is how observed hourly temperatures align with the sensitive periods for chill accumulation. This alignment shifts not only between locations but also across different months, highlighting the importance of local climatic conditions in determining the effectiveness of chill accumulation.\r\nBeijing, China\r\nIn Beijing, the relationship between observed temperatures and effective chill accumulation varies significantly across months:\r\nOctober: Temperatures are evenly distributed between effective chill accumulation conditions and values that are too warm. This variation may generate chill-related signals, potentially influencing bloom dates in a detectable way.\r\nNovember: Almost all temperatures fall within the highly effective chill accumulation range. Despite considerable temperature fluctuations, chill accumulation rates remain similar for most days. Without meaningful variation, PLS regression is unlikely to produce useful results.\r\nDecember–February: Temperatures drop further, leading to many hours below the effective chill accumulation range. If the Dynamic Model correctly represents chill accumulation at low temperatures (which remains uncertain), a response might still be observed.\r\nMarch: Similar to November, temperatures remain almost always optimal for chill accumulation, resulting in limited variation.\r\nSummary:\r\nBeijing’s conditions should allow for clear identification of chill effects in most months, except for November, where the lack of variation may reduce the effectiveness of PLS regression.\r\nKlein-Altendorf, Germany\r\nCompared to Beijing, Klein-Altendorf appears to be less suitable for using PLS regression to delineate the chilling period. While both locations experience a mix of optimal and suboptimal temperatures, the proportion of days with suboptimal chill conditions is much lower in Klein-Altendorf.\r\nOctober, December, January, February: Most days have near-optimal chill conditions, with only a few too warm days (mainly in October) or too cold days for effective chill accumulation.\r\nNovember and March: Almost all days fall within the optimal chill accumulation range, leading to minimal variation.\r\nSummary:\r\nDue to the lack of strong temperature fluctuations, Klein-Altendorf may not be well-suited for PLS regression analysis, as the method relies on meaningful variation to detect temperature response phases.\r\nDavis, California\r\nDavis provides more favorable conditions for PLS regression compared to the colder locations, particularly due to the distribution of daily temperatures across chill model sensitivity levels during certain months.\r\nNovember and March: Daily temperatures are evenly distributed between effective and ineffective chill accumulation conditions, creating meaningful variation that could help detect temperature response phases.\r\nFebruary: Most days have optimal chill accumulation conditions, with relatively few days experiencing low or zero chill accumulation.\r\nJanuary and December: Conditions are mostly optimal for chill accumulation, with little variation, which may limit PLS regression’s ability to detect meaningful signals.\r\nSummary:\r\nWhile Davis offers better conditions than colder locations for PLS-based analysis, it may still face limitations, particularly in months where chill accumulation remains consistently high, reducing variability in the data.\r\nSfax, Tunisia\r\nIn Sfax, temperature conditions are generally more favorable for PLS regression, particularly between December and February:\r\nDecember–February: Temperatures vary enough to create meaningful chill signals, making these months well-suited for PLS-based analysis.\r\nNovember and March: Some chill accumulation occurs, but the proportion of effective chill days is much lower compared to peak winter months.\r\nOctober: Temperatures remain consistently too high for chill accumulation, meaning no significant chill-related signal is expected.\r\nSummary:\r\nAmong the four locations, Sfax presents the best conditions for PLS regression, especially in mid-winter. However, October shows no variation, and November/March may produce only weak signals, limiting their usefulness in identifying temperature response phases.\r\nHeat model sensitivity vs. observed temperature\r\nCompared to the Dynamic Model, the Growing Degree Hours (GDH) model exhibits a much simpler temperature response. Any day with a minimum temperature above 4°C contributes to heat accumulation, with warmer temperatures leading to a stronger response.\r\nBeijing (December–May): Heat accumulation days are rare in December, January, and February, but become frequent from March onward.\r\nOther locations: Heat accumulation conditions are met in all months, making it easier to detect response phases.\r\nAcross the four sites, there are few months where PLS regression would struggle to identify heat accumulation phases. This likely explains why PLS regression has been more effective at detecting ecodormancy (heat accumulation) phases than endodormancy (chill accumulation) periods, where temperature conditions are often more stable and less variable.\r\nExercises on expected PLS responsiveness\r\nProduce chill and heat model sensitivity plots for the location you focused on in previous exercises.\r\n\r\n\r\n# Generate model sensitivity data\r\nModel_sensitivities_Yakima <-\r\n  Chill_model_sensitivity(latitude = 46.6,\r\n                          temp_models = list(Dynamic_Model = Dynamic_Model,\r\n                                             GDH = GDH),\r\n                          month_range = c(10:12, 1:5))\r\n\r\nwrite.csv(Model_sensitivities_Sfax,\r\n          \"Yakima/Model_sensitivities_Yakima.csv\",\r\n          row.names = FALSE)\r\n\r\n\r\n\r\n\r\n# Chill sensitivity model for Yakima\r\nYakima_weather <- read.csv(\"Yakima/Yakima_weather.csv\")\r\nModel_sensitivities_Yakima <- read.csv(\"Yakima/Model_sensitivities_Yakima.csv\")\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Yakima,\r\n                        Yakima_weather,\r\n                        temp_model = \"Dynamic_Model\",\r\n                        month_range = c(10, 11, 12, 1, 2, 3),\r\n                        legend_label = \"Chill per day \\n(Chill Portions)\") +\r\n  ggtitle(\"Chill model sensitivity at Yakima, USA\") \r\n\r\n\r\n\r\n\r\n\r\n# Heat sensitivity model for Yakima\r\n\r\nChill_sensitivity_temps(Model_sensitivities_Yakima,\r\n                        Yakima_weather,\r\n                        temp_model = \"GDH\",\r\n                        month_range = c(12, 1:5),\r\n                        legend_label = \"Heat per day \\n(GDH)\") +\r\n  ggtitle(\"Heat model sensitivity at Yakima, USA\") \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:53:54+01:00"
    },
    {
      "path": "PLS_regression.html",
      "title": "Delineating temperature response phases with PLS regression",
      "author": [],
      "contents": "\r\nPhenology and Temperature Analysis\r\nAttempts to correlate specific temperature periods with phenology (like bloom or leafing dates) were largely unsuccessful. This reflects the understanding that spring phenology is influenced by both chilling and heat exposure over extended periods, making single-period temperature data insufficient. The challenge is the imbalance between detailed temperature data and limited phenology observations, creating statistical issues. This is similar to challenges in remote sensing, where advanced techniques, like machine learning, are employed to analyze complex data.\r\nHyperspectral Data for Damage Detection\r\nIn California, hyperspectral data were used to assess spider mite damage in peaches. Reflectance spectra from 350 to 2500 nm showed clear differences between healthy and damaged leaves. Statistical methods, such as Partial Least Squares (PLS) regression, were used to identify significant wavelengths correlating with mite damage. While promising, mite detection was only reliable for severe damage, which could be visually observed.\r\nHyperspectral reflectance spectra of healthy (0% damage) and highly mite-affected (90% damage) peach leaves in California (Luedeling et al., 2009c)PLS Regression for Phenology Analysis\r\nPLS regression was applied to study leaf emergence in walnut cultivars in California. Using data from 1953 to 2007, the analysis correlated leaf emergence with daily temperatures. It revealed that:\r\nChilling phase (Nov-Jan): Warmer temperatures delayed emergence\r\nForcing phase (Jan-Mar): Warmer temperatures accelerated leaf emergence. For different cultivars, there were variations in temperature sensitivity, which was clearer in the ‘Payne’ cultivar.\r\nVIP scores and model coefficients of PLS models relating leaf emergence dates of the walnut cultivars ‘Payne’, ‘Chandler’ and ‘Franquette’ in Davis, California, with mean daily temperatures (Luedeling et al., 2012)Caution in Interpreting PLS Results\r\nWhile PLS regression is effective with large datasets, it requires caution when applied to smaller datasets. Results should not be overinterpreted but used to test or refine existing hypotheses. Random patterns should not be rationalized without supporting theory.\r\nPLS Regression on ‘Alexander Lucas’ Pears\r\nFor the pear cultivar ‘Alexander Lucas,’ bloom dates (first bloom) were analyzed using PLS regression:\r\n\r\n\r\nAlex_first <- read_tab(\"data/Alexander_Lucas_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\n\r\nTemperature data was also integrated:\r\n\r\n\r\nKA_temps <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  make_JDay()\r\n\r\n\r\nThe data.frame can be more complex but must adhere to the standard chillR temperature dataset structure, including columns for Year, Month, Day, Tmin, Tmax, and, in this case, JDay.\r\nWith both the temperature and phenology datasets prepared, the PLS_pheno function from the chillR package, which uses the plsr package, can be applied.\r\nThis function requires at least the temperature dataset (weather_data) and the phenology dataset (bio_data). It returns a list containing three objects: $object_type (the string ‘PLS_Temp_pheno’), $pheno (the phenology data), and $PLS_summary (the analysis results).\r\n\r\n\r\nPLS_results <- PLS_pheno(KA_temps,\r\n                         Alex_first)\r\n\r\n\r\nThe PLS_results element can be visualized using the plot_PLS function, which generates a plot of the results:\r\nTemperature response pattern of ‘Alexander Lucas’ pears in Klein-Altendorf, as determined by PLS regression and plotted with the standard function in chillRThe plot produced by the PLS_pheno function includes several elements:\r\nThe top row shows the VIP plot, with important days highlighted in blue.\r\nThe second row displays the model coefficients, with important days colored red (for negative) or green (for positive).\r\nThe bottom row illustrates the temperature dynamics, with the mean temperature across all years shown as a black line and the standard deviation indicated by shading. The same color scheme as the model coefficient plot is used. Additionally, the gray shading on the left shows the full range of observed bloom dates, with the dashed line indicating the mean bloom date.\r\nAlthough this figure looks appealing, it is difficult to customize, especially since it was created using R’s standard plotting functions, which were limited by the available programming skills at the time. Modifying text sizes and other elements required direct writing of the plot to an image file to ensure acceptable output.\r\nBefore replicating this plot using ggplot, it is helpful to understand the parameters of the PLS_pheno function:\r\nsplit_month: Defines the start of the phenological year, with the default set to 7 (ending at the end of July).\r\nrunn_mean: Controls the running mean applied to temperature data before analysis, with the default set to 11.\r\nend_at_pheno_end: If TRUE (default), temperatures after the latest Julian date in the phenology dataset are excluded from the analysis.\r\nuse_Tmean: Set to TRUE if the temperature dataset includes a Tmean column, otherwise means are calculated from Tmin and Tmax.\r\nexpl.var, ncomp.fix, and crossvalidate: Specific to the PLS analysis.\r\nreturn.all: If set to TRUE, all PLS outputs are returned.\r\nFor more details, the help function for PLS_pheno and the plsr package should be consulted. Now, the task is to reproduce this plot using ggplot.\r\n\r\n\r\nPLS_gg <- PLS_results$PLS_summary %>%\r\n  mutate(Month = trunc(Date / 100),\r\n         Day = Date - Month * 100,\r\n         Date = NULL) \r\n\r\nPLS_gg$Date <- ISOdate(2002, \r\n                       PLS_gg$Month, \r\n                       PLS_gg$Day)\r\nPLS_gg$Date[PLS_gg$JDay <= 0] <-\r\n  ISOdate(2001, \r\n          PLS_gg$Month[PLS_gg$JDay <= 0], \r\n          PLS_gg$Day[PLS_gg$JDay <= 0])\r\n\r\nPLS_gg <- PLS_gg %>%\r\n  mutate(VIP_importance = VIP >= 0.8,\r\n         VIP_Coeff = factor(sign(Coef) * VIP_importance))\r\n\r\n\r\nVIP_plot<- ggplot(PLS_gg,\r\n                  aes(x = Date,y = VIP)) +\r\n  geom_bar(stat = 'identity',\r\n           aes(fill = VIP > 0.8))\r\n\r\nVIP_plot <- VIP_plot +\r\n  scale_fill_manual(name=\"VIP\", \r\n                    labels = c(\"<0.8\", \">0.8\"), \r\n                    values = c(\"FALSE\" = \"grey\", \"TRUE\" = \"blue\")) +\r\n  theme_bw(base_size = 15) +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank())\r\n\r\nVIP_plot\r\n\r\n\r\n\r\n\r\n\r\ncoeff_plot <- ggplot(PLS_gg,\r\n                     aes(x = Date,\r\n                         y = Coef)) +\r\n  geom_bar(stat ='identity',\r\n           aes(fill = VIP_Coeff)) +\r\n  scale_fill_manual(name = \"Effect direction\", \r\n                    labels = c(\"Advancing\",\r\n                               \"Unimportant\",\r\n                               \"Delaying\"), \r\n                    values = c(\"-1\" = \"red\", \r\n                               \"0\" = \"grey\",\r\n                               \"1\" = \"dark green\")) +\r\n  theme_bw(base_size = 15) +\r\n  ylab(\"PLS coefficient\") +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank() )\r\n\r\ncoeff_plot\r\n\r\n\r\n\r\n\r\n\r\ntemp_plot <- ggplot(PLS_gg) +\r\n  geom_ribbon(aes(x = Date,\r\n                  ymin = Tmean - Tstdev,\r\n                  ymax = Tmean + Tstdev),\r\n              fill = \"grey\") +\r\n  geom_ribbon(aes(x = Date,\r\n                  ymin = Tmean - Tstdev * (VIP_Coeff == -1),\r\n                  ymax = Tmean + Tstdev * (VIP_Coeff == -1)),\r\n              fill = \"red\") +\r\n  geom_ribbon(aes(x = Date,\r\n                  ymin = Tmean - Tstdev * (VIP_Coeff == 1),\r\n                  ymax = Tmean + Tstdev * (VIP_Coeff == 1)),\r\n              fill = \"dark green\") +\r\n  geom_line(aes(x = Date,\r\n                y = Tmean)) +\r\n  theme_bw(base_size = 15) +\r\n  ylab(expression(paste(T[mean],\" (°C)\")))\r\n\r\ntemp_plot\r\n\r\n\r\n\r\n\r\n\r\nplot<- (VIP_plot +\r\n          coeff_plot +\r\n          temp_plot +\r\n          plot_layout(ncol=1,\r\n            guides = \"collect\")\r\n        ) & theme(legend.position = \"right\",\r\n                  legend.text = element_text(size = 8),\r\n                  legend.title = element_text(size = 10),\r\n                  axis.title.x = element_blank())\r\n\r\nplot\r\n\r\n\r\n\r\nThe code now generates a clear compound figure that displays all the expected outputs of a PLS analysis. Creating a function from this would make the process more efficient and reusable.\r\n\r\n\r\nggplot_PLS <- function(PLS_results)\r\n{\r\n  library(ggplot2)\r\n  PLS_gg <- PLS_results$PLS_summary %>%\r\n    mutate(Month = trunc(Date / 100),\r\n           Day = Date - Month * 100,\r\n           Date = NULL) \r\n  \r\n  PLS_gg$Date <- ISOdate(2002, \r\n                         PLS_gg$Month, \r\n                         PLS_gg$Day)\r\n  \r\n  PLS_gg$Date[PLS_gg$JDay <= 0] <-\r\n    ISOdate(2001, \r\n            PLS_gg$Month[PLS_gg$JDay <= 0], \r\n            PLS_gg$Day[PLS_gg$JDay <= 0])\r\n  \r\n  PLS_gg <- PLS_gg %>%\r\n    mutate(VIP_importance = VIP >= 0.8,\r\n           VIP_Coeff = factor(sign(Coef) * VIP_importance))\r\n  \r\n  VIP_plot<- ggplot(PLS_gg,aes(x=Date,y=VIP)) +\r\n    geom_bar(stat='identity',aes(fill=VIP>0.8))\r\n  \r\n  VIP_plot <- VIP_plot +\r\n    scale_fill_manual(name=\"VIP\", \r\n                      labels = c(\"<0.8\", \">0.8\"), \r\n                      values = c(\"FALSE\" = \"grey\", \r\n                                 \"TRUE\" = \"blue\")) +\r\n    theme_bw(base_size=15) +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank())\r\n  \r\n  coeff_plot <- ggplot(PLS_gg,\r\n                       aes(x = Date,\r\n                           y = Coef)) +\r\n    geom_bar(stat ='identity',\r\n             aes(fill = VIP_Coeff)) +\r\n    scale_fill_manual(name = \"Effect direction\", \r\n                      labels = c(\"Advancing\",\r\n                                 \"Unimportant\",\r\n                                 \"Delaying\"), \r\n                      values = c(\"-1\" = \"red\", \r\n                                 \"0\" = \"grey\",\r\n                                 \"1\" = \"dark green\")) +\r\n    theme_bw(base_size = 15) +\r\n    ylab(\"PLS coefficient\") +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank())\r\n  \r\n  temp_plot <- ggplot(PLS_gg) +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = Tmean - Tstdev,\r\n                    ymax = Tmean + Tstdev),\r\n                fill = \"grey\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = Tmean - Tstdev * (VIP_Coeff == -1),\r\n                    ymax = Tmean + Tstdev * (VIP_Coeff == -1)),\r\n                fill = \"red\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = Tmean - Tstdev * (VIP_Coeff == 1),\r\n                    ymax = Tmean + Tstdev * (VIP_Coeff == 1)),\r\n                fill = \"dark green\") +\r\n    geom_line(aes(x = Date,\r\n                  y = Tmean)) +\r\n    theme_bw(base_size = 15) +\r\n    ylab(expression(paste(T[mean],\" (°C)\")))\r\n  \r\n  library(patchwork)\r\n  \r\n  plot<- (VIP_plot +\r\n            coeff_plot +\r\n            temp_plot +\r\n            plot_layout(ncol=1,\r\n                        guides = \"collect\")\r\n          ) & theme(legend.position = \"right\",\r\n                    legend.text = element_text(size = 8),\r\n                    legend.title = element_text(size = 10),\r\n                    axis.title.x = element_blank())\r\n  \r\n  plot}\r\n\r\nggplot_PLS(PLS_results)\r\n\r\n\r\n\r\nA function has now been created to automatically plot the outputs of a PLS analysis. However, the results don’t match expectations. While there is a clear bloom-advancing response to high temperatures in spring, the chilling period is absent. This will be addressed in the next lesson, but possible explanations might already be forming.\r\nExercises on chill model comparison\r\nBriefly explain why you shouldn’t take the results of a PLS regression analysis between temperature and phenology at face value. What do you need in addition in order to make sense of such outputs?\r\nPLS regression results between temperature and phenology should not be taken at face value because they are heavily influenced by the specific dataset and the assumptions underlying the model. The results need to be interpreted in the context of a theory or understanding of the biological processes involved, such as the chilling and forcing phases in phenology. Without this theoretical framework, the patterns found in the data could be misleading or overinterpreted. Additionally, the small sample sizes often used in phenology datasets can lead to unstable or unclear results. Hence, theoretical context and caution in interpretation are essential.\r\nReplicate the PLS analysis for the Roter Boskoop dataset that you used in a previous lesson.\r\n\r\n\r\n# Load dataset 'Roter Boskoop' and convert it into a long format \r\nRoter_Boskoop <- read_tab(\"data/Roter_Boskoop_bloom_1958_2019.csv\")\r\n\r\nRoter_Boskoop <- pivot_longer(Roter_Boskoop,\r\n                     cols = c(First_bloom:Last_bloom),\r\n                     names_to = \"Stage\",\r\n                     values_to = \"YEARMODA\")\r\n\r\nRB_first <- Roter_Boskoop %>%\r\n  mutate(Year = as.numeric(substr(YEARMODA, 1, 4)),\r\n         Month = as.numeric(substr(YEARMODA, 5, 6)),\r\n         Day = as.numeric(substr(YEARMODA, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  filter(Stage == \"First_bloom\")\r\n\r\nRB_first <- read_tab(\"data/Roter_Boskoop_bloom_1958_2019.csv\") %>%\r\n  select(Pheno_year, First_bloom) %>%\r\n  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),\r\n         Month = as.numeric(substr(First_bloom, 5, 6)),\r\n         Day = as.numeric(substr(First_bloom, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  select(Pheno_year, JDay) %>%\r\n  rename(Year = Pheno_year,\r\n         pheno = JDay)\r\n\r\n\r\n\r\n\r\n# Results of PLS analysis\r\nPLS_results <- PLS_pheno(KA_temps,\r\n                         RB_first)\r\n\r\n\r\n\r\n\r\n# Plot results of PLS analysis \r\nggplot_PLS <- function(PLS_results)\r\n{\r\n  library(ggplot2)\r\n  PLS_gg <- PLS_results$PLS_summary %>%\r\n    mutate(Month = trunc(Date / 100),\r\n           Day = Date - Month * 100,\r\n           Date = NULL) \r\n  \r\n  PLS_gg$Date <- ISOdate(2002, \r\n                         PLS_gg$Month, \r\n                         PLS_gg$Day)\r\n  \r\n  PLS_gg$Date[PLS_gg$JDay <= 0] <-\r\n    ISOdate(2001, \r\n            PLS_gg$Month[PLS_gg$JDay <= 0], \r\n            PLS_gg$Day[PLS_gg$JDay <= 0])\r\n  \r\n  PLS_gg <- PLS_gg %>%\r\n    mutate(VIP_importance = VIP >= 0.8,\r\n           VIP_Coeff = factor(sign(Coef) * VIP_importance))\r\n  \r\n  VIP_plot<- ggplot(PLS_gg,aes(x=Date,y=VIP)) +\r\n    geom_bar(stat='identity',aes(fill=VIP>0.8))\r\n  \r\n  VIP_plot <- VIP_plot +\r\n    scale_fill_manual(name=\"VIP\", \r\n                      labels = c(\"<0.8\", \">0.8\"), \r\n                      values = c(\"FALSE\" = \"grey\", \r\n                                 \"TRUE\" = \"blue\")) +\r\n    theme_bw(base_size=15) +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank())\r\n  \r\n  coeff_plot <- ggplot(PLS_gg,\r\n                       aes(x = Date,\r\n                           y = Coef)) +\r\n    geom_bar(stat ='identity',\r\n             aes(fill = VIP_Coeff)) +\r\n    scale_fill_manual(name = \"Effect direction\", \r\n                      labels = c(\"Advancing\",\r\n                                 \"Unimportant\",\r\n                                 \"Delaying\"), \r\n                      values = c(\"-1\" = \"red\", \r\n                                 \"0\" = \"grey\",\r\n                                 \"1\" = \"dark green\")) +\r\n    theme_bw(base_size = 15) +\r\n    ylab(\"PLS coefficient\") +\r\n    theme(axis.text.x = element_blank(),\r\n          axis.ticks.x = element_blank(),\r\n          axis.title.x = element_blank())\r\n  \r\n  temp_plot <- ggplot(PLS_gg) +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = Tmean - Tstdev,\r\n                    ymax = Tmean + Tstdev),\r\n                fill = \"grey\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = Tmean - Tstdev * (VIP_Coeff == -1),\r\n                    ymax = Tmean + Tstdev * (VIP_Coeff == -1)),\r\n                fill = \"red\") +\r\n    geom_ribbon(aes(x = Date,\r\n                    ymin = Tmean - Tstdev * (VIP_Coeff == 1),\r\n                    ymax = Tmean + Tstdev * (VIP_Coeff == 1)),\r\n                fill = \"dark green\") +\r\n    geom_line(aes(x = Date,\r\n                  y = Tmean)) +\r\n    theme_bw(base_size = 15) +\r\n    ylab(expression(paste(T[mean],\" (°C)\")))\r\n  \r\n  library(patchwork)\r\n  \r\n  plot<- (VIP_plot +\r\n            coeff_plot +\r\n            temp_plot +\r\n            plot_layout(ncol=1,\r\n                        guides = \"collect\")\r\n          ) & theme(legend.position = \"right\",\r\n                    legend.text = element_text(size = 8),\r\n                    legend.title = element_text(size = 10),\r\n                    axis.title.x = element_blank())\r\n  \r\n  plot}\r\n\r\nggplot_PLS(PLS_results)\r\n\r\n\r\n\r\nWrite down your thoughts on why we’re not seeing the temperature response pattern we may have expected. What happened to the chill response?\r\nThe absence of the expected chill response might be due to several factors:\r\nData Limitations: Small sample sizes and potential overfitting in PLS regression could obscure the chill phase\r\nTemperature Representation: The temperature data may not adequately reflect the chilling period or might focus too much on the spring temperatures (forcing phase)\r\nChill Requirements: Some cultivars may have specific chilling requirements not captured by average temperatures or irregular temperature fluctuation\r\nOther Environmental Factors: Additional factors like soil moisture and photoperiod might influence phenology, potentially overshadowing the chill response.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:54:35+01:00"
    },
    {
      "path": "save.html",
      "title": "Saving and loading data",
      "author": [],
      "contents": "\r\nThe compilation time increases with more data processing, especially with a large dataset (100 years of hourly weather data). Each hourly calculation requires extensive computations, and scenario analysis for climate change will add even more complexity. To handle this, saving results for faster reloading is essential.\r\nSaving and Loading Data R provides the save and load functions, but simpler formats like CSV are preferred for easy inspection. Here’s how to save and load data using CSV:\r\nSaving Data\r\n\r\n\r\n# Save Temperatures dataset as CSV\r\nwrite.csv(Temperatures, file = \"Yakima/Temperatures.csv\", row.names = FALSE)\r\n\r\n\r\nLoading Data\r\n\r\n\r\n# Load data using chillR's read_tab function for compatibility across regions\r\nTemperatures <- read_tab(\"Yakima/Temperatures.csv\")\r\n\r\n\r\nread_tab is preferred over read.csv because it automatically handles regional differences (e.g., commas vs. semicolons as delimiters).\r\nSaving and Loading Complex Data\r\nFor more complex objects (like lists of data frames), use chillR functions:\r\n\r\n\r\n# Save a list with multiple elements\r\ntest_list <- list(Number = 1, \r\n                  String = \"Thanks for using chillR!\", \r\n                  DataFrame = data.frame(a = c(1, 2, 3)))\r\n\r\nsave_temperature_scenarios(test_list, \r\n                           path = \"data\", \r\n                           prefix = \"test_list\")\r\n\r\n\r\nThis creates multiple files for each list element:\r\ntest_list_1_Number.csv\r\ntest_list_2_String.csv\r\ntest_list_3_DataFrame.csv\r\nTo reload the list:\r\n\r\n\r\ntest_list <- load_temperature_scenarios(path = \"data\", prefix = \"test_list\")\r\n\r\n\r\nOptimizing Markdown Document Execution\r\nTo avoid rerunning time-intensive calculations, save results and reload them during document knitting. Control code visibility and execution with these options:\r\necho = FALSE: Hide code but run it.\r\neval = FALSE: Show code but don’t run it.\r\ninclude = FALSE: Hide both code and output but run it.\r\nmessage = FALSE: Hide messages.\r\nwarning = FALSE: Hide warnings.\r\nThese options allow preloading necessary data for later code chunks without re-executing time-consuming tasks.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:54:47+01:00"
    },
    {
      "path": "simple_phenology.html",
      "title": "Simple phenology analysis",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nIn this section the focus shifts to a detailed phenology analysis, using the bloom dates of the pear cultivar ‘Alexander Lucas’ as an example. This dataset was previously examined in the frost analysis and includes a time series (1958–2019) of first, full, and last bloom dates recorded at Campus Klein-Altendorf.\r\nFor this analysis, only first bloom dates will be used. If working on a personal computer, the dataset can be downloaded from the provided link.\r\nData Reading and Preparation\r\nThe first step is to load the dataset from a CSV file. The read_tab function is used to handle potential issues with delimiters (such as commas or semicolons).\r\n\r\n\r\nAlex <- read_tab(\"data/Alexander_Lucas_bloom_1958_2019.csv\")\r\n\r\n\r\nOnce the data is loaded, it is prepared for analysis. The pivot_longer function transforms the data from wide to long format so that bloom stages are consolidated into a single column.\r\n\r\n\r\nAlex <- pivot_longer(Alex,\r\n                     cols = c(First_bloom:Last_bloom),\r\n                     names_to = \"Stage\",\r\n                     values_to = \"YEARMODA\")\r\n\r\nAlex_first <- Alex %>%\r\n  mutate(Year = as.numeric(substr(YEARMODA, 1, 4)),\r\n         Month = as.numeric(substr(YEARMODA, 5, 6)),\r\n         Day = as.numeric(substr(YEARMODA, 7, 8))) %>%\r\n  make_JDay() %>%\r\n  filter(Stage == \"First_bloom\")\r\n\r\n\r\nThis process extracts the year, month, and day from the YEARMODA column, converts them into a Julian day (JDay), and filters the data for the first bloom.\r\nTime Series Analysis\r\nThe first step in analyzing the phenology is visualizing the first bloom dates over the years to identify any trends or changes over time.\r\nVisualization of First Bloom Dates\r\nA scatter plot is used to visualize the first bloom dates across years:\r\n\r\n\r\nggplot(Alex_first,\r\n       aes(Pheno_year,\r\n           JDay)) +\r\n  geom_point() +\r\n  ylab(\"First bloom date (day of the year)\") +\r\n  xlab (\"Year\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nAt first glance, the plot does not reveal a clear pattern or trend. To check for any underlying trend, the Kendall trend test is applied to statistically assess the presence of a trend.\r\nKendall Trend Test\r\nThe Kendall test checks whether there is a statistically significant trend in the bloom dates over the years:\r\n\r\n\r\nlibrary(Kendall)\r\nKendall(x = Alex_first$Pheno_year,\r\n        y = Alex_first$JDay)\r\n\r\ntau = -0.186, 2-sided pvalue =0.03533\r\n\r\nThe p-value of 0.035 and the negative Tau value suggest that there is a significant trend toward earlier blooming. This indicates that the bloom dates have been shifting earlier over time.\r\nLinear Regression for Trend Analysis\r\nTo quantify the trend, a linear regression is fitted, and the trend is visualized with a regression line:\r\n\r\n\r\nx <- Alex_first$Pheno_year\r\ny <- Alex_first$JDay\r\n\r\nsummary(lm(y ~ x))\r\n\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-30.0959  -6.3591  -0.5959   6.6468  20.1238 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept) 429.16615  142.06000   3.021   0.0037 **\r\nx            -0.16184    0.07144  -2.266   0.0271 * \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 10.07 on 60 degrees of freedom\r\nMultiple R-squared:  0.0788,    Adjusted R-squared:  0.06345 \r\nF-statistic: 5.133 on 1 and 60 DF,  p-value: 0.0271\r\n\r\n\r\n\r\nggplot(Alex_first,\r\n       aes(Year,\r\n           JDay)) +\r\n  geom_point() +\r\n  geom_smooth(method = 'lm',\r\n              formula = y ~ x) +\r\n  ylab(\"First bloom date (day of the year)\") +\r\n  xlab (\"Year\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nThe estimated slope of -0.16 indicates that, on average, the bloom dates are occurring 0.16 days earlier each year.\r\nPolynomial Regression for a Complex Model\r\nA linear regression is a simple approach, but in many cases, a more complex model might be necessary. Here, a 25th-degree polynomial is used to improve the model fit.\r\n\r\n\r\nsummary(lm(y ~ poly(x, 25)))\r\n\r\n\r\nCall:\r\nlm(formula = y ~ poly(x, 25))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-13.7311  -4.5098  -0.1227   2.8640  15.4590 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept)   107.3387     1.0549 101.753 < 0.0000000000000002 ***\r\npoly(x, 25)1  -22.8054     8.3063  -2.746              0.00937 ** \r\npoly(x, 25)2   -5.8672     8.3063  -0.706              0.48451    \r\npoly(x, 25)3   14.7725     8.3063   1.778              0.08377 .  \r\npoly(x, 25)4   -5.3974     8.3063  -0.650              0.51995    \r\npoly(x, 25)5  -11.6801     8.3063  -1.406              0.16825    \r\npoly(x, 25)6    2.1928     8.3063   0.264              0.79329    \r\npoly(x, 25)7   -0.3034     8.3063  -0.037              0.97107    \r\npoly(x, 25)8    6.0115     8.3063   0.724              0.47391    \r\npoly(x, 25)9  -22.2895     8.3063  -2.683              0.01094 *  \r\npoly(x, 25)10   5.9522     8.3063   0.717              0.47825    \r\npoly(x, 25)11  -6.1217     8.3063  -0.737              0.46590    \r\npoly(x, 25)12   3.2676     8.3063   0.393              0.69636    \r\npoly(x, 25)13 -14.8467     8.3063  -1.787              0.08229 .  \r\npoly(x, 25)14  13.5180     8.3063   1.627              0.11237    \r\npoly(x, 25)15  10.1544     8.3063   1.222              0.22946    \r\npoly(x, 25)16 -12.6116     8.3063  -1.518              0.13767    \r\npoly(x, 25)17  -1.3315     8.3063  -0.160              0.87354    \r\npoly(x, 25)18  -6.3438     8.3063  -0.764              0.45000    \r\npoly(x, 25)19  14.9753     8.3063   1.803              0.07978 .  \r\npoly(x, 25)20   3.4573     8.3063   0.416              0.67972    \r\npoly(x, 25)21 -29.1997     8.3063  -3.515              0.00121 ** \r\npoly(x, 25)22  10.4145     8.3063   1.254              0.21799    \r\npoly(x, 25)23   2.9898     8.3063   0.360              0.72100    \r\npoly(x, 25)24 -14.3045     8.3063  -1.722              0.09363 .  \r\npoly(x, 25)25 -20.9324     8.3063  -2.520              0.01631 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 8.306 on 36 degrees of freedom\r\nMultiple R-squared:  0.6237,    Adjusted R-squared:  0.3623 \r\nF-statistic: 2.386 on 25 and 36 DF,  p-value: 0.008421\r\n\r\n\r\n\r\nggplot(Alex_first,\r\n       aes(Year,\r\n           JDay)) +\r\n  geom_point() +\r\n  geom_smooth(method='lm',\r\n              formula = y ~ poly(x, 25)) +\r\n  ylab(\"First bloom date (day of the year)\") +\r\n  xlab (\"Year\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nThe 25th-degree polynomial fits the data very well, but this is an example of overfitting, where the model becomes too complex and captures random fluctuations in the data instead of real underlying patterns.\r\nOverfitting\r\nAn overly complex model that fits the data too well can be problematic because it may capture random noise instead of genuine relationships. While a good model typically has low error, overfitting makes it difficult to generalize the model to new data.\r\np-Hacking\r\np-hacking refers to the practice of searching through large datasets to find random correlations and then presenting them as statistically significant. This practice leads to false discoveries and is considered poor scientific practice.\r\nEcological Theory\r\nTo understand phenomena like bloom dates better, it is crucial to consider the underlying ecological processes. A basic ecological theory suggests that temperature affects phenology.\r\nCausal Diagram: Time → Temperature → Phenology\r\nThe direct influence of time on phenology might be misleading, as it is not time itself that drives the bloom dates but rather changes in temperature, which are influenced by climate change:\r\nTime → Greenhouse Gas Concentrations → Climate Forcing → Temperature\r\nTemperature Correlations\r\nTo better understand the relationship between temperature and bloom dates, weather data is included. Annual average temperatures are calculated:\r\n\r\n\r\ntemperature <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\n\r\nTmin <- temperature %>%\r\n  group_by(Year) %>%\r\n  summarise(Tmin = mean(Tmin))\r\n\r\nTmax <- temperature %>%\r\n  group_by(Year) %>% \r\n  summarise(Tmax = mean(Tmax))\r\n\r\nAnnual_means <- Tmin %>%\r\n  cbind(Tmax[,2]) %>%\r\n  mutate(Tmean = (Tmin + Tmax)/2)\r\n\r\nAnnual_means <- merge(Annual_means,\r\n                      Alex_first)\r\n\r\nAnnual_means_longer <- Annual_means[,c(1:4,10)] %>%\r\n  pivot_longer(cols = c(Tmin:Tmean),\r\n               names_to = \"Variable\",\r\n               values_to = \"Temp\")\r\n\r\n\r\nThese temperature data are then combined with bloom data to create a plot showing the relationship between temperature and first bloom dates.\r\nVisualizing Temperature and Bloom Date\r\n\r\n\r\nggplot(Annual_means_longer,\r\n       aes(x=Temp,\r\n           y=JDay)) + \r\n  geom_point() +\r\n  geom_smooth(method=\"lm\",\r\n              formula=y~x) + \r\n  facet_wrap(\"Variable\")\r\n\r\n\r\n\r\nThe linear regression indicates that there is a correlation between temperature and bloom date, though this correlation could be driven by climate change rather than just temperature itself.\r\nRegression Analysis for Temperature\r\nTo analyze the influence of different temperature variables (Tmin, Tmax, Tmean) on the bloom date, a linear regression is performed for each:\r\n\r\n\r\nsummary(lm(Annual_means$JDay ~ Annual_means$Tmin))\r\n\r\n\r\nCall:\r\nlm(formula = Annual_means$JDay ~ Annual_means$Tmin)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-25.4960  -6.9227  -0.0472   6.9066  18.4940 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value             Pr(>|t|)\r\n(Intercept)        140.288      8.610  16.293 < 0.0000000000000002\r\nAnnual_means$Tmin   -6.020      1.558  -3.864             0.000277\r\n                     \r\n(Intercept)       ***\r\nAnnual_means$Tmin ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 9.385 on 60 degrees of freedom\r\nMultiple R-squared:  0.1992,    Adjusted R-squared:  0.1859 \r\nF-statistic: 14.93 on 1 and 60 DF,  p-value: 0.0002765\r\n\r\n\r\n\r\nsummary(lm(Annual_means$JDay ~ Annual_means$Tmax))\r\n\r\n\r\nCall:\r\nlm(formula = Annual_means$JDay ~ Annual_means$Tmax)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-28.2420  -5.7340   0.3032   5.8515  19.4918 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value             Pr(>|t|)\r\n(Intercept)       168.5020    12.9573  13.004 < 0.0000000000000002\r\nAnnual_means$Tmax  -4.3586     0.9198  -4.739            0.0000136\r\n                     \r\n(Intercept)       ***\r\nAnnual_means$Tmax ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 8.947 on 60 degrees of freedom\r\nMultiple R-squared:  0.2723,    Adjusted R-squared:  0.2602 \r\nF-statistic: 22.45 on 1 and 60 DF,  p-value: 0.00001363\r\n\r\n\r\n\r\nsummary(lm(Annual_means$JDay ~ Annual_means$Tmean))\r\n\r\n\r\nCall:\r\nlm(formula = Annual_means$JDay ~ Annual_means$Tmean)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-25.9808  -5.5032   0.3793   6.1267  18.1822 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value             Pr(>|t|)\r\n(Intercept)         173.467     12.379  14.013 < 0.0000000000000002\r\nAnnual_means$Tmean   -6.780      1.264  -5.363           0.00000138\r\n                      \r\n(Intercept)        ***\r\nAnnual_means$Tmean ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 8.623 on 60 degrees of freedom\r\nMultiple R-squared:  0.324, Adjusted R-squared:  0.3128 \r\nF-statistic: 28.76 on 1 and 60 DF,  p-value: 0.000001381\r\n\r\nFunction for Temperature and Bloom Date Correlations\r\nTo better capture the effect of temperature on bloom dates, a function is developed to compute correlations over different periods.\r\n\r\n\r\ntemps_JDays <-\r\n  make_JDay(temperature)\r\n\r\ncorr_temp_pheno <- function(start_JDay, # the start JDay of the period\r\n                            end_JDay, # the start JDay of the period\r\n                            temps_JDay = temps_JDays, # the temperature dataset\r\n                            bloom = Alex_first) # a data.frame with bloom dates\r\n{\r\n  temps_JDay <- temps_JDay %>%\r\n    mutate(Season = Year)\r\n  \r\n  if(start_JDay > end_JDay)\r\n    temps_JDay$Season[temps_JDay$JDay >= start_JDay]<-\r\n      temps_JDay$Year[temps_JDay$JDay >= start_JDay]+1\r\n  \r\n  if(start_JDay > end_JDay)\r\n    sub_temps <- subset(temps_JDay,\r\n                        JDay <= end_JDay | JDay >= start_JDay)\r\n  \r\n  if(start_JDay <= end_JDay) \r\n    sub_temps <- subset(temps_JDay,\r\n                        JDay <= end_JDay & JDay >= start_JDay)\r\n  \r\n  mean_temps <- sub_temps %>%\r\n    group_by(Season) %>%\r\n    summarise(Tmin = mean(Tmin),\r\n              Tmax = mean(Tmax)) %>%\r\n    mutate(Tmean = (Tmin + Tmax)/2)\r\n  \r\n  colnames(mean_temps)[1] <- c(\"Pheno_year\")\r\n  \r\n  temps_bloom <- merge(mean_temps,\r\n                       bloom[c(\"Pheno_year\",\r\n                               \"JDay\")])\r\n  \r\n  # Let's just extract the slopes of the regression model for now\r\n  slope_Tmin <- summary(lm(temps_bloom$JDay~temps_bloom$Tmin))$coefficients[2,1]\r\n  slope_Tmean <- summary(lm(temps_bloom$JDay~temps_bloom$Tmean))$coefficients[2,1]\r\n  slope_Tmax <- summary(lm(temps_bloom$JDay~temps_bloom$Tmax))$coefficients[2,1]\r\n  \r\n  c(start_JDay = start_JDay,\r\n    end_JDay = end_JDay,\r\n    length = length(unique(sub_temps$JDay)),\r\n    slope_Tmin = slope_Tmin,\r\n    slope_Tmean = slope_Tmean,\r\n    slope_Tmax = slope_Tmax)\r\n}\r\n\r\n\r\nCalculating Correlations for Specific Periods\r\nThe function is applied to specific periods:\r\n\r\n\r\ncorr_temp_pheno(start_JDay = 305,\r\n                end_JDay = 45,\r\n                temps_JDay = temps_JDays,\r\n                bloom = Alex_first)\r\n\r\n start_JDay    end_JDay      length  slope_Tmin slope_Tmean \r\n 305.000000   45.000000  107.000000   -2.254426   -2.655599 \r\n slope_Tmax \r\n  -2.799758 \r\n\r\n\r\n\r\ncorr_temp_pheno(start_JDay = 305,\r\n                end_JDay = 29,\r\n                temps_JDay = temps_JDays,\r\n                bloom = Alex_first)\r\n\r\n start_JDay    end_JDay      length  slope_Tmin slope_Tmean \r\n 305.000000   29.000000   91.000000   -1.821312   -2.135616 \r\n slope_Tmax \r\n  -2.237499 \r\n\r\nThe function can now be applied to various reasonable day ranges. Instead of testing all possible combinations, only every 5th start and end date will be used to balance computational efficiency with thorough analysis.\r\n\r\n\r\nlibrary(colorRamps) \r\n\r\nstJDs <- seq(from = 1,\r\n             to = 366,\r\n             by = 10)\r\n\r\neJDs <- seq(from = 1,\r\n            to = 366,\r\n            by = 10)\r\n\r\nfor(stJD in stJDs)\r\n  for(eJD in eJDs)\r\n    {correlations <- corr_temp_pheno(stJD,\r\n                                     eJD)\r\n    \r\n    if(stJD == 1 & eJD == 1)\r\n      corrs <- correlations else\r\n        corrs <- rbind(corrs, correlations)\r\n}\r\n\r\n\r\nslopes <- as.data.frame(corrs) %>%\r\n  rename(Tmin = slope_Tmin,\r\n         Tmax = slope_Tmax,\r\n         Tmean = slope_Tmean) %>%\r\n  pivot_longer(cols = c(Tmin : Tmax),\r\n               values_to = \"Slope\",\r\n               names_to = \"Variable\")\r\n\r\n\r\nPlotting Correlations\r\nThe correlations between temperature and bloom dates for different periods are visualized:\r\n\r\n\r\nggplot(data = slopes,\r\n       aes(x = start_JDay,\r\n           y = length,\r\n           fill = Slope)) +\r\n  geom_tile() +\r\n  facet_wrap(vars(Variable)) +\r\n  scale_fill_gradientn(colours = matlab.like(15)) +\r\n  ylab(\"Interval duration (days)\") + \r\n  xlab(\"Start date of temperature summary interval (Day of year)\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nExercises on simple phenology analysis\r\nProvide a brief narrative describing what p-hacking is, and why this is a problematic approach to data analysis.\r\nP-hacking is the practice of manipulating data analysis to achieve statistically significant results, often by testing multiple hypotheses or adjusting methods until a low p-value appears. This increases the risk of false positives, leading to misleading conclusions and poor reproducibility. To avoid this, researchers should predefine hypotheses, apply proper statistical corrections, and ensure transparency.\r\nProvide a sketch of your causal understanding of the relationship between temperature and bloom dates.\r\nA simplified causal diagram for the relationship between temperature and bloom dates is:\r\nTemp_chilling → Chill accumulation → Temp_forcing → Heat accumulation → Bloom Date\r\nTemp_chilling: Cold temperatures during winter contribute to chill accumulation.\r\nChill accumulation: Trees require a certain amount of chilling to end dormancy.\r\nTemp_forcing: Warmer temperatures in spring promote heat accumulation.\r\nHeat accumulation: Once enough heat is accumulated, the tree initiates blooming.\r\nBloom Date: The final outcome, determined by the balance of chilling and forcing.\r\nWhat do we need to know to build a process-based model from this?\r\nA process-based model for bloom timing requires:\r\nChilling & Forcing: Defining cold and warm periods, selecting appropriate models, and determining temperature thresholds.\r\nTemperature Response: Understanding how trees react to temperature changes and how chilling and forcing interact.\r\nData for Calibration: Historical bloom records, temperature data, and experimental studies for validation. By parameterizing and testing, the model can be optimized for accurate predictions.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:56:40+01:00"
    },
    {
      "path": "successes_and_limitations.html",
      "title": "Successes and limitations of PLS regression analysis",
      "author": [],
      "contents": "\r\nPLS regression\r\nPartial Least Squares (PLS) regression is a statistical method used to correlate high-resolution temperature data (e.g., daily temperatures) with lower-resolution phenological events (e.g., annual bloom times). In the phenology analysis at Klein-Altendorf, PLS regression successfully identified the forcing period (when warm temperatures accelerate bloom) for pears but failed to detect the chilling phase. This contrasts with a study on walnut leaf emergence in California, where both dormancy phases were clearly visible. Further examples illustrate where and why PLS regression is effective.\r\nPLS examples\r\nGrasslands on the Tibetan Plateau\r\nAn early application of PLS regression examined how grasslands on the Tibetan Plateau respond to temperature changes, particularly the start of the growing season under climate change. Initially, there was a clear trend of earlier growth until the late 1990s, followed by an unexpected delay in green-up dates.\r\nBeginning of the growing season (BGS) for steppe (A) and meadow (B) vegetation on the Tibetan Plateau between 1982 and 2006, derived from 15-day NDVI composites obtained from the Advanced Very High Resolution Radiometer (AVHRR) sensorBetween 1982 and 2006, PLS regression identified significant influences of both winter (October–March) and spring (May–June) temperatures on the beginning of the growing season (BGS). The Variable Importance in Projection (VIP) scores highlighted these periods as crucial (VIP > 0.8). Model coefficients indicated that warm winters delayed spring phenology, whereas warm spring temperatures advanced BGS.\r\nResponse of the BGS (A–D) in steppe and meadow vegetation of the Tibetan Plateau between 1982 and 2006 to monthly temperatures, according to PLS regressionA spatial analysis further examined these temperature responses at a pixel-by-pixel scale:\r\nCorrelations of monthly temperatures (left) and precipitation (right) with the beginning of the growing season (BGS) on the Tibetan Plateau, according to Partial Least Squares (PLS) regressionThese findings resemble those observed in California walnuts, though the underlying mechanisms may differ. Surprisingly, increasing temperatures have not consistently led to an earlier growing season, suggesting a mismatch between current ecosystems and future climate conditions. This misalignment could threaten sustainability and create opportunities for invasive species better adapted to new thermal conditions.\r\nDeciduous Trees\r\nPLS regression has also been applied to tree phenology, notably in collaboration with Guo Liang, a researcher at Northwest A&F University in China. His analysis of Chinese chestnut (Castanea mollissima) in Beijing identified a clear forcing period with consistent negative model coefficients from January to May. However, the chilling period was only partially visible, with inconsistent model coefficients.\r\nResults of Partial Least Squares (PLS) regression correlating first flowering dates for chestnut at BeijingA similar pattern emerged in an analysis of cherry bloom dates from Klein-Altendorf, where the forcing phase was clearly defined, but the chilling phase remained difficult to pinpoint.\r\nResults of Partial Least Squares (PLS) regression of bloom dates for cv. ‘Schneiders späte Knorpelkirsche’ cherries in Klein-Altendorf, GermanyThis recurring pattern—clear forcing but an obscured chilling phase—contrasts with earlier studies in California, where both phases were more distinct.\r\nResults of a PLS analysis relating leaf emergence dates of ‘Payne’ walnuts in California to mean daily temperatureChallenges in Detecting the Chilling Phase\r\nThe absence of the chilling phase in PLS regression does not imply that the method is ineffective. Instead, it highlights that PLS is sensitive to monotonic relationships, where increases in temperature or another signal directly correlate with a response variable. For example, in spider mite studies, PLS successfully detected mite damage based on increased reflectance.\r\nHowever, chilling accumulation follows a more complex, non-monotonic relationship with temperature. Effective chill accumulation depends on temperature ranges rather than a linear correlation. To understand this, different chill models can be analyzed using the chillR and dormancyR R packages:\r\n\r\n\r\nlibrary(chillR)\r\nlibrary(dormancyR)\r\nlibrary(ggplot2)\r\nlibrary(kableExtra)\r\nlibrary(patchwork)\r\nlibrary(tidyverse)\r\n\r\n\r\nhourly_models <- \r\n  list(\r\n    Chilling_units = chilling_units,\r\n    Low_chill = low_chill_model,\r\n    Modified_Utah = modified_utah_model,\r\n    North_Carolina = north_carolina_model,\r\n    Positive_Utah = positive_utah_model,\r\n    Chilling_Hours = Chilling_Hours,\r\n    Utah_Chill_Units = Utah_Model,\r\n    Chill_Portions = Dynamic_Model)\r\n\r\ndaily_models <-\r\n  list(\r\n    Rate_of_Chill = rate_of_chill, \r\n    Exponential_Chill = exponential_chill,\r\n    Triangular_Chill_Haninnen = triangular_chill_1,\r\n    Triangular_Chill_Legave = triangular_chill_2)\r\n\r\nmetrics <- c(names(daily_models),\r\n             names(hourly_models))\r\n\r\nmodel_labels <- c(\"Rate of Chill\",\r\n                  \"Exponential Chill\",\r\n                  \"Triangular Chill (Häninnen)\",\r\n                  \"Triangular Chill (Legave)\",\r\n                  \"Chilling Units\",\r\n                  \"Low-Chill Chill Units\",\r\n                  \"Modified Utah Chill Units\",\r\n                  \"North Carolina Chill Units\",\r\n                  \"Positive Utah Chill Units\",\r\n                  \"Chilling Hours\",\r\n                  \"Utah Chill Units\",\r\n                  \"Chill Portions\")\r\n\r\n\r\n\r\nfor(T in -20:30)\r\n {\r\n  hourly <- sapply( hourly_models,\r\n                    function(x)\r\n                      x(rep(T,1000))\r\n                    )[1000,]\r\n \r\n  temp_frame <- data.frame(Tmin = rep(T,1000),\r\n                           Tmax = rep(T,1000),\r\n                           Tmean = rep(T,1000))\r\n  \r\n  daily <- sapply( daily_models,\r\n                   function(x) \r\n                     x(temp_frame)\r\n                   )[1000,]\r\n \r\n  if(T == -20)\r\n    sensitivity <- c(T = T,\r\n                     daily,\r\n                     hourly) else   \r\n      sensitivity <- rbind(sensitivity,\r\n                           c(T = T,\r\n                             daily,\r\n                             hourly))\r\n  }\r\n\r\nsensitivity_normal <- \r\n  as.data.frame(cbind(sensitivity[,1],\r\n                      sapply(2:ncol(sensitivity),\r\n                             function(x)\r\n                               sensitivity[,x]/max(sensitivity[,x]))))\r\n\r\ncolnames(sensitivity_normal) <- colnames(sensitivity)\r\n\r\nsensitivity_gg <- \r\n  sensitivity_normal %>%\r\n  pivot_longer(Rate_of_Chill:Chill_Portions)\r\n  \r\n # melt(sensitivity_normal,id.vars=\"T\")\r\nsensitivity_gg$value[sensitivity_gg$value<=0.001] <- NA\r\n\r\n\r\nchill<-\r\n  ggplot(sensitivity_gg,\r\n         aes(x = T,\r\n             y = factor(name),\r\n             size = value)) +\r\n  geom_point(col = \"light blue\") +\r\n  scale_y_discrete(labels = model_labels) +\r\n  ylab(\"Chill model\") +\r\n  xlab(\"Temperature (assumed constant, °C)\") +\r\n  xlim(c(-30, 40)) +\r\n  theme_bw(base_size = 15) +\r\n  labs(size = \"Chill \\nWeight\")\r\n\r\n\r\n\r\n\r\nchill\r\n\r\n\r\n\r\nWinter temperatures at the three locations (Klein-Altendorf, Beijing, and Davis) can be summarized by obtaining and processing the relevant temperature data. Once the data files are saved in the data subfolder of the working directory, the provided code will process and analyze the temperatures. This enables the comparison of winter temperature patterns across the locations and their influence on phenological responses.\r\n\r\n\r\nKA_temps <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\") %>%\r\n  make_JDay() %>%\r\n  filter(JDay > 305 | JDay < 90) %>%\r\n  stack_hourly_temps(latitude = 50.6)\r\n\r\n\r\nhh_KA <- hist(KA_temps$hourtemps$Temp,\r\n              breaks = c(-30:30),\r\n              plot=FALSE)\r\n\r\nhh_KA_df <- data.frame(\r\n  T = hh_KA$mids,\r\n  name = \"Klein-Altendorf, Germany\",\r\n  value = hh_KA$counts / max(hh_KA$counts))\r\n\r\nhh_KA_df$value[hh_KA_df$value == 0] <- NA\r\n\r\n\r\nBeijing_temps <- read_tab(\"data/Beijing_weather.csv\") %>%\r\n  make_JDay() %>%\r\n  filter(JDay > 305 | JDay < 90) %>%\r\n  stack_hourly_temps(latitude = 39.9)\r\n\r\nhh_Beijing <- hist(Beijing_temps$hourtemps$Temp,\r\n                   breaks = c(-30:30),\r\n                   plot=FALSE)\r\n\r\nhh_Beijing_df<-data.frame(\r\n  T = hh_Beijing$mids,\r\n  name = \"Beijing, China\",\r\n  value = hh_Beijing$counts / max(hh_Beijing$counts))\r\n\r\nhh_Beijing_df$value[hh_Beijing_df$value==0]<-NA\r\n\r\n\r\nDavis_temps <- read_tab(\"data/Davis_weather.csv\") %>%\r\n  make_JDay() %>%\r\n  filter(JDay > 305 | JDay < 90) %>%\r\n  stack_hourly_temps(latitude = 38.5)\r\n\r\n\r\nhh_Davis <- hist(Davis_temps$hourtemps$Temp,\r\n              breaks = c(-30:40),\r\n              plot=FALSE)\r\n\r\nhh_Davis_df <- data.frame(\r\n  T = hh_Davis$mids,\r\n  name = \"Davis, California\",\r\n  value = hh_Davis$counts / max(hh_Davis$counts))\r\n\r\nhh_Davis_df$value[hh_Davis_df$value == 0] <- NA\r\n\r\n\r\nhh_df<-rbind(hh_KA_df,\r\n             hh_Beijing_df,\r\n             hh_Davis_df)\r\n\r\nlocations<-\r\n  ggplot(data = hh_df,\r\n         aes(x = T,\r\n             y = name,\r\n             size = value)) +\r\n  geom_point(col = \"coral2\") +\r\n  ylab(\"Location\") +\r\n  xlab(\"Temperature (between November and March, °C)\") + \r\n  xlim(c(-30, 40)) +\r\n  theme_bw(base_size = 15) +\r\n  labs(size = \"Relative \\nfrequency\")\r\n\r\n\r\n\r\n\r\nlocations\r\n\r\n\r\n\r\nTo compare the plots, the results can be combined into one figure using the patchwork package:\r\n\r\n\r\nplot <- (chill +\r\n             locations +\r\n             plot_layout(guides = \"collect\",\r\n                         heights = c(1, 0.4))\r\n           ) & theme(legend.position = \"right\",\r\n                     legend.text = element_text(size = 10),\r\n                     legend.title = element_text(size = 12))\r\n\r\nplot\r\n\r\n\r\n\r\nTo simplify the analysis, the focus can be shifted to plotting chill according to the Dynamic Model, as it is a more reliable approach compared to some of the other models. This will help in creating a clearer, more focused visualization of chill accumulation and its relationship with temperature.\r\n\r\n\r\nchill <-\r\n  ggplot(sensitivity_gg %>%\r\n           filter(name == \"Chill_Portions\"),\r\n         aes(x = T,\r\n             y = factor(name),\r\n             size=value)) +\r\n  geom_point(col = \"light blue\") +\r\n  scale_y_discrete(labels = \"Chill Portions\") +\r\n  ylab(\"Chill model\") +\r\n  xlab(\"Temperature (assumed constant, °C)\") +\r\n  xlim(c(-30, 40)) +\r\n  theme_bw(base_size = 15) +\r\n  labs(size = \"Chill \\nWeight\")\r\n\r\n  plot<- (chill +\r\n            locations +\r\n            plot_layout(guides = \"collect\",\r\n                        heights = c(0.5,1))\r\n        ) & theme(legend.position = \"right\",\r\n                  legend.text = element_text(size = 10),\r\n                  legend.title = element_text(size = 12))\r\n\r\nplot\r\n\r\n\r\n\r\nWhen comparing the effective chill ranges with winter temperatures at the three locations, it becomes evident that in Klein-Altendorf and Beijing, temperatures are often cooler than the effective temperature range for chill accumulation. In contrast, Davis experiences more frequent temperatures that are too warm for effective chill accumulation.\r\nAt Davis, warmer winter temperatures are likely to reduce chill accumulation, while in Klein-Altendorf and Beijing, the relationship is more complex. In colder conditions, warming might increase chill accumulation, but in warmer conditions, it could decrease it. This non-monotonic relationship between temperature and chill accumulation means that PLS regression is unlikely to yield clear results at these locations.\r\nIn the following chapter, a solution to address this issue will be explored.\r\nExercises on chill model comparison\r\nBriefly explain in what climatic settings we can expect PLS regression to detect the chilling phase - and in what settings this probably won’t work.\r\nPLS regression works well for detecting the chilling phase in climates where temperatures stay within the effective chill range, allowing for a clear relationship between temperature and chill accumulation. However, in areas with fluctuating or excessively warm temperatures (like Davis, California), the relationship becomes non-monotonic, making PLS less effective.\r\nHow could we overcome this problem?\r\nTo improve chill detection, we could use models that account for non-monotonic relationships, such as the Dynamic Model. Additionally, incorporating environmental factors and refining chill models to better reflect temperature thresholds can help overcome the limitations of PLS regression.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:57:19+01:00"
    },
    {
      "path": "temp_data.html",
      "title": "Getting temperature data",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nTemperature data is crucial for phenology and chill models, as it serves as a key input for these calculations. However, accessing weather data is often challenging due to restrictions and high costs, even when the data is publicly funded.\r\nThe chillR package facilitates access to global and California-specific weather databases, helping researchers obtain necessary temperature records.\r\nGlobal Summary of the Day (GSOD)\r\nThe National Centers for Environmental Information (NCEI) provides temperature data through the GSOD database. While retrieving data manually can be cumbersome, chillR offers a streamlined solution with the function handle_gsod().\r\nListing Weather Stations\r\nTo retrieve a list of weather stations near a specific location, sorted by proximity, use:\r\n\r\n\r\nstation_list <- handle_gsod(action = \"list_stations\", \r\n                            location = c(7.10, 50.73), \r\n                            time_interval = c(1990, 2020))\r\n\r\n\r\nThis function returns a table containing station codes, available data years, and the percentage of the selected time period covered.\r\nDownloading Weather Data\r\nOnce a suitable station is identified, its chillR_code can be used to download temperature records:\r\n\r\n\r\nweather <- handle_gsod(action = \"download_weather\", \r\n                       location = station_list$chillR_code[4], \r\n                       time_interval = c(1990,2020))\r\n\r\n\r\nThis returns a list where weather[[1]] contains metadata, and weather[[2]] holds the actual temperature dataset.\r\nCleaning Weather Data\r\nRaw GSOD data contains unnecessary variables and is recorded in Fahrenheit. chillR provides a function to clean and convert these records:\r\n\r\n\r\ncleaned_weather <- handle_gsod(weather)\r\n\r\n\r\nTemperature values are converted using the formula:\r\n\\(Temperature[°C]=(Temperature[°F]-32)\\cdot\\frac{5}{9}\\)\r\nThis results in a more usable dataset suitable for further analysis.\r\nSaving Data for Future Use\r\n\r\n\r\nwrite.csv(station_list, \"data/station_list.csv\", row.names=FALSE)\r\nwrite.csv(weather[[1]], \"data/Bonn_raw_weather.csv\", row.names=FALSE)\r\nwrite.csv(cleaned_weather[[1]], \"data/Bonn_chillR_weather.csv\", row.names=FALSE)\r\n\r\n\r\nExercises on getting temperature data\r\n\r\n\r\n\r\nChoose a location of interest and find the 25 closest weather stations using the handle_gsod function\r\n\r\n\r\nstation_list_Yakima <- handle_gsod(action = \"list_stations\",\r\n                                   location = c(long = -120.50, lat = 46.60), \r\n                                   time_interval = c(1990, 2020))\r\n\r\n\r\n\r\n\r\n\r\nchillR_code\r\n\r\n\r\nSTATION.NAME\r\n\r\n\r\nCTRY\r\n\r\n\r\nLat\r\n\r\n\r\nLong\r\n\r\n\r\nBEGIN\r\n\r\n\r\nEND\r\n\r\n\r\nDistance\r\n\r\n\r\nOverlap_years\r\n\r\n\r\nPerc_interval_covered\r\n\r\n\r\n72781024243\r\n\r\n\r\nYAKIMA AIR TERMINAL/MCALSR FIELD AP\r\n\r\n\r\nUS\r\n\r\n\r\n46.564\r\n\r\n\r\n-120.535\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n4.82\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924243\r\n\r\n\r\nYAKIMA AIR TERMINAL\r\n\r\n\r\nUS\r\n\r\n\r\n46.568\r\n\r\n\r\n-120.543\r\n\r\n\r\n19480101\r\n\r\n\r\n19721231\r\n\r\n\r\n4.85\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72781399999\r\n\r\n\r\nVAGABOND AAF / YAKIMA TRAINING CENTER WASHINGTON USA\r\n\r\n\r\nUS\r\n\r\n\r\n46.667\r\n\r\n\r\n-120.454\r\n\r\n\r\n20030617\r\n\r\n\r\n20081110\r\n\r\n\r\n8.25\r\n\r\n\r\n5.40\r\n\r\n\r\n17\r\n\r\n\r\n72056299999\r\n\r\n\r\nRANGE OP 13 / YAKIMA TRAINING CENTER\r\n\r\n\r\nUS\r\n\r\n\r\n46.800\r\n\r\n\r\n-120.167\r\n\r\n\r\n20080530\r\n\r\n\r\n20170920\r\n\r\n\r\n33.79\r\n\r\n\r\n9.31\r\n\r\n\r\n30\r\n\r\n\r\n72788399999\r\n\r\n\r\nBOWERS FLD\r\n\r\n\r\nUS\r\n\r\n\r\n47.033\r\n\r\n\r\n-120.531\r\n\r\n\r\n20000101\r\n\r\n\r\n20031231\r\n\r\n\r\n48.26\r\n\r\n\r\n4.00\r\n\r\n\r\n13\r\n\r\n\r\n72788324220\r\n\r\n\r\nBOWERS FIELD AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.034\r\n\r\n\r\n-120.531\r\n\r\n\r\n19880106\r\n\r\n\r\n20250304\r\n\r\n\r\n48.37\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924220\r\n\r\n\r\nELLENSBURG BOWERS FI\r\n\r\n\r\nUS\r\n\r\n\r\n47.034\r\n\r\n\r\n-120.530\r\n\r\n\r\n19480601\r\n\r\n\r\n19550101\r\n\r\n\r\n48.37\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72784094187\r\n\r\n\r\nHANFORD AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n46.567\r\n\r\n\r\n-119.600\r\n\r\n\r\n20060101\r\n\r\n\r\n20130326\r\n\r\n\r\n68.96\r\n\r\n\r\n7.23\r\n\r\n\r\n23\r\n\r\n\r\n72784099999\r\n\r\n\r\nHANFORD\r\n\r\n\r\nUS\r\n\r\n\r\n46.567\r\n\r\n\r\n-119.600\r\n\r\n\r\n19730101\r\n\r\n\r\n19971231\r\n\r\n\r\n68.96\r\n\r\n\r\n8.00\r\n\r\n\r\n26\r\n\r\n\r\n72782594239\r\n\r\n\r\nPANGBORN MEMORIAL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.397\r\n\r\n\r\n-120.201\r\n\r\n\r\n20000101\r\n\r\n\r\n20250304\r\n\r\n\r\n91.58\r\n\r\n\r\n21.00\r\n\r\n\r\n68\r\n\r\n\r\n72782599999\r\n\r\n\r\nPANGBORN MEM\r\n\r\n\r\nUS\r\n\r\n\r\n47.399\r\n\r\n\r\n-120.207\r\n\r\n\r\n19730101\r\n\r\n\r\n19971231\r\n\r\n\r\n91.69\r\n\r\n\r\n8.00\r\n\r\n\r\n26\r\n\r\n\r\n72788499999\r\n\r\n\r\nRICHLAND AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n46.306\r\n\r\n\r\n-119.304\r\n\r\n\r\n19810203\r\n\r\n\r\n20250303\r\n\r\n\r\n97.39\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n72781524237\r\n\r\n\r\nSTAMPASS PASS FLTWO\r\n\r\n\r\nUS\r\n\r\n\r\n47.277\r\n\r\n\r\n-121.337\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n98.63\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924237\r\n\r\n\r\nSTAMPEDE PASS\r\n\r\n\r\nUS\r\n\r\n\r\n47.277\r\n\r\n\r\n-121.337\r\n\r\n\r\n19480101\r\n\r\n\r\n19721231\r\n\r\n\r\n98.63\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72790024141\r\n\r\n\r\nEPHRATA MUNICIPAL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.308\r\n\r\n\r\n-119.516\r\n\r\n\r\n20050101\r\n\r\n\r\n20250304\r\n\r\n\r\n108.64\r\n\r\n\r\n16.00\r\n\r\n\r\n52\r\n\r\n\r\n72782624141\r\n\r\n\r\nEPHRATA MUNICIPAL\r\n\r\n\r\nUS\r\n\r\n\r\n47.308\r\n\r\n\r\n-119.515\r\n\r\n\r\n19420101\r\n\r\n\r\n19971231\r\n\r\n\r\n108.69\r\n\r\n\r\n8.00\r\n\r\n\r\n26\r\n\r\n\r\n99999924141\r\n\r\n\r\nEPHRATA AP FCWOS\r\n\r\n\r\nUS\r\n\r\n\r\n47.308\r\n\r\n\r\n-119.515\r\n\r\n\r\n19480101\r\n\r\n\r\n19550101\r\n\r\n\r\n108.69\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72782724110\r\n\r\n\r\nGRANT COUNTY INTL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n47.193\r\n\r\n\r\n-119.315\r\n\r\n\r\n19430610\r\n\r\n\r\n20250304\r\n\r\n\r\n111.73\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n72782799999\r\n\r\n\r\nMOSES LAKE/GRANT CO\r\n\r\n\r\nUS\r\n\r\n\r\n47.200\r\n\r\n\r\n-119.317\r\n\r\n\r\n20000101\r\n\r\n\r\n20031231\r\n\r\n\r\n112.06\r\n\r\n\r\n4.00\r\n\r\n\r\n13\r\n\r\n\r\n72784524163\r\n\r\n\r\nTRI-CITIES AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n46.270\r\n\r\n\r\n-119.118\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n112.21\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n72784599999\r\n\r\n\r\nTRI CITIES\r\n\r\n\r\nUS\r\n\r\n\r\n46.267\r\n\r\n\r\n-119.117\r\n\r\n\r\n20000101\r\n\r\n\r\n20031231\r\n\r\n\r\n112.40\r\n\r\n\r\n4.00\r\n\r\n\r\n13\r\n\r\n\r\n99999924163\r\n\r\n\r\nPASCO NAS\r\n\r\n\r\nUS\r\n\r\n\r\n46.267\r\n\r\n\r\n-119.117\r\n\r\n\r\n19450401\r\n\r\n\r\n19460601\r\n\r\n\r\n112.40\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72698824219\r\n\r\n\r\nMUNICIPAL AIRPORT\r\n\r\n\r\nUS\r\n\r\n\r\n45.619\r\n\r\n\r\n-121.166\r\n\r\n\r\n19730101\r\n\r\n\r\n20250304\r\n\r\n\r\n120.70\r\n\r\n\r\n31.00\r\n\r\n\r\n100\r\n\r\n\r\n99999924219\r\n\r\n\r\nTHE DALLES MUNICIPAL ARPT\r\n\r\n\r\nUS\r\n\r\n\r\n45.619\r\n\r\n\r\n-121.166\r\n\r\n\r\n19480101\r\n\r\n\r\n19650101\r\n\r\n\r\n120.70\r\n\r\n\r\n0.00\r\n\r\n\r\n0\r\n\r\n\r\n72688399999\r\n\r\n\r\nHERMISTON MUNI\r\n\r\n\r\nUS\r\n\r\n\r\n45.828\r\n\r\n\r\n-119.259\r\n\r\n\r\n19980514\r\n\r\n\r\n20051231\r\n\r\n\r\n128.55\r\n\r\n\r\n7.64\r\n\r\n\r\n25\r\n\r\n\r\n\r\nDownload weather data for the most promising station on the list\r\n\r\n\r\nweather_Yakima <- handle_gsod(action = \"download_weather\",\r\n                              location = station_list_Yakima$chillR_code[1],\r\n                              time_interval = c(1990, 2020))\r\n\r\n\r\n\r\n\r\nweather_Yakima[[1]][1:20,]\r\n\r\n\r\n\r\n\r\n\r\nDATE\r\n\r\n\r\nDate\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nTmin\r\n\r\n\r\nTmax\r\n\r\n\r\nTmean\r\n\r\n\r\nPrec\r\n\r\n\r\nYEARMODA\r\n\r\n\r\n1990-01-01 12:00:00\r\n\r\n\r\n1990-01-01\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n-1.722\r\n\r\n\r\n1.722\r\n\r\n\r\n-0.500\r\n\r\n\r\n0.000\r\n\r\n\r\n19900101\r\n\r\n\r\n1990-01-02 12:00:00\r\n\r\n\r\n1990-01-02\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n-5.611\r\n\r\n\r\n6.111\r\n\r\n\r\n0.056\r\n\r\n\r\n0.000\r\n\r\n\r\n19900102\r\n\r\n\r\n1990-01-03 12:00:00\r\n\r\n\r\n1990-01-03\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n-5.611\r\n\r\n\r\n7.778\r\n\r\n\r\n0.056\r\n\r\n\r\n0.000\r\n\r\n\r\n19900103\r\n\r\n\r\n1990-01-04 12:00:00\r\n\r\n\r\n1990-01-04\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n-3.278\r\n\r\n\r\n13.889\r\n\r\n\r\n2.444\r\n\r\n\r\n0.000\r\n\r\n\r\n19900104\r\n\r\n\r\n1990-01-05 12:00:00\r\n\r\n\r\n1990-01-05\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n-3.278\r\n\r\n\r\n13.889\r\n\r\n\r\n2.556\r\n\r\n\r\n0.000\r\n\r\n\r\n19900105\r\n\r\n\r\n1990-01-06 12:00:00\r\n\r\n\r\n1990-01-06\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n-3.278\r\n\r\n\r\n11.722\r\n\r\n\r\n6.222\r\n\r\n\r\n0.000\r\n\r\n\r\n19900106\r\n\r\n\r\n1990-01-07 12:00:00\r\n\r\n\r\n1990-01-07\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n7\r\n\r\n\r\n-2.222\r\n\r\n\r\n11.722\r\n\r\n\r\n9.000\r\n\r\n\r\n3.048\r\n\r\n\r\n19900107\r\n\r\n\r\n1990-01-08 12:00:00\r\n\r\n\r\n1990-01-08\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n8\r\n\r\n\r\n2.778\r\n\r\n\r\n12.222\r\n\r\n\r\n6.722\r\n\r\n\r\n9.652\r\n\r\n\r\n19900108\r\n\r\n\r\n1990-01-09 12:00:00\r\n\r\n\r\n1990-01-09\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n9\r\n\r\n\r\n1.111\r\n\r\n\r\n11.111\r\n\r\n\r\n4.111\r\n\r\n\r\n15.748\r\n\r\n\r\n19900109\r\n\r\n\r\n1990-01-10 12:00:00\r\n\r\n\r\n1990-01-10\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n10\r\n\r\n\r\n1.111\r\n\r\n\r\n12.222\r\n\r\n\r\n7.167\r\n\r\n\r\n2.794\r\n\r\n\r\n19900110\r\n\r\n\r\n1990-01-11 12:00:00\r\n\r\n\r\n1990-01-11\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\n0.000\r\n\r\n\r\n8.278\r\n\r\n\r\n2.778\r\n\r\n\r\n0.000\r\n\r\n\r\n19900111\r\n\r\n\r\n1990-01-12 12:00:00\r\n\r\n\r\n1990-01-12\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\n0.000\r\n\r\n\r\n6.111\r\n\r\n\r\n2.833\r\n\r\n\r\n0.254\r\n\r\n\r\n19900112\r\n\r\n\r\n1990-01-13 12:00:00\r\n\r\n\r\n1990-01-13\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n13\r\n\r\n\r\n-2.778\r\n\r\n\r\n6.111\r\n\r\n\r\n0.944\r\n\r\n\r\n0.254\r\n\r\n\r\n19900113\r\n\r\n\r\n1990-01-14 12:00:00\r\n\r\n\r\n1990-01-14\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n14\r\n\r\n\r\n-3.889\r\n\r\n\r\n5.000\r\n\r\n\r\n-0.111\r\n\r\n\r\n0.000\r\n\r\n\r\n19900114\r\n\r\n\r\n1990-01-15 12:00:00\r\n\r\n\r\n1990-01-15\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n15\r\n\r\n\r\n-3.889\r\n\r\n\r\n4.389\r\n\r\n\r\n0.889\r\n\r\n\r\n0.000\r\n\r\n\r\n19900115\r\n\r\n\r\n1990-01-16 12:00:00\r\n\r\n\r\n1990-01-16\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n-0.611\r\n\r\n\r\n7.222\r\n\r\n\r\n3.556\r\n\r\n\r\n0.000\r\n\r\n\r\n19900116\r\n\r\n\r\n1990-01-17 12:00:00\r\n\r\n\r\n1990-01-17\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\n-4.389\r\n\r\n\r\n7.778\r\n\r\n\r\n1.111\r\n\r\n\r\n0.000\r\n\r\n\r\n19900117\r\n\r\n\r\n1990-01-18 12:00:00\r\n\r\n\r\n1990-01-18\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n18\r\n\r\n\r\n-7.778\r\n\r\n\r\n8.278\r\n\r\n\r\n-2.667\r\n\r\n\r\n0.000\r\n\r\n\r\n19900118\r\n\r\n\r\n1990-01-19 12:00:00\r\n\r\n\r\n1990-01-19\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n19\r\n\r\n\r\n-7.778\r\n\r\n\r\n3.278\r\n\r\n\r\n0.389\r\n\r\n\r\n0.000\r\n\r\n\r\n19900119\r\n\r\n\r\n1990-01-20 12:00:00\r\n\r\n\r\n1990-01-20\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n20\r\n\r\n\r\n-2.222\r\n\r\n\r\n2.222\r\n\r\n\r\n0.778\r\n\r\n\r\n0.000\r\n\r\n\r\n19900120\r\n\r\n\r\n\r\nConvert the weather data into chillR format\r\n\r\n\r\ncleaned_weather_Yakima <- handle_gsod(weather_Yakima) \r\n\r\n\r\n\r\n\r\ncleaned_weather_Yakima[[1]][1:20,]\r\n\r\n\r\n\r\n\r\n\r\nDate\r\n\r\n\r\nYear\r\n\r\n\r\nMonth\r\n\r\n\r\nDay\r\n\r\n\r\nTmin\r\n\r\n\r\nTmax\r\n\r\n\r\nTmean\r\n\r\n\r\nPrec\r\n\r\n\r\n1990-01-01\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n-1.722\r\n\r\n\r\n1.722\r\n\r\n\r\n-0.500\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-02\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n-5.611\r\n\r\n\r\n6.111\r\n\r\n\r\n0.056\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-03\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n-5.611\r\n\r\n\r\n7.778\r\n\r\n\r\n0.056\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-04\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n-3.278\r\n\r\n\r\n13.889\r\n\r\n\r\n2.444\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-05\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n-3.278\r\n\r\n\r\n13.889\r\n\r\n\r\n2.556\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-06\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n-3.278\r\n\r\n\r\n11.722\r\n\r\n\r\n6.222\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-07\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n7\r\n\r\n\r\n-2.222\r\n\r\n\r\n11.722\r\n\r\n\r\n9.000\r\n\r\n\r\n3.048\r\n\r\n\r\n1990-01-08\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n8\r\n\r\n\r\n2.778\r\n\r\n\r\n12.222\r\n\r\n\r\n6.722\r\n\r\n\r\n9.652\r\n\r\n\r\n1990-01-09\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n9\r\n\r\n\r\n1.111\r\n\r\n\r\n11.111\r\n\r\n\r\n4.111\r\n\r\n\r\n15.748\r\n\r\n\r\n1990-01-10\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n10\r\n\r\n\r\n1.111\r\n\r\n\r\n12.222\r\n\r\n\r\n7.167\r\n\r\n\r\n2.794\r\n\r\n\r\n1990-01-11\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\n0.000\r\n\r\n\r\n8.278\r\n\r\n\r\n2.778\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-12\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\n0.000\r\n\r\n\r\n6.111\r\n\r\n\r\n2.833\r\n\r\n\r\n0.254\r\n\r\n\r\n1990-01-13\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n13\r\n\r\n\r\n-2.778\r\n\r\n\r\n6.111\r\n\r\n\r\n0.944\r\n\r\n\r\n0.254\r\n\r\n\r\n1990-01-14\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n14\r\n\r\n\r\n-3.889\r\n\r\n\r\n5.000\r\n\r\n\r\n-0.111\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-15\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n15\r\n\r\n\r\n-3.889\r\n\r\n\r\n4.389\r\n\r\n\r\n0.889\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-16\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n-0.611\r\n\r\n\r\n7.222\r\n\r\n\r\n3.556\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-17\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\n-4.389\r\n\r\n\r\n7.778\r\n\r\n\r\n1.111\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-18\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n18\r\n\r\n\r\n-7.778\r\n\r\n\r\n8.278\r\n\r\n\r\n-2.667\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-19\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n19\r\n\r\n\r\n-7.778\r\n\r\n\r\n3.278\r\n\r\n\r\n0.389\r\n\r\n\r\n0.000\r\n\r\n\r\n1990-01-20\r\n\r\n\r\n1990\r\n\r\n\r\n1\r\n\r\n\r\n20\r\n\r\n\r\n-2.222\r\n\r\n\r\n2.222\r\n\r\n\r\n0.778\r\n\r\n\r\n0.000\r\n\r\n\r\n\r\n\r\n\r\ndir.create(\"Yakima\")\r\nwrite.csv(station_list_Yakima,\"Yakima/station_list.csv\", row.names = FALSE)\r\nwrite.csv(weather_Yakima[[1]],\"Yakima/raw_weather.csv\", row.names = FALSE)\r\nwrite.csv(cleaned_weather_Yakima[[1]],\"Yakima/chillR_weather.csv\", row.names = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:57:34+01:00"
    },
    {
      "path": "tools.html",
      "title": "Some useful tools in R",
      "author": [],
      "contents": "\r\nAn evolving language - and a lifelong learning process\r\nThe R universe is continuously evolving, offering more than just its original base functions. Over time, modern tools and more elegant programming styles have become integral. In the upcoming chapters, we will introduce some of these new tools, along with the basics required to use them effectively.\r\nThe tidyverse\r\nMany of the tools introduced here come from the tidyverse – a collection of packages developed by Hadley Wickham and his team. This collection offers numerous ways to improve programming skills. In this book, only the functions that are directly used will be covered. A major advantage of the tidyverse is that with a single command – library(tidyverse) – all functions in the package collection become available.\r\nThe ggplot2 package\r\nThe ggplot2 package, first released by Hadley Wickham in 2007, has become one of the most popular R packages because it significantly simplifies the creation of attractive graphics. The package history can be found here, and an introduction with links to various tutorials is available here.\r\nThe tibble package\r\nA tibble is an enhanced version of a data.frame offering several improvements. The most notable improvement is that tibbles avoid the common data.frame behavior of unexpectedly converting strings into factors. Although tibbles are relatively new here, they will be used throughout the rest of the book.\r\nTo create a tibble from a regular data.frame (or a similar structure), the as_tibble command can be used:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\n\r\ndat <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6))\r\nd <- as_tibble(dat)\r\nd\r\n\r\n# A tibble: 3 × 2\r\n      a     b\r\n  <dbl> <dbl>\r\n1     1     4\r\n2     2     5\r\n3     3     6\r\n\r\nThe magrittr package - pipes\r\nMagrittr helps organize steps applied to the same dataset by using the pipe operator %>%. This operator links multiple operations on a data structure, such as a tibble, making it easier to perform tasks like calculating the sum of all numbers in the dataset:\r\n\r\n\r\nd %>% sum()\r\n\r\n[1] 21\r\n\r\nAfter the pipe operator %>%, the next function automatically takes the piped-in data as its first input, so it’s unnecessary to specify it explicitly. Additional commands can be chained by adding more pipes, allowing for building more complex workflows, as shown in examples later.\r\nThe tidyr package\r\nThe tidyr package offers helpful functions for organizing data. The KA_weather dataset from chillR will be used here to illustrate some of these functions:\r\n\r\n\r\nlibrary(chillR)\r\n\r\n\r\n\r\n\r\nKAw <- as_tibble(KA_weather[1:10,])\r\nKAw\r\n\r\n# A tibble: 10 × 5\r\n    Year Month   Day  Tmax  Tmin\r\n   <int> <int> <int> <dbl> <dbl>\r\n 1  1998     1     1   8.2   5.1\r\n 2  1998     1     2   9.1   5  \r\n 3  1998     1     3  10.4   3.3\r\n 4  1998     1     4   8.4   4.5\r\n 5  1998     1     5   7.7   4.5\r\n 6  1998     1     6   8.1   4.4\r\n 7  1998     1     7  12     6.9\r\n 8  1998     1     8  11.2   8.6\r\n 9  1998     1     9  13.9   8.5\r\n10  1998     1    10  14.5   3.6\r\n\r\npivot_longer\r\nThe pivot_longer function reshapes data from separate columns (like Tmin and Tmax) into individual rows. This transformation is often necessary for tasks like plotting data with the ggplot2 package. The function can be combined with a pipe for a streamlined workflow:\r\n\r\n\r\nKAwlong <- KAw %>% pivot_longer(cols = Tmax:Tmin)\r\nKAwlong\r\n\r\n# A tibble: 20 × 5\r\n    Year Month   Day name  value\r\n   <int> <int> <int> <chr> <dbl>\r\n 1  1998     1     1 Tmax    8.2\r\n 2  1998     1     1 Tmin    5.1\r\n 3  1998     1     2 Tmax    9.1\r\n 4  1998     1     2 Tmin    5  \r\n 5  1998     1     3 Tmax   10.4\r\n 6  1998     1     3 Tmin    3.3\r\n 7  1998     1     4 Tmax    8.4\r\n 8  1998     1     4 Tmin    4.5\r\n 9  1998     1     5 Tmax    7.7\r\n10  1998     1     5 Tmin    4.5\r\n11  1998     1     6 Tmax    8.1\r\n12  1998     1     6 Tmin    4.4\r\n13  1998     1     7 Tmax   12  \r\n14  1998     1     7 Tmin    6.9\r\n15  1998     1     8 Tmax   11.2\r\n16  1998     1     8 Tmin    8.6\r\n17  1998     1     9 Tmax   13.9\r\n18  1998     1     9 Tmin    8.5\r\n19  1998     1    10 Tmax   14.5\r\n20  1998     1    10 Tmin    3.6\r\n\r\npivot_wider\r\nThe pivot_wider function allows for the opposite transformation of pivot_longer, converting rows back into separate columns:\r\n\r\n\r\nKAwwide <- KAwlong %>% pivot_wider(names_from = name) \r\nKAwwide\r\n\r\n# A tibble: 10 × 5\r\n    Year Month   Day  Tmax  Tmin\r\n   <int> <int> <int> <dbl> <dbl>\r\n 1  1998     1     1   8.2   5.1\r\n 2  1998     1     2   9.1   5  \r\n 3  1998     1     3  10.4   3.3\r\n 4  1998     1     4   8.4   4.5\r\n 5  1998     1     5   7.7   4.5\r\n 6  1998     1     6   8.1   4.4\r\n 7  1998     1     7  12     6.9\r\n 8  1998     1     8  11.2   8.6\r\n 9  1998     1     9  13.9   8.5\r\n10  1998     1    10  14.5   3.6\r\n\r\nselect\r\nThe select function allows users to choose a subset of columns from a data.frame or tibble:\r\n\r\n\r\nKAw %>% select(c(Month, Day, Tmax))\r\n\r\n# A tibble: 10 × 3\r\n   Month   Day  Tmax\r\n   <int> <int> <dbl>\r\n 1     1     1   8.2\r\n 2     1     2   9.1\r\n 3     1     3  10.4\r\n 4     1     4   8.4\r\n 5     1     5   7.7\r\n 6     1     6   8.1\r\n 7     1     7  12  \r\n 8     1     8  11.2\r\n 9     1     9  13.9\r\n10     1    10  14.5\r\n\r\nfilter\r\nThe filter function reduces a data.frame or tibble to just the rows that fulfill certain conditions:\r\n\r\n\r\nKAw %>% filter(Tmax > 10)\r\n\r\n# A tibble: 5 × 5\r\n   Year Month   Day  Tmax  Tmin\r\n  <int> <int> <int> <dbl> <dbl>\r\n1  1998     1     3  10.4   3.3\r\n2  1998     1     7  12     6.9\r\n3  1998     1     8  11.2   8.6\r\n4  1998     1     9  13.9   8.5\r\n5  1998     1    10  14.5   3.6\r\n\r\nmutate\r\nThe mutate function is essential for creating, modifying, and deleting columns in a data.frame or tibble. For example, it can be used to add new columns, such as converting Tmin and Tmax to Kelvin:\r\n\r\n\r\nKAw_K <- KAw %>% mutate(Tmax_K = Tmax + 273.15, Tmin_K = Tmin + 273.15)\r\nKAw_K\r\n\r\n# A tibble: 10 × 7\r\n    Year Month   Day  Tmax  Tmin Tmax_K Tmin_K\r\n   <int> <int> <int> <dbl> <dbl>  <dbl>  <dbl>\r\n 1  1998     1     1   8.2   5.1   281.   278.\r\n 2  1998     1     2   9.1   5     282.   278.\r\n 3  1998     1     3  10.4   3.3   284.   276.\r\n 4  1998     1     4   8.4   4.5   282.   278.\r\n 5  1998     1     5   7.7   4.5   281.   278.\r\n 6  1998     1     6   8.1   4.4   281.   278.\r\n 7  1998     1     7  12     6.9   285.   280.\r\n 8  1998     1     8  11.2   8.6   284.   282.\r\n 9  1998     1     9  13.9   8.5   287.   282.\r\n10  1998     1    10  14.5   3.6   288.   277.\r\n\r\nTo delete the columns created with mutate, you can set them to NULL:\r\n\r\n\r\nKAw_K %>% mutate(Tmin_K = NULL, Tmax_K = NULL)\r\n\r\n# A tibble: 10 × 5\r\n    Year Month   Day  Tmax  Tmin\r\n   <int> <int> <int> <dbl> <dbl>\r\n 1  1998     1     1   8.2   5.1\r\n 2  1998     1     2   9.1   5  \r\n 3  1998     1     3  10.4   3.3\r\n 4  1998     1     4   8.4   4.5\r\n 5  1998     1     5   7.7   4.5\r\n 6  1998     1     6   8.1   4.4\r\n 7  1998     1     7  12     6.9\r\n 8  1998     1     8  11.2   8.6\r\n 9  1998     1     9  13.9   8.5\r\n10  1998     1    10  14.5   3.6\r\n\r\nNext, the original temperature values will be replaced directly with their corresponding Kelvin values:\r\n\r\n\r\nKAw %>% mutate(Tmin = Tmin + 273.15, Tmax = Tmax + 273.15)\r\n\r\n# A tibble: 10 × 5\r\n    Year Month   Day  Tmax  Tmin\r\n   <int> <int> <int> <dbl> <dbl>\r\n 1  1998     1     1  281.  278.\r\n 2  1998     1     2  282.  278.\r\n 3  1998     1     3  284.  276.\r\n 4  1998     1     4  282.  278.\r\n 5  1998     1     5  281.  278.\r\n 6  1998     1     6  281.  278.\r\n 7  1998     1     7  285.  280.\r\n 8  1998     1     8  284.  282.\r\n 9  1998     1     9  287.  282.\r\n10  1998     1    10  288.  277.\r\n\r\narrange\r\nThe arrange function sorts data in data.frames or tibbles:\r\n\r\n\r\nKAw %>% arrange(Tmax, Tmin)\r\n\r\n# A tibble: 10 × 5\r\n    Year Month   Day  Tmax  Tmin\r\n   <int> <int> <int> <dbl> <dbl>\r\n 1  1998     1     5   7.7   4.5\r\n 2  1998     1     6   8.1   4.4\r\n 3  1998     1     1   8.2   5.1\r\n 4  1998     1     4   8.4   4.5\r\n 5  1998     1     2   9.1   5  \r\n 6  1998     1     3  10.4   3.3\r\n 7  1998     1     8  11.2   8.6\r\n 8  1998     1     7  12     6.9\r\n 9  1998     1     9  13.9   8.5\r\n10  1998     1    10  14.5   3.6\r\n\r\nIt can also sort in descending order:\r\n\r\n\r\nKAw %>% arrange(desc(Tmax), Tmin)\r\n\r\n# A tibble: 10 × 5\r\n    Year Month   Day  Tmax  Tmin\r\n   <int> <int> <int> <dbl> <dbl>\r\n 1  1998     1    10  14.5   3.6\r\n 2  1998     1     9  13.9   8.5\r\n 3  1998     1     7  12     6.9\r\n 4  1998     1     8  11.2   8.6\r\n 5  1998     1     3  10.4   3.3\r\n 6  1998     1     2   9.1   5  \r\n 7  1998     1     4   8.4   4.5\r\n 8  1998     1     1   8.2   5.1\r\n 9  1998     1     6   8.1   4.4\r\n10  1998     1     5   7.7   4.5\r\n\r\nLoops\r\nUnderstanding loops is essential for efficient coding. Loops enable the repetition of operations multiple times without needing to retype or copy-paste code. There are two primary types of loops: for loops and while loops.\r\nFor loops\r\nIn a for loop, explicit instructions dictate how many times the code inside the loop should be executed, based on a vector or list of elements:\r\n\r\n\r\nfor (i in 1:3) print(\"Hello\")\r\n\r\n[1] \"Hello\"\r\n[1] \"Hello\"\r\n[1] \"Hello\"\r\n\r\nThis code executes the loop three times, printing “Hello” each time. A more complex example uses multiple lines inside curly brackets:\r\n\r\n\r\naddition <- 1\r\n\r\nfor (i in 1:3)\r\n{\r\n  addition <- addition + 1\r\n  print(addition)\r\n}\r\n\r\n[1] 2\r\n[1] 3\r\n[1] 4\r\n\r\nYou can also use a variable such as i in more creative ways within the loop:\r\n\r\n\r\nnames <- c(\"Paul\", \"Mary\", \"John\")\r\n\r\nfor (i in 1:3)\r\n{\r\n  print(paste(\"Hello\", names[i]))\r\n}\r\n\r\n[1] \"Hello Paul\"\r\n[1] \"Hello Mary\"\r\n[1] \"Hello John\"\r\n\r\nWhile loops\r\nA while loop continues until a condition is no longer met:\r\n\r\n\r\ncond <- 5\r\n\r\nwhile (cond > 0)\r\n{\r\n  print(cond)\r\n  cond <- cond - 1\r\n}\r\n\r\n[1] 5\r\n[1] 4\r\n[1] 3\r\n[1] 2\r\n[1] 1\r\n\r\napply functions\r\nR offers a more efficient way to perform operations on multiple elements simultaneously using functions from the apply family: apply, lapply, and sapply. These functions require two key arguments: the list of items to apply the operation to and the operation itself.\r\nsapply\r\nThe sapply function is used to apply an operation to a vector:\r\n\r\n\r\nfunc <- function(x) x + 1\r\nsapply(1:5, func)\r\n\r\n[1] 2 3 4 5 6\r\n\r\nlapply\r\nThe lapply function returns a list as the output, even if the input is a vector:\r\n\r\n\r\nlapply(1:5, func)\r\n\r\n[[1]]\r\n[1] 2\r\n\r\n[[2]]\r\n[1] 3\r\n\r\n[[3]]\r\n[1] 4\r\n\r\n[[4]]\r\n[1] 5\r\n\r\n[[5]]\r\n[1] 6\r\n\r\napply\r\nThe apply function is designed for arrays, allowing operations to be performed either on rows (MARGIN = 1) or columns (MARGIN = 2):\r\n\r\n\r\nmat <- matrix(c(1, 1, 1, 2, 2, 2, 3, 3, 3), c(3, 3))\r\napply(mat, MARGIN = 1, sum) # sum of rows\r\n\r\n[1] 6 6 6\r\n\r\napply(mat, MARGIN = 2, sum) # sum of columns\r\n\r\n[1] 3 6 9\r\n\r\nExercises on useful R tools\r\nBased on the Winters_hours_gaps dataset, use magrittr pipes and functions of the tidyverse to accomplish the following:\r\nConvert the dataset into a tibble\r\n\r\nSelect only the top 10 rows of the dataset\r\n\r\n\r\n\r\nWHG <- as_tibble(Winters_hours_gaps[1:10, ])\r\nWHG\r\n\r\n# A tibble: 10 × 6\r\n    Year Month   Day  Hour Temp_gaps  Temp\r\n   <int> <int> <int> <int>     <dbl> <dbl>\r\n 1  2008     3     3    10      15.1  15.1\r\n 2  2008     3     3    11      17.2  17.2\r\n 3  2008     3     3    12      18.7  18.7\r\n 4  2008     3     3    13      18.7  18.7\r\n 5  2008     3     3    14      18.8  18.8\r\n 6  2008     3     3    15      19.5  19.5\r\n 7  2008     3     3    16      19.3  19.3\r\n 8  2008     3     3    17      17.7  17.7\r\n 9  2008     3     3    18      15.4  15.4\r\n10  2008     3     3    19      12.7  12.7\r\n\r\nConvert the tibble to a long format, with separate rows for Temp_gaps and Temp\r\n\r\nTo see the difference between the columns Temp_gaps and Temp, rows 279 to 302 (Julian Day 15) are used below:\r\n\r\n\r\nWHG <- as_tibble(Winters_hours_gaps[279:302, ])\r\nWHGlong <- WHG %>% pivot_longer(cols = Temp_gaps:Temp)\r\nWHGlong\r\n\r\n# A tibble: 48 × 6\r\n    Year Month   Day  Hour name      value\r\n   <int> <int> <int> <int> <chr>     <dbl>\r\n 1  2008     3    15     0 Temp_gaps  6.76\r\n 2  2008     3    15     0 Temp       6.76\r\n 3  2008     3    15     1 Temp_gaps  6.48\r\n 4  2008     3    15     1 Temp       6.48\r\n 5  2008     3    15     2 Temp_gaps  5.51\r\n 6  2008     3    15     2 Temp       5.51\r\n 7  2008     3    15     3 Temp_gaps  6.89\r\n 8  2008     3    15     3 Temp       6.89\r\n 9  2008     3    15     4 Temp_gaps  6.10\r\n10  2008     3    15     4 Temp       6.10\r\n# ℹ 38 more rows\r\n\r\nUse ggplot2 to plot Temp_gaps and Temp as facets (point or line plot)\r\n\r\n\r\n\r\nggplot(WHGlong, aes(Hour, value)) +\r\n  geom_line(lwd = 1.5) +\r\n  facet_grid(cols = vars(name)) +\r\n  ylab(\"Temperature (°C)\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nConvert the dataset back to the wide format\r\n\r\n\r\n\r\nWHGwide <- WHGlong %>% pivot_wider(names_from = name)\r\nWHGwide\r\n\r\n# A tibble: 24 × 6\r\n    Year Month   Day  Hour Temp_gaps  Temp\r\n   <int> <int> <int> <int>     <dbl> <dbl>\r\n 1  2008     3    15     0      6.76  6.76\r\n 2  2008     3    15     1      6.48  6.48\r\n 3  2008     3    15     2      5.51  5.51\r\n 4  2008     3    15     3      6.89  6.89\r\n 5  2008     3    15     4      6.10  6.10\r\n 6  2008     3    15     5     NA     6.91\r\n 7  2008     3    15     6     NA     6.10\r\n 8  2008     3    15     7     NA     5.98\r\n 9  2008     3    15     8     NA     8.99\r\n10  2008     3    15     9     10.8  10.8 \r\n# ℹ 14 more rows\r\n\r\nSelect only the following columns: Year, Month, Day and Temp\r\n\r\n\r\n\r\nWHG %>% select(c(Year, Month, Day, Temp))\r\n\r\n# A tibble: 24 × 4\r\n    Year Month   Day  Temp\r\n   <int> <int> <int> <dbl>\r\n 1  2008     3    15  6.76\r\n 2  2008     3    15  6.48\r\n 3  2008     3    15  5.51\r\n 4  2008     3    15  6.89\r\n 5  2008     3    15  6.10\r\n 6  2008     3    15  6.91\r\n 7  2008     3    15  6.10\r\n 8  2008     3    15  5.98\r\n 9  2008     3    15  8.99\r\n10  2008     3    15 10.8 \r\n# ℹ 14 more rows\r\n\r\nSort the dataset by the Temp column, in descending order\r\n\r\n\r\n\r\nWHG %>% arrange(desc(Temp))\r\n\r\n# A tibble: 24 × 6\r\n    Year Month   Day  Hour Temp_gaps  Temp\r\n   <int> <int> <int> <int>     <dbl> <dbl>\r\n 1  2008     3    15    13      NA    15.2\r\n 2  2008     3    15    16      14.5  14.5\r\n 3  2008     3    15    14      NA    14.2\r\n 4  2008     3    15    12      NA    14.2\r\n 5  2008     3    15    15      14.1  14.1\r\n 6  2008     3    15    11      NA    13.6\r\n 7  2008     3    15    17      13.3  13.3\r\n 8  2008     3    15    18      12.1  12.1\r\n 9  2008     3    15    10      12.0  12.0\r\n10  2008     3    15     9      10.8  10.8\r\n# ℹ 14 more rows\r\n\r\nFor the Winter_hours_gaps dataset, write a for loop to convert all temperatures (Temp column) to degrees Fahrenheit\r\nSo that the execution of the following code does not take too long, only Julian Day 15 (rows 279 to 302) is used here. To convert the entire Temp column to Fahrenheit, just omit [279:302]\r\n\r\n\r\nTemp <- Winters_hours_gaps$Temp[279:302]\r\n\r\nfor (i in Temp)\r\n{\r\n  Fahrenheit <- i * 1.8 + 32 \r\n  print(Fahrenheit)\r\n}\r\n\r\n[1] 44.1734\r\n[1] 43.6712\r\n[1] 41.9252\r\n[1] 44.4002\r\n[1] 42.9836\r\n[1] 44.4452\r\n[1] 42.9836\r\n[1] 42.755\r\n[1] 48.182\r\n[1] 51.3698\r\n[1] 53.69\r\n[1] 56.4692\r\n[1] 57.506\r\n[1] 59.4446\r\n[1] 57.5492\r\n[1] 57.3332\r\n[1] 58.1522\r\n[1] 55.9058\r\n[1] 53.8196\r\n[1] 49.4708\r\n[1] 47.6474\r\n[1] 47.3342\r\n[1] 48.182\r\n[1] 47.7806\r\n\r\nExecute the same operation with a function from the apply family\r\nHere it is the same as in 2, just omit [279:302] to convert the entire Temp column\r\n\r\n\r\nx <- Winters_hours_gaps$Temp\r\n\r\nfahrenheit <- function(x)\r\n  x * 1.8 + 32\r\n\r\nsapply(x[279:302], fahrenheit)\r\n\r\n [1] 44.1734 43.6712 41.9252 44.4002 42.9836 44.4452 42.9836 42.7550\r\n [9] 48.1820 51.3698 53.6900 56.4692 57.5060 59.4446 57.5492 57.3332\r\n[17] 58.1522 55.9058 53.8196 49.4708 47.6474 47.3342 48.1820 47.7806\r\n\r\nNow use the tidyverse function mutate to achieve the same outcome\r\n\r\n\r\nWHG_F <- WHG %>% mutate(Temp_F = Temp * 1.8 + 32)\r\nWHG_F\r\n\r\n# A tibble: 24 × 7\r\n    Year Month   Day  Hour Temp_gaps  Temp Temp_F\r\n   <int> <int> <int> <int>     <dbl> <dbl>  <dbl>\r\n 1  2008     3    15     0      6.76  6.76   44.2\r\n 2  2008     3    15     1      6.48  6.48   43.7\r\n 3  2008     3    15     2      5.51  5.51   41.9\r\n 4  2008     3    15     3      6.89  6.89   44.4\r\n 5  2008     3    15     4      6.10  6.10   43.0\r\n 6  2008     3    15     5     NA     6.91   44.4\r\n 7  2008     3    15     6     NA     6.10   43.0\r\n 8  2008     3    15     7     NA     5.98   42.8\r\n 9  2008     3    15     8     NA     8.99   48.2\r\n10  2008     3    15     9     10.8  10.8    51.4\r\n# ℹ 14 more rows\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:57:57+01:00"
    },
    {
      "path": "tree_dormancy.html",
      "title": "Tree dormancy in temperate fruit trees",
      "author": [],
      "contents": "\r\nThis chapter explores dormancy in temperate fruit trees. This topic remains complex and has many unanswered questions. A central question for researchers is, “How do trees know when to flower?” Although it seems clear that trees bloom in spring, the reality is more complicated. This chapter provides a better understanding of dormancy and demonstrates how to use the chillR tool to predict flowering times, even in the face of challenges posed by global warming.\r\nDormancy definition\r\nTree dormancy is a state of reduced activity that occurs when environmental conditions are unfavorable, especially during winter. This state acts as a survival strategy, helping trees withstand extreme temperatures, water shortages, and other stress factors. During dormancy, trees slow down or stop their growth to conserve energy and avoid damage. Dormancy is a continuous process divided into three phases:\r\nDormancy Establishment\r\nDormancy establishment is the process where temperate trees transition from active growth in summer to a dormant state in autumn. This shift is mainly triggered by shorter daylight hours and decreasing temperatures, causing buds to form, growth to stop, and leaves to fall. The importance of these factors varies by species, with some trees responding more to day length and others to temperature.\r\nEndo-dormancy\r\nEndo-dormancy is a phase of dormancy controlled by the plant’s internal factors, where growth is suppressed even under favorable conditions. It requires a period of cold exposure (chilling) to release the buds from this state, preventing premature growth during temporary warm spells in winter. Low temperatures are the main trigger for breaking endo-dormancy, while the role of light (photoperiod) is still uncertain.\r\nEco-dormancy\r\nEco-dormancy is the phase after endo-dormancy, where buds have regained their ability to grow but remain inactive due to unfavorable environmental conditions, mainly low temperatures. During this phase, growth is on hold until warmer temperatures trigger it. Heat accumulation is needed to resume growth. Eco-dormancy ends when enough heat has been accumulated, leading to visible growth changes, typically in late winter or early spring.\r\nThe below video Introduction to dormancy by Dr. Erica Fadón gives the basic knowledge of this dormancy phases and processes that regulate dormancy.\r\n\r\n\r\n\r\n\r\nDormancy physiology\r\nDormancy as a whole is the result of complex interactions between numerous physiological processes that occur in different parts of the tree, such as buds, twigs, meristems, and vascular tissues. We divide these processes into four main themes:\r\nTransport: occurs at both the whole-plant and cellular levels\r\nPhytohormone Dynamics: behavior and levels of plant hormones during dormancy\r\nGenetic and Epigenetic Regulation: how genetic factors and their modifications influence dormancy\r\nDynamics of Nonstructural Carbohydrates: changes in carbohydrate levels that affect dormancy\r\nThe following figure from the study “A conceptual framework for Winter Dormancy in Deciduous Trees” by Fadón et al. (2015) presents a conceptual framework of winter dormancy in deciduous trees and summarizes the three dormancy phases along with their respective physiological processes.\r\nConceptual framework of winter dormancy in deciduous trees. The dormancy framework (gray background) indicates three main phases: (a) dormancy establishment (light gray), (b) endo-dormancy (dark gray), and (c) eco-dormancy (medium gray). For each phase (a–c), the dormancy-related physiological processes are represented by colored shapes and numbers (0 to 4).All the processes depicted are explained in detail in the video below, Dormancy Physiology by Dr. Erica Fadón.\r\n\r\n\r\n\r\n\r\nExperimental and statistical determination of chilling and forcing periods\r\nDormancy consists of two phases where temperatures have opposite effects on flowering. During endodormancy, higher chill accumulation leads to earlier flowering, whereas similar cool temperatures during ecodormancy can delay flowering. The challenge lies in differentiating between these two phases, as the tree buds appear to be in the same developmental stage throughout. To address this, there are two methods available:\r\nExperimental method: collecting buds periodically during winter, exposing them to favorable growth conditions, and evaluating bud break to determine when dormancy is overcome\r\nStatistical method: uses long-term phenological data and temperature records to estimate the dates of chilling fulfillment and heat accumulation through partial least squares (PLS) regression analysis\r\nThe video Dormancy determination covers the experimental and statistical methods to determine the chilling and forcing periods for temperate fruit trees to overcome dormancy and initiate growth. It explains the concept of dormancy, its phases (endodormancy and ecodormancy), and the temperature requirements for breaking dormancy.\r\n\r\n\r\n\r\n\r\nPhenology record and BBCH scale\r\nPhenology is the study of periodic events in biological life cycles and how these are influenced by seasonal and interannual variations in climate. This module will involve working with phenology data sets, primarily focusing on a specific stage, usually budbreak, even though trees pass through various developmental stages during the year. These stages are typically identified by numerical codes.\r\nTo describe these growth stages systematically, the BBCH scale is employed. This internationally standardized system outlines the growth and developmental phases of plants. The BBCH scale consists of ten main stages, known as principal growth stages, which are numbered from 0 to 9. Each of these main stages is further divided into substages, enabling a more detailed description of a plant’s development.\r\nPrincipal growth stages:\r\nStage\r\nDescription\r\n0\r\nGermination / sprouting / bud development\r\n1\r\nLeaf development (main shoot)\r\n2\r\nFormation of side shoots / tillering\r\n3\r\nStem elongation or rosette growth / shoot development (main shoot)\r\n4\r\nDevelopment of harvestable vegetative plant parts or vegetatively propagated organs / booting (main shoot)\r\n5\r\nInflorescence emergence\r\n6\r\nFlowering (main shoot)\r\n7\r\nDevelopment of fruit\r\n8\r\nRipening or maturity of fruit and seed\r\n9\r\nSenescence, beginning of dormancy\r\nFor a comprehensive overview of phenology and the BBCH scale, the video Phenology by Dr. Erica Fadón is recommended. In this video, Dr. Fadón explains the concept of phenology and how the BBCH scale uses numerical codes to represent the different developmental stages of trees, from budding and flowering to fruit ripening and leaf fall.\r\n\r\n\r\n\r\n\r\nExcercises on tree dormancy\r\nPut yourself in the place of a breeder who wants to calculate the temperature requirements of a newly released cultivar. Which method will you use to calculate the chilling and forcing periods? Please justify your answer.\r\nAs a breeder aiming to calculate the temperature requirements for the chilling and forcing periods of a newly released cultivar, I would use the experimental method to determine the chilling and forcing periods. Here’s my justification:\r\nDirect measurement of bud response: The experimental method allows me to directly observe when buds break under controlled temperature conditions. By regularly collecting buds during winter and placing them in ideal growth conditions, I can determine exactly when dormancy ends. This practical approach gives me quick and useful information about the specific cultivar\r\nSpecific to the cultivar: Each cultivar has its own unique chilling and forcing needs. The experimental method looks at the specific traits of the new cultivar, making sure the results are relevant and applicable to that variety\r\nImmediate results for breeding decisions: The experimental method provides quick evaluations of bud break, allowing me to make faster decisions about breeding and managing the new cultivar\r\nWhich are the advantages (2) of the BBCH scale compared with earlier scales?\r\nStandardization: The BBCH scale provides a standardized framework for describing plant growth stages, enabling consistent communication and comparisons across studies\r\nDetailed Staging: It offers a more granular categorization of developmental stages using a two-digit system, allowing for a better understanding of plant development and environmental impacts.\r\nClassify the following phenological stages of sweet cherry according to the BBCH scale:\r\n\r\nleft image: BBCH stage 55 (single flower buds visible (still closed), green scales slightly open)\r\nmiddle image: BBCH stage 65 (full flowering: at least 50% of flowers open, first petals falling)\r\nright image: BBCH stage 89 (fruit ripe for harvesting)\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T09:58:04+01:00"
    },
    {
      "path": "valid_models.html",
      "title": "Making valid tree phenology models",
      "author": [],
      "contents": "\r\nThe Modeling Challenge\r\nModeling winter chill and predicting budburst dates is complex due to uncertainties in temperature responses, chill accumulation, and heat requirements. These challenges hinder the development of reliable models. While experimental data provide insights into tree responses, including the compensatory effects of heat for low chill, modeling these behaviors remains difficult. A credible model must account for these dynamics to ensure accuracy, and models ignoring key uncertainties or processes lack credibility.\r\nValidating Models\r\nSome models predict bloom dates accurately but may not represent underlying processes. Validation should include output and process validation. Output validation checks prediction accuracy, while process validation compares biological mechanisms in the model with established knowledge. Models must be evaluated within their validity domain, ensuring they are suitable for their intended purpose.\r\n\r\n\r\ndat <-data.frame(x = c(1, 2, 3, 4),\r\n                y = c(2.3, 2.5, 2.7, 2.7))\r\n\r\nggplot(dat,aes(x = x,\r\n               y = y)) +\r\n  geom_smooth(method = \"lm\",\r\n              fullrange = TRUE) +\r\n  geom_smooth(method = \"lm\",\r\n              fullrange = FALSE,\r\n              col = \"dark green\") +\r\n  geom_point() +\r\n  xlim(c(0,10)) +\r\n  geom_vline(xintercept = 8, col=\"red\") +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nA simple regression model based on data for x-values between 1 and 4 cannot reliably predict y for x=8, as its validity is limited to the range used for calibration. Beyond this range, system behavior is unknown, making extrapolation uncertain. While some might argue that prior knowledge of the system allows for reasonable assumptions about behavior outside the observed range, any such assumption should be explicitly stated and justified.\r\nMapping validity domains\r\nFor complex models like crop or phenology models, defining validity domains is even more difficult, yet often overlooked. Reliable predictions require validation under conditions similar to those the model aims to predict. This is particularly challenging for studies on phenology responses to climate change, where temperature influences may shift over time. Despite this, many models rely on simple equations that fail to account for such changes.\r\n\r\n\r\npast_weather <- read_tab(\"data/TMaxTMin1958-2019_patched.csv\")\r\npast_weather$SSP_Time <- \"Past\"\r\n\r\nfuture_temps <- load_temperature_scenarios(\"data/future_climate\",\r\n                                           \"Bonn_futuretemps\")\r\n\r\nSSPs <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\r\nTimes <- c(2050, 2085)\r\n\r\nlist_ssp <- \r\n  strsplit(names(future_temps), '\\\\.') %>%\r\n  map(2) %>%\r\n  unlist()\r\n\r\nlist_gcm <-\r\n  strsplit(names(future_temps), '\\\\.') %>%\r\n  map(3) %>%\r\n  unlist()\r\n\r\nlist_time <-\r\n  strsplit(names(future_temps), '\\\\.') %>%\r\n  map(4) %>%\r\n  unlist()\r\n\r\nfor(SSP in SSPs)\r\n  for(Time in Times)\r\n   {Temps <- future_temps[list_ssp == SSP & list_time == Time]\r\n    names(Temps) <- list_gcm[list_ssp == SSP & list_time == Time]\r\n    \r\n    for(gcm in names(Temps))\r\n      Temps[[gcm]] <- Temps[[gcm]] %>% \r\n        mutate(GCM = gcm,\r\n               SSP = SSP,\r\n               Time = Time)\r\n    \r\n    Temps <- do.call(\"rbind\", Temps)\r\n    if(SSP == SSPs[1] & Time == Times[1])\r\n      results <- Temps else\r\n        results <- rbind(results,\r\n                         Temps)\r\n    }\r\n\r\nresults$SSP[results$SSP == \"ssp126\"] <- \"SSP1\"\r\nresults$SSP[results$SSP == \"ssp245\"] <- \"SSP2\"\r\nresults$SSP[results$SSP == \"ssp370\"] <- \"SSP3\"\r\nresults$SSP[results$SSP == \"ssp585\"] <- \"SSP5\"\r\n\r\nresults$SSP_Time <- paste0(results$SSP,\" \",results$Time)\r\n\r\nfuture_months <-\r\n  aggregate(results[, c(\"Tmin\", \"Tmax\")],\r\n            by = list(results$SSP_Time,\r\n                      results$Year,\r\n                      results$Month),\r\n            FUN = mean)\r\ncolnames(future_months)[1:3] <- c(\"SSP_Time\",\r\n                                  \"Year\",\r\n                                  \"Month\")\r\n\r\npast_months <-\r\n  aggregate(past_weather[, c(\"Tmin\",\"Tmax\")],\r\n            by = list(past_weather$SSP_Time,\r\n                      past_weather$Year,\r\n                      past_weather$Month),\r\n            FUN=mean)\r\ncolnames(past_months)[1:3] <- c(\"SSP_Time\", \"Year\", \"Month\")\r\n\r\nall_months <- rbind(past_months,\r\n                    future_months)\r\n\r\nall_months$month_name <- factor(all_months$Month,\r\n                                levels = c(6:12, 1:5),\r\n                                labels = month.name[c(6:12, 1:5)])\r\n\r\n\r\nCombining Data for Improved Model Coverage\r\nHistorical data alone are insufficient for reliable future predictions. Combining past observations with experimentally enhanced data can help expand the model’s validity domain. However, even enhanced observations may not cover all future scenarios, particularly extreme temperature conditions. A combined approach of historical and enhanced data ensures better coverage for reliable phenology forecasts.\r\n\r\n\r\nenhanced <- read_tab(\"data/final_weather_data_S1_S2.csv\")\r\nenhanced$Year <- enhanced$Treatment\r\nenhanced$SSP_Time <- \"Past enhanced\"\r\n\r\n\r\nenhanced_months <- aggregate(enhanced[, c(\"Tmin\", \"Tmax\")],\r\n                             by = list(enhanced$SSP_Time,\r\n                                       enhanced$Year,\r\n                                       enhanced$Month),\r\n                             FUN = mean)\r\ncolnames(enhanced_months)[1:3] <- c(\"SSP_Time\", \"Year\", \"Month\")\r\n\r\nall_months_enhanced <- rbind(enhanced_months,\r\n                             future_months)\r\n\r\nall_months_enhanced$month_name <- factor(all_months_enhanced$Month,\r\n                                         levels = c(6:12, 1:5),\r\n                                         labels = month.name[c(6:12, 1:5)])\r\n\r\n# Calculate the hulls for each group\r\nhull_temps_enhanced <- all_months_enhanced %>%\r\n  group_by(SSP_Time,\r\n           month_name) %>%\r\n  slice(chull(Tmin, \r\n              Tmax))\r\n\r\nggplot(hull_temps_enhanced[\r\n  which(hull_temps_enhanced$Month %in% c(10, 11, 12, 1, 2, 3)),],\r\n       aes(Tmin,\r\n           Tmax,\r\n           fill = factor(SSP_Time))) +\r\n  geom_polygon() +\r\n  facet_wrap(vars(month_name)) +\r\n  scale_fill_manual(name=\"Scenario\",\r\n                    breaks=c(\"Past enhanced\",\r\n                             \"SSP1 2050\",\r\n                             \"SSP1 2085\",\r\n                             \"SSP2 2050\",\r\n                             \"SSP2 2085\",                            \r\n                             \"SSP3 2050\",\r\n                             \"SSP3 2085\",\r\n                             \"SSP5 2050\",\r\n                             \"SSP5 2085\"),\r\n                    values=c(\"black\",\r\n                             alpha(\"light green\",0.4),\r\n                             alpha(\"dark green\",0.4),\r\n                             alpha(\"coral\",0.4),\r\n                             alpha(\"dark red\",0.4),\r\n                             alpha(\"yellow\",0.4),\r\n                             alpha(\"orange\",0.4),                            \r\n                             alpha(\"light blue\",0.4),\r\n                             alpha(\"dark blue\",0.4))) +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nOne Last Thought on Model Validity\r\nWhile phenology models are straightforward, models for crops and other systems are more complex, considering factors like pest management, soil types, and farming methods. Applying these models to diverse agricultural systems, especially smallholder farms, can be problematic and should be done with caution, as they may not accurately represent real-world conditions.\r\n\r\n\r\npast_months$SSP_Time <- \"Past combined\"\r\nenhanced_months$SSP_Time <- \"Past combined\"\r\n\r\nall_months_both <- rbind(enhanced_months,\r\n                         past_months,\r\n                         future_months)\r\n\r\nall_months_both$month_name <- factor(all_months_both$Month,\r\n                                     levels = c(6:12, 1:5),\r\n                                     labels = month.name[c(6:12, 1:5)])\r\n\r\nhull_temps_both <- all_months_both %>%\r\n  group_by(SSP_Time,\r\n           month_name) %>%\r\n  slice(chull(Tmin,\r\n              Tmax))\r\n\r\nggplot(hull_temps_both[\r\n  which(hull_temps_both$Month %in% c(10, 11, 12, 1, 2, 3)),],\r\n       aes(Tmin,\r\n           Tmax,\r\n           fill = factor(SSP_Time))) +\r\n  geom_polygon() +\r\n  facet_wrap(vars(month_name)) +\r\n  scale_fill_manual(name=\"Scenario\",\r\n                    breaks=c(\"Past combined\",\r\n                             \"SSP1 2050\",\r\n                             \"SSP1 2085\",\r\n                             \"SSP2 2050\",\r\n                             \"SSP2 2085\",                            \r\n                             \"SSP3 2050\",\r\n                             \"SSP3 2085\",\r\n                             \"SSP5 2050\",\r\n                             \"SSP5 2085\"),\r\n                    values=c(\"black\",\r\n                             alpha(\"light green\",0.4),\r\n                             alpha(\"dark green\",0.4),\r\n                             alpha(\"coral\",0.4),\r\n                             alpha(\"dark red\",0.4),\r\n                             alpha(\"yellow\",0.4),\r\n                             alpha(\"orange\",0.4),                            \r\n                             alpha(\"light blue\",0.4),\r\n                             alpha(\"dark blue\",0.4))) +\r\n  theme_bw(base_size = 15)\r\n\r\n\r\n\r\nExercises on making valid tree phenology models\r\nExplain the difference between output validation and process validation.\r\nOutput validation and process validation assess model reliability in different ways. Output validation checks whether a model accurately predicts observed data using statistical measures like RMSE. However, this does not ensure that the model’s internal processes are correctly represented—it may produce accurate results for the wrong reasons.\r\nProcess validation, in contrast, examines whether the model’s mechanisms align with real-world processes. This requires domain knowledge and ensures that key dynamics, such as temperature responses in phenology models, are properly captured. While harder to quantify, process validation is essential for applying models to new conditions.\r\nThe main difference is that output validation focuses on predictive accuracy, while process validation ensures that the model’s structure reflects reality.\r\nExplain what a validity domain is and why it is important to consider this whenever we want to use our model to forecast something.\r\nA validity domain defines the range of conditions under which a model produces reliable results. If used outside this range, predictions may be inaccurate.\r\nConsidering the validity domain is crucial for forecasting, as models based on past data may not perform well under new conditions. Applying a model beyond its tested limits without validation increases the risk of misleading conclusions. To ensure reliability, predictions should stay within the model’s validated range or the model should be refined with additional data.\r\nWhat is validation for purpose?\r\nValidation for purpose ensures a model is suitable for its intended use. It checks whether the model has been tested under relevant conditions and captures key processes. A model may perform well on past data but still be unreliable for future forecasts if conditions change. This approach prevents misapplication and ensures meaningful predictions.\r\nHow can we ensure that our model is suitable for the predictions we want to make?\r\nTo ensure a model is suitable for predictions, we must:\r\nDefine the Purpose – Clearly identify what the model should predict and under what conditions.\r\nCheck the Validity Domain – Ensure the model has been tested under conditions similar to those it will be applied to.\r\nValidate Both Output and Process – Confirm that predictions match observed data (output validation) and that the model correctly represents real-world processes (process validation).\r\nAssess Uncertainty – Identify possible errors and quantify uncertainties in the predictions.\r\nRefine with Additional Data – Expand or adjust the model using experimental or new observational data to improve its reliability.\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T10:07:57+01:00"
    },
    {
      "path": "winter_chill.html",
      "title": "Winter chill projections",
      "author": [],
      "contents": "\r\nThis section provides an overview of how winter chill can be modeled. It summarizes past studies on this topic, aiming to clarify the methodological aspects that lead to the analyses conducted. By the end of this lesson, most of the analyses presented in the discussed papers should be understandable.\r\nWinter chill in Oman\r\nDuring his doctoral studies at the University of Kassel, Prof. Dr. Eike Lüdeling became interested in winter chill while participating in research on mountain oases in Oman. Initially focused on calculating nutrient budgets for the oases, particularly in the “Hanging Gardens” of Ash Sharayjah, the study shifted when many fruit trees failed to bear fruit. This led to the hypothesis that insufficient winter chill might be the issue, especially since the oases hosted temperate species such as pomegranates (Punica granatum), walnuts (Juglans regia), and apricots (Prunus armeniaca).\r\nTo investigate this, temperature loggers were placed in three oases at different levels of elevation, allowing for the study of chill accumulation along an elevation gradient. A map of the study area illustrates the locations of the oases:\r\nMap of oasis systems in Al Jabal Al Akhdar, OmanA nearby long-term weather station provided valuable data, although its location - 1000 meters above the lowest oasis - limited its representativeness. Since records were available from the oases, transfer functions were defined to derive oasis temperatures from the long-term data. These transfer functions were set up using PLS regression, which, in hindsight, wasn’t a very good idea, to directly calculate hourly temperatures in the oases from the daily records of the official station at Saiq.\r\nRegression between temperature at Saiq and temperature in three oases, Al Jabal Al Akhdar, OmanThis approach facilitated the calculation of hourly temperatures, which were essential for assessing winter chill dynamics over several years.\r\nChill dynamics between 1983 and 2007, Al Jabal Al Akhdar, OmanThe findings were submitted to the journal Climatic Change (Luedeling et al., 2009b), where reviewers suggested incorporating future climate scenarios. To address this, the LARS-WG weather generator was employed to simulate plausible weather conditions for the oases under scenarios of 1°C and 2°C warming.\r\nChill prospects for 1°C and 2°C warming scenarios in Al Jabal Al Akhdar, OmanThe results illustrated the potential impacts of climate change on winter chill, marking the beginning of a career focused on chill modeling.\r\nChill model sensitivity\r\nAfter completing a PhD at the University of Kassel, Prof. Dr. Eike Lüdeling became a Postdoctoral Scholar at the University of California at Davis, where his research focused on climate change impacts on winter chill in California’s Central Valley, a key region for temperate fruit tree production.\r\nUpon arriving in California, it became evident that the choice of chill model significantly impacts winter chill quantification. Initially, the simplest model was chosen due to a lack of programming skills, but further investigation highlighted the importance of model selection. Extensive library research revealed the need for a thorough examination of various chill models. Knowledge gained in Oman was utilized to create temperature scenarios for multiple locations, allowing for the analysis of how chill accumulation would likely change in the future.\r\nThe analysis focused on changes predicted by various models for the same locations and future scenarios. Here are the locations examined:\r\nWeather station locations in CaliforniaThe results revealed considerable variation in chill projections for these locations. The analysis illustrated significant differences in estimates of chill losses by 2050, indicating that not all models could accurately represent winter chill dynamics. Ultimately, the Dynamic Model emerged as the most reliable option, prompting its primary use in subsequent research.\r\nSensitivity of chill projections to model choice (CH - Chilling Hours; Utah - Utah Model; Utah+ - Positive Utah Model; Port. - Dynamic Model)However, challenges arose with the complexity of the Dynamic Model, which required outdated Excel software for calculations. Additionally, the data processing steps necessary to generate credible temperature scenarios proved cumbersome and error-prone, highlighting the need to develop programming skills for more efficient analysis.\r\nWinter chill in California\r\nThe primary goal during the time in California was to create a winter chill projection for the Central Valley, an important region for fruit and nut production. Utilizing California’s extensive network of weather stations, the plan involved using data from over 100 stations and generating multiple climate scenarios. To manage this complex task efficiently, a decision was made to automate most processes, leading to an exploration of programming.\r\nThe automation was implemented using JSL, a programming language associated with the statistics software JMP, which facilitated the handling of the data. Despite some challenges, the automation was largely successful, though running the weather generator manually for each station remained tedious. Ultimately, projections were generated for all stations, illustrating chill accumulation over 100 plausible winter seasons for each climate scenario.\r\nTo present the results effectively, a metric called ‘Safe Winter Chill’ was developed, defined as the 10th percentile of the chill distribution, indicating the minimum chill amount that would be exceeded in 90% of the years. Here’s an illustration of the Safe Winter Chill metric:\r\nIllustration of the Safe Winter Chill conceptA method for spatially interpolating the station results was also established, leading to the creation of maps that depicted winter chill prospects for the Central Valley. Here’s one of the maps that resulted from this:\r\nWinter chill prospects for California’s Central ValleyThis analysis was published in the journal PLOS ONE (Luedeling et al., 2009d).\r\nWinter chill ratios\r\nFollowing the automation of processing steps in JSL, attention turned to creating a global winter chill projection. The Global Summary of the Day database was identified as a valuable data source, featuring records from thousands of weather stations. The project proved challenging due to limited programming skills. Data processing was carried out on six computers operating around the clock for several weeks, likely a result of initial setup difficulties rather than the complexity of the analyses. In the end, data for about 5,000 weather stations were processed, generating multiple chill metrics.\r\nThis extensive dataset allowed for a comparison of chill models by calculating the ratios between various chill metrics at each station. If these ratios had been consistent worldwide (e.g., one Chill Portion always equating to ten Chilling Hours), any chill model could have been reliably used. However, significant variations in chill metric ratios were observed globally.\r\nChill metric ratios around the worldThis study was published in the International Journal of Biometeorology (Luedeling & Brown, 2011a).\r\nA global projection of future winter chill\r\nUsing the same analytical methods, a global projection of the potential impacts of climate change on winter chill was also generated:\r\nProjected decline in available winter chill around the worldThe regions marked in red and orange on the lower two maps may experience significant impacts on fruit and nut production due to decreasing winter chill. With substantial chill losses, it is unlikely that growers will be able to sustain their current tree cultivars. Notably, the Mediterranean region is expected to be particularly affected.\r\nWinter chill projection for the Mediterranean regionThis prompted collaboration with partners in the Mediterranean region and other countries with similar climates, such as South Africa and Chile.\r\nWinter chill in Germany\r\nGermany is not highlighted as particularly vulnerable to chill losses, and an analysis of historical chilling trends from 1950 supports this observation:\r\nWinter chill in Germany in 2010, and changes between 1950 and 2010Winter chill in Tunisia\r\nProspects for orchards in Tunisia are particularly challenging due to the region being close to the warmest limits for many fruit and nut tree species. An assessment published in 2018 examined past and future trends in winter chill for an orchard in Central Tunisia, following a seven-year gap from earlier studies. This delay stemmed from other professional commitments and the difficulty of obtaining suitable future climate data for chill modeling.\r\nWhile climate change data is widely available, much of it is presented as spatial grids, making it cumbersome to work with. Each climate scenario requires numerous grids for temperature and rainfall, leading to substantial data storage needs, sometimes exceeding 700 GB. Soon after establishing a processing structure for these datasets, the IPCC introduced the Representative Concentration Pathways (RCPs), rendering earlier scenarios outdated and complicating the analysis further, especially given the limited data transfer capabilities while based in Kenya.\r\nCollaboration with colleagues in Tunisia, particularly Haifa Benmoussa, revealed that tree crops like almonds and pistachios are highly vulnerable to climate change impacts. Fortunately, a new climate database specifically for Africa, called AFRICLIM, was developed, facilitating the acquisition and processing of relevant climate scenarios. This allowed for the incorporation of new functions in chillR to sample from AFRICLIM grids and produce the necessary climate projections.\r\nWinter chill analysis for an orchard near Sfax in Central Tunisia (blue bars indicate approximate chilling requirements of pistachios and two types of almonds)The figure, which is to be created by the end of the semester, illustrates the historical development of chill accumulation at a specific location, with observed values represented by red dots and typical chill distributions shown as boxplots. These data were generated using a weather generator that is calibrated with historical weather data and produces artificial annual weather records. The generator was also used to create future scenarios based on the AFRICLIM database.\r\nThe analysis indicates that in none of the future scenarios does the cultivation of pistachios or high-chill almonds remain viable. This conclusion is supported by observations in Tunisia, where, after the warm winter of 2015/16, many pistachio trees barely developed any flowers, leading to crop failures.\r\nPistachio tree near Sfax, Central Tunisia, in April of 2016Winter chill in Chile\r\nAFRICLIM addressed the challenge of obtaining future climate data for Africa but did not fully meet the needs for integrating climate change projections into chillR. It was limited to African data, and users seeking information for single locations had to download large datasets, which was inefficient. A more effective solution was needed to access data quickly for individual weather stations globally.\r\nAn early resource was ClimateWizard, developed by Evan Girvetz, which initially provided gridded data but later included a script for extracting information for specific locations. This functionality was eventually made available through an API at CIAT, allowing access to outputs from 15 climate models for the latest RCP scenarios. This advancement enabled Eduardo Fernández to analyze past and future chill development across nine locations in Chile, expanding the geographic scope of the research.\r\nMap of fruit growing locations in ChileThe following diagram illustrates the assessment of past and future winter chill across nine locations in Chile:\r\nAssessment of past and future winter chill for 9 locations across ChileEduardo preferred a different plot design and utilized the ggplot2 package, a robust plotting tool for R, to redesign it. The complexity of having data from multiple sites made interpretation challenging, prompting Eduardo to creatively summarize key information for each scenario. He presented this information as a heat map, simplifying the visualization.\r\nHeatmap showing Safe Winter Chill (10% quantile of chill distribution) for nine locations in ChileChill projection for Patagonia\r\nCertain regions may become more suitable for agriculture as the climate changes. An analysis was conducted to assess the climatic suitability for fruit and nut trees in Patagonia, southern Argentina, which is located at the southern frontier of agriculture:\r\nMap of parts of Patagonia in Argentina, showing locations that were analyzed in this studyWeather station records for all locations on the map were obtained, enabling the calibration of a weather generator and the download of climate projections from the ClimateWizard database. This facilitated the creation of past and future temperature scenarios for all stations, as well as the computation of winter chill and other agroclimatic metrics. However, the results of the winter chill calculations were not particularly noteworthy, as minimal changes were projected.\r\nHeatmap showing Safe Winter Chill (10% quantile of chill distribution) for eleven locations in PatagoniaClimate change could potentially enhance land suitability for fruit trees by providing increased summer heat:\r\nPast and projected future heat availability for four exemplary locations in PatagoniaA further beneficial development is a likely reduction in the number of frost hours:\r\nPast and projected frost risk for four exemplary locations in PatagoniaWhile the changes observed may appear minor, they are likely to shift many locations from a climate that is too cool for agriculture, particularly for fruit trees, to a more optimal situation. This presents a rare instance of potentially positive news related to climate change, though it is important to acknowledge that these changes could have negative consequences for natural ecosystems and other agricultural systems.\r\nChill model comparison\r\nEduardo Fernandez recently utilized the climate change analysis framework to enhance previous chill model comparisons, significantly building on earlier work. He compiled a collection of 13 methods for quantifying chill accumulation from existing literature and applied these models to datasets from several locations in Germany, Tunisia, and Chile, which are part of the PASIT project. A map illustrates the locations included in this analysis.\r\nLocations used for comparing predictions by a total of 13 chill models across different climatesThe expectation was that the models would show significant differences in the extent of changes they predicted, and this anticipation was indeed fulfilled:\r\nChill change projections by a total of 13 chill models across different climate scenariosThe figure illustrates the changes predicted by 13 different models across various sites and climate scenarios, categorized into three groups: warm, moderate, and cool. Eduardo’s analysis reveals significant discrepancies among the models, highlighting the risks of selecting the most convenient model for predictions. The variation in predictions is evident in the color distribution across the rows of the panels, with a uniform color indicating consistency among models—something that is not observed here.\r\nFor locations in Tunisia and Chile, the predictions mainly concern the extent of chill losses, ranging from mild to alarming. In Germany, the situation is even less clear, with some models predicting increases in chill and others predicting decreases.\r\nThese findings underscore the importance of model choice, as many models may be arbitrary and can be disregarded, yet uncertainties remain regarding which models accurately represent future conditions. This area of research offers opportunities for further exploration and innovation.\r\nChill projection for all of Tunisia\r\nThe study projected climate change impacts on winter chill for an orchard near Sfax in Central Tunisia, but the region is not the most favorable for temperate fruit and nut tree cultivation. Tunisia is climatically diverse, featuring mountains, plains, coastal areas, and interior deserts, leading to significant variation in historical and future chill availability across the country.\r\nUnder the leadership of Haifa Benmoussa, the team mapped chill accumulation throughout Tunisia using a framework previously developed. This analysis utilized data from 20 weather stations in Tunisia and neighboring countries. By applying the established analytical framework to each location, they were able to interpolate results and create chill maps that illustrate the trends in chill availability in Tunisia over the past few decades:\r\nChill availability maps for Tunisia for several past scenariosThe process of interpolating site-specific results into a comprehensive map for Tunisia involves some areas for improvement. Currently, the methodology uses site-specific predictions of Safe Winter Chill, defined as the 10th percentile of the chill distribution derived from annual temperature dynamics generated by the weather model. This information is then interpolated using the Kriging technique.\r\nIn addition, the elevations of the locations where chill was modeled are also considered. A linear model is fitted to establish a relationship between chill accumulation and elevation. Using a Digital Elevation Model (DEM), the differences between the model-derived elevations from weather stations and the actual elevations of each location are calculated. This difference, not accounted for in the initial chill surface derived from weather station data, is corrected using the established elevation-chill relationship.\r\nWhile this method seems reasonable for Tunisia, it may not be suitable for cooler regions like Germany, where the relationship between elevation and chill availability may not be linear. The resulting projection of future chill for Tunisia is displayed in the following map:\r\nChill availability for Tunisia for various plausible scenarios of future climateThe projections reveal significant concern regarding winter chill in Tunisia. The Dynamic Model, which is regarded as a reliable predictor, indicates substantial decreases in Chill Portions, the units used by the model. This trend poses serious challenges for much of the country. Even in areas where some winter chill is expected to persist, farmers will need to adapt their practices, as the tree species currently cultivated are suited to past climate conditions. Adaptation strategies may include shifting to tree cultivars with lower chilling requirements, provided such options are available.\r\nRevisiting chill accumulation in Oman\r\nAfter a decade of exploration in other regions, the analysis turned back to Oman, where there was a desire to enhance the initial study of chill accumulation. The first assessment had limitations, particularly concerning model selection and a lack of adequate future climate data. With encouragement from Prof. Dr. Andreas Bürkert, a more robust evaluation became possible using the climate change analysis framework. This involved incorporating new methods to convert daily temperatures into hourly data. Updated assessments of past winter chill and future forecasts for the oases of Al Jabal Al Akhdar were produced, with the findings published in Climatic Change (Buerkert et al., 2020).\r\nExercises on past chill projections\r\nSketch out three data access and processing challenges that had to be overcome in order to produce chill projections with state-of-the-art methodology.\r\nAccessing Climate Data for Specific Locations:\r\nPrevious climate datasets like AFRICLIM and ClimateWizard only provided large-scale data. To get weather data for specific locations without downloading too much extra information, an API was created to quickly access data for single sites\r\nConverting Daily to Hourly Temperature Data:\r\nChill models need hourly temperature data, but many databases only give daily averages. Early methods for converting daily to hourly data weren’t very good, especially in areas with unique temperatures. Improved algorithms were developed to estimate hourly temperatures more accurately from daily data\r\nHandling Large Volumes of Climate Model Outputs:\r\nStudying different climate futures involves dealing with a lot of data from many climate models, which can be hard to manage. To handle this large amount of data effectively, workflows were streamlined and selective processing techniques were used\r\nOutline, in your understanding, the basic steps that are necessary to make such projections.\r\nTo make climate-based chill projections for specific regions, here are the essential steps typically involved:\r\nData Collection and Calibration: collect historical weather data and use it to calibrate a weather generator for realistic temperature simulations\r\nModel Selection and Scenario Setup: choose relevant climate models and emission scenarios to explore various future climates\r\nGenerate Temperature Projections: downscale climate data, converting it to daily or hourly temperatures as needed for chill calculations\r\nChill Calculation: apply chill models to estimate chill accumulation across different climate scenarios\r\nAnalysis and Visualization: compare chill projections across models and scenarios and visualize the findings\r\nInterpretation: validate projections with observed data where possible and assess agricultural impacts and adaptation needs\r\n\r\n\r\n\r\n",
      "last_modified": "2025-03-15T10:08:54+01:00"
    }
  ],
  "collections": []
}
